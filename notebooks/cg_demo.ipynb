{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from cords.cords.selectionstrategies.supervisedlearning import DataSelectionStrategy\n",
    "from cords.cords.utils.models import ResNet18\n",
    "from gable.gable.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "# for cuda\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class custom_subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "        labels(sequence) : targets as required for the indices. will be the same length as indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, labels):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.targets = labels.type(torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target = self.targets[idx]\n",
    "        return (image, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device):\n",
    "    if name == 'ResNet18':\n",
    "        model = ResNet18(num_cls)\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def kernel(x, y, measure=\"cosine\", exp=2):\n",
    "    if(measure==\"eu_sim\"):\n",
    "        lam = 0.25\n",
    "        dist = pairwise_distances(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = max(dist.ravel()) - dist\n",
    "        sim *= lam\n",
    "        sim = np.exp(sim)\n",
    "#         n = x.size(0)\n",
    "#         m = y.size(0)\n",
    "#         d = x.size(1)\n",
    "#         x = x.unsqueeze(1).expand(n, m, d)\n",
    "#         y = y.unsqueeze(0).expand(n, m, d)\n",
    "#         dist = torch.pow(x - y, exp).sum(2)\n",
    "#         const = torch.max(dist).item()\n",
    "#         sim = (const - dist)\n",
    "    \n",
    "        #dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))\n",
    "    if(measure==\"cosine\"):\n",
    "        sim = cosine_similarity(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = np.exp(sim)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def save_kernel_hdf5(lake_kernel, lake_target_kernel, target_kernel=[], numpy=True):\n",
    "    if(not(numpy)):\n",
    "        lake_kernel = lake_kernel.cpu().numpy()\n",
    "    with h5py.File(\"lake_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_target_kernel = lake_target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"lake_target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_target_kernel)\n",
    "    if(not(numpy)):\n",
    "        target_kernel = target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=target_kernel)\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "    saveDir = os.path.join(saveDir, prefix)\n",
    "    if(not(os.path.exists(saveDir))):\n",
    "        os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    class_err_log = []\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        print(\"val, test error% for class \", i, \" : \", round((len(val_err_class_idx)/len(val_class_idxs))*100,2), round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        class_err_log.append(\"val, test error% for class \"+ str(i) + \" : \"+ str(round((len(val_err_class_idx)/len(val_class_idxs))*100,2)) + \", \" + str(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)))\n",
    "        tst_err_log.append(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        \n",
    "    return tst_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget)):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = custom_subset(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    return aug_train_set, remain_lake_set\n",
    "                        \n",
    "def getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def getSMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, sf):\n",
    "    if(sf==\"fl1mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\")\n",
    "    elif(sf == \"logdetmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \"  -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\") + \" -queryqueryKernelFile \" + os.path.join(hdf5Path, \"target_kernel.hdf5\")\n",
    "    elif(sf==\"fl2mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gcmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gccg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl1cg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"logdetcg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\") + \" -privateprivateKernelFile \" + os.path.join(hdf5Path, \"target_kernel.hdf5\")\n",
    "    elif(sf==\"fl\" or sf==\"logdet\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\")\n",
    "    elif(sf ==\"gc\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\")\n",
    "    print(\"Executing SIM command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "# def getDuplicates(subset):\n",
    "    \n",
    "#check overlap with prev selections\n",
    "def check_overlap(prev_idx, prev_idx_hist, idx):\n",
    "    prev_idx = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in prev_idx ]\n",
    "    prev_idx_hist = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in prev_idx_hist]\n",
    "    idx = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in idx]\n",
    "    # overlap = set(prev_idx).intersection(set(idx))\n",
    "    overlap = [value for value in idx if value in prev_idx] \n",
    "    # overlap_hist = set(prev_idx_hist).intersection(set(idx))\n",
    "    overlap_hist = [value for value in idx if value in prev_idx_hist]\n",
    "    new_points = set(idx) - set(prev_idx_hist)\n",
    "    total_unique_points = set(idx+prev_idx_hist)\n",
    "    print(\"Num unique points within this selection: \", len(set(idx)))\n",
    "    print(\"New unique points: \", len(new_points))\n",
    "    print(\"Total unique points: \", len(total_unique_points))\n",
    "    print(\"overlap % of sel with prev idx: \", len(overlap)/len(idx))\n",
    "    print(\"overlap % of sel with all prev idx: \", len(overlap_hist)/len(idx))\n",
    "#     return len(overlap)/len(idx), len(overlap_hist)/len(idx)\n",
    "    return len(total_unique_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "datadir = 'data/'\n",
    "data_name = 'cifar10'\n",
    "num_cls=10\n",
    "fraction = float(0.1)\n",
    "budget=500\n",
    "num_epochs = int(5)\n",
    "select_every = int(2)\n",
    "num_rep = 10\n",
    "# feature='vanilla'\n",
    "feature = 'duplicate'\n",
    "# feature = 'classimb'\n",
    "num_runs = 1  # number of random runs\n",
    "learning_rate = 0.01\n",
    "model_name = 'ResNet18'\n",
    "magnification = 10\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# split_cfg = {\"num_cls_imbalance\":2, \"per_imbclass_train\":50, \"per_imbclass_val\":25, \"per_imbclass_lake\":150, \"per_class_train\":1000, \"per_class_val\":25, \"per_class_lake\":3000}\n",
    "split_cfg = {\"train_size\":500, \"val_size\":1000, \"lake_size\":5000, \"num_rep\":num_rep, \"lake_subset_repeat_size\":1000}\n",
    "datkbuildPath = \"/home/snk170001/bioml/dss/notebooks/datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "initModelPath = \"weights/cg_\" + data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"train_size\"])\n",
    "print(\"Using Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, select_every, learning_rate, run,\n",
    "#                 device, strategy, sf=\"\")\n",
    "\n",
    "train_model(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, select_every, learning_rate, 1, device, False, \"SIM\",'gccg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distil.distil.active_learning_strategies import BADGE, EntropySampling\n",
    "from distil.distil.utils.DataHandler import DataHandler_CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    if(feature==\"classimb\"):\n",
    "        train_set, val_set, test_set, lake_set, imb_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        print(\"imbalanced classes are: \", imb_cls_idx)\n",
    "    if(feature==\"duplicate\" or feature==\"vanilla\"):\n",
    "        sel_cls_idx = None\n",
    "        if(strategy == \"SIM\" or strategy==\"random\"):\n",
    "            train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        elif(strategy==\"AL\"):\n",
    "            X_tr, y_tr, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True)\n",
    "        \n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "    print(\"Budget: \", bud, \"selected every: \", select_every)\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "    all_logs_dir = 'CG_active_learning_results/' + sf  + '/' + dataset_name + '/' + str(bud) + '/' + str(run)\n",
    "    print(all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "    path_logfile = os.path.join(all_logs_dir, dataset_name + '.txt')\n",
    "    logfile = open(path_logfile, 'w')\n",
    "    exp_name = dataset_name + '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + \\\n",
    "               '_selEvery:' + str(select_every) + '_variant' + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \"feature\":feature, \"sel_budget\":budget, \"num_selections\":num_epochs, \"model\":model_name, \"learning_rate\":learning_rate, \"setting\":split_cfg, \"test_acc\":[], \"num_unique_samples\":[], \"sel_cls_idx\":sel_cls_idx}\n",
    "\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device)\n",
    "    model1 = create_model(model_name, num_cls, device)\n",
    "    if(strategy == \"AL\"):\n",
    "        strategy_args = {'batch_size' : budget}\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "#     optimizer, scheduler = optimizer_with_scheduler(model, num_epochs, learning_rate)\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "    #overlap vars\n",
    "    prev_idx = None\n",
    "    prev_idx_hist = []\n",
    "    per_ep_overlap = []\n",
    "    overall_overlap = []\n",
    "    idx_tracker = np.array(list(range(len(lake_set))))\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training\")\n",
    "                for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    tst_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    tst_total += targets.size(0)\n",
    "                    tst_correct += predicted.eq(targets).sum().item()\n",
    "                tst_acc[i] = tst_correct / tst_total\n",
    "                continue\n",
    "        else:\n",
    "#             if(full_trn_acc[i-1] >= 0.99): #The model has already trained on the seed dataset\n",
    "            #use misclassifications on validation set as queries\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append([\"epoch \"+str(i+1)]+tst_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                    miscls_set = getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx)\n",
    "                    misclsloader = torch.utils.data.DataLoader(miscls_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, misclsloader, model1, num_cls, True, device) #set last arg to true for linear layer\n",
    "                elif(sf.endswith(\"cg\")): #atleast one selection must be done for private set in cond gain functions\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, privateSetloader, model1, num_cls, True, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        #compute subset with private set a NULL\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, True, device)\n",
    "                else:\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, True, device)\n",
    "                start_time = time.time()\n",
    "                cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "                clone_dict = copy.deepcopy(model.state_dict())\n",
    "                #update the selection strategy model with new params for gradient computation\n",
    "                setf_model.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mi\")): #SMI functions need the target set gradients\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                    train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    numQueryPrivate = train_val_kernel.shape[1]\n",
    "                elif(sf.endswith(\"cg\")):\n",
    "                    if(len(private_set)!=0):\n",
    "                        setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                        print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                        print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_private_kernel\n",
    "                        numQueryPrivate = train_val_kernel.shape[1]\n",
    "                    else:\n",
    "#                         assert(((i + 1)/select_every)==1)\n",
    "                        setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        train_val_kernel = []\n",
    "                        numQueryPrivate = 0\n",
    "                else: # For other submodular functions needing only image kernel\n",
    "                    setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                    train_val_kernel = []\n",
    "                    numQueryPrivate = 0\n",
    "\n",
    "                kernel_time = time.time()\n",
    "                train_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.grads_per_elem.double()) #img_img_kernel\n",
    "                if(sf==\"logdetmi\" or sf==\"logdetcg\"):\n",
    "                    if(len(private_set)!=0):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                    else:\n",
    "                        val_kernel = []\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel, val_kernel)\n",
    "                else:\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel)\n",
    "                print(\"kernel compute time: \", time.time()-kernel_time)\n",
    "                #call the c++ exec to read kernel and compute subset of selected minibatches\n",
    "                subset = getSMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQueryPrivate), sf)\n",
    "                print(subset[:5])\n",
    "                model.load_state_dict(cached_state_dict)\n",
    "                if(sf.endswith(\"cg\")): #for first selection\n",
    "                    if(len(private_set)==0):\n",
    "                        private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "                    else:\n",
    "                        private_set = getPrivateSet(lake_set, subset, private_set)\n",
    "                    print(\"size of private set: \", len(private_set))\n",
    "\n",
    "    #           temp = np.array(list(trainloader.batch_sampler))[subset] #if per batch\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                strategy_sel.update_model(model)\n",
    "                if(sf==\"badge\"):\n",
    "                    subset = strategy_sel.select(budget)\n",
    "                if(sf==\"us\"):\n",
    "                    subset = list(strategy_sel.select(budget).cpu().numpy())\n",
    "                print(len(subset), \" samples selected\")\n",
    "                X_tr = np.concatenate((X_tr, X_unlabeled[subset]), axis=0)\n",
    "                X_unlabeled = np.delete(X_unlabeled, subset, axis = 0)\n",
    "                y_tr = np.concatenate((y_tr, y_unlabeled[subset]), axis = 0)\n",
    "                y_unlabeled = np.delete(y_unlabeled, subset, axis = 0)\n",
    "                strategy_sel.update_data(X_tr, y_tr, X_unlabeled)\n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            if(i>1):\n",
    "                curr_unique_points = check_overlap(prev_idx, prev_idx_hist, list(idx_tracker[subset]))\n",
    "                res_dict[\"num_unique_samples\"].append(curr_unique_points)\n",
    "#                 per_ep_overlap.append(per_ep)\n",
    "#                 overall_overlap.append(overall)\n",
    "            prev_idx = list(idx_tracker[subset])\n",
    "            prev_idx_hist += list(idx_tracker[subset])\n",
    "            idx_tracker = np.delete(idx_tracker, subset, axis=0)\n",
    "\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "\n",
    "            #augment the train_set with selected indices from the lake\n",
    "            train_set, lake_set = aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set))\n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            assert(len(idx_tracker)==len(lake_set))\n",
    "#             model =  model.apply(weight_reset).cuda()\n",
    "            model = create_model(model_name, num_cls, device)\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<150):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # print(batch_idx)\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "#                 if(i == (num_epochs-1)):\n",
    "            final_val_predictions += list(predicted.cpu().numpy())\n",
    "            final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "            # sys.exit()\n",
    "\n",
    "        if((val_correct/val_total) > best_val_acc):\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            if((val_correct/val_total) > best_val_acc):\n",
    "#                 if(i == (num_epochs-1)):\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "        if((val_correct/val_total) > best_val_acc):\n",
    "            best_val_acc = (val_correct/val_total)\n",
    "        val_acc[i] = val_correct / val_total\n",
    "        tst_acc[i] = tst_correct / tst_total\n",
    "        val_losses[i] = val_loss\n",
    "        fulltrn_losses[i] = full_trn_loss\n",
    "        tst_losses[i] = tst_loss\n",
    "        full_val_acc = list(np.array(val_acc))\n",
    "        full_timing = list(np.array(timing))\n",
    "        res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "        print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_class_err_idxs = find_err_per_class(test_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append([\"final\"]+tst_err_log)\n",
    "        print(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    return tst_acc, prev_idx_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL badge\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500 selected every:  2\n",
      "CG_active_learning_results/badge/cifar10/500/1\n",
      "cifar10_budget:500_epochs:5_selEvery:2_variant_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Epoch: 1 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.035170621238649 0.994 313.12787848711014 0.405 338.0977272987366 0.4127 64.1098985671997\n",
      "saving initial model\n",
      "AL epoch:  1\n"
     ]
    }
   ],
   "source": [
    "# train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n",
    "badge_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM gccg\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500 selected every:  2\n",
      "CG_active_learning_results/gccg/cifar10/500/1\n",
      "cifar10_budget:500_epochs:5_selEvery:2_variant_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  36.30133628845215\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 0 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[4170, 4645, 363, 5017, 13956]\n",
      "size of private set:  500\n",
      "selEpoch: 1, Selection Ended at: 2021-04-02 01:20:15.980339\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.7778787149582058 0.998 291.8450954556465 0.466 334.3442895412445 0.4647 223.44091820716858\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13500, 5130])\n",
      "val minibatch gradients shape  torch.Size([500, 5130])\n",
      "kernel compute time:  23.755255222320557\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[8680, 5385, 13170, 1260, 8977]\n",
      "size of private set:  1000\n",
      "Num unique points within this selection:  245\n",
      "New unique points:  244\n",
      "Total unique points:  505\n",
      "overlap % of sel with prev idx:  0.006\n",
      "overlap % of sel with all prev idx:  0.006\n",
      "selEpoch: 2, Selection Ended at: 2021-04-02 01:25:15.097660\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.160608742502518 0.9933333333333333 267.64781934022903 0.5 298.6595447063446 0.496 180.01894783973694\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13000, 5130])\n",
      "val minibatch gradients shape  torch.Size([1000, 5130])\n",
      "kernel compute time:  22.636207103729248\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 1000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[1444]\n",
      "size of private set:  1001\n",
      "Num unique points within this selection:  1\n",
      "New unique points:  1\n",
      "Total unique points:  506\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 3, Selection Ended at: 2021-04-02 01:29:15.180391\n",
      "Budget not filled, adding  499  randomly.\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5cfef738f215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgccg_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_al\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatkbuildPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SIM\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gccg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-51d4517acf7e>\u001b[0m in \u001b[0;36mtrain_model_al\u001b[0;34m(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrn_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mlakeloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlake_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtst_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlake_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;31m#             model =  model.apply(weight_reset).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gccg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL1CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl1cg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'fl1cg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDETCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logdetcg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'logdetcg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500 selected every:  2\n",
      "CG_active_learning_results/fl/cifar10/500/2\n",
      "cifar10_budget:500_epochs:5_selEvery:2_variant_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  15.207470417022705\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2483, 4181, 3527, 1857, 5793]\n",
      "selEpoch: 1, Selection Ended at: 2021-03-29 23:10:09.041215\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.287552594440058 0.99 259.42974132299423 0.485 299.59412145614624 0.4981 168.09311747550964\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  13.139288187026978\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[6612, 9011, 547, 7258, 9167]\n",
      "Num unique points within this selection:  478\n",
      "New unique points:  303\n",
      "Total unique points:  787\n",
      "overlap % of sel with prev idx:  0.368\n",
      "overlap % of sel with all prev idx:  0.368\n",
      "selEpoch: 2, Selection Ended at: 2021-03-29 23:13:43.972345\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.8611042362172157 0.9906666666666667 207.64523619413376 0.569 239.0443538427353 0.5517 222.57295441627502\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  12.79050350189209\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[8715, 4436, 3773, 1461, 404]\n",
      "Num unique points within this selection:  473\n",
      "New unique points:  200\n",
      "Total unique points:  987\n",
      "overlap % of sel with prev idx:  0.362\n",
      "overlap % of sel with all prev idx:  0.582\n",
      "selEpoch: 3, Selection Ended at: 2021-03-29 23:18:10.484621\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.2586957255844027 0.9905 209.94555367529392 0.579 237.49738681316376 0.5747 335.6748836040497\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  11.672476768493652\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[5642, 3139, 6676, 9870, 6442]\n",
      "Num unique points within this selection:  489\n",
      "New unique points:  107\n",
      "Total unique points:  1094\n",
      "overlap % of sel with prev idx:  0.384\n",
      "overlap % of sel with all prev idx:  0.782\n",
      "selEpoch: 4, Selection Ended at: 2021-03-29 23:24:26.657542\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.5392707443679683 0.9952 193.57985192537308 0.582 208.8108308315277 0.5889 294.56528902053833\n"
     ]
    }
   ],
   "source": [
    "fl_test_acc, selected_idx = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [int(i/5) for i in selected_idx]\n",
    "print(x[:5])\n",
    "print(len(x[3500:]),len(set(x[3500:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"random\",'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  5000\n",
      "Budget:  500 selected every:  2\n",
      "CG_active_learning_results/fl/cifar10/500/2\n",
      "cifar10_budget:500_epochs:5_selEvery:2_variant_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Epoch: 1 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0976276411674917 0.99 300.9132677912712 0.422 338.40636587142944 0.4212 78.468909740448\n",
      "saving initial model\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  2.3506908416748047\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2380, 3494, 1470, 997, 2472]\n",
      "selEpoch: 1, Selection Ended at: 2021-03-29 22:38:57.727570\n",
      "After augmentation, size of train_set:  1000  lake set:  4500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.527952621690929 0.994 258.5853514075279 0.508 281.0728050470352 0.511 156.07492542266846\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.9650421142578125\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[4179, 4268, 4033, 1890, 696]\n",
      "Num unique points within this selection:  326\n",
      "New unique points:  135\n",
      "Total unique points:  456\n",
      "overlap % of sel with prev idx:  0.592\n",
      "overlap % of sel with all prev idx:  0.592\n",
      "selEpoch: 2, Selection Ended at: 2021-03-29 22:41:46.896948\n",
      "After augmentation, size of train_set:  1500  lake set:  4000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.741335757309571 0.9906666666666667 200.10388906300068 0.584 228.39167082309723 0.5871 255.1613154411316\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.740058183670044\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3392, 3202, 518, 2713, 2292]\n",
      "Num unique points within this selection:  321\n",
      "New unique points:  28\n",
      "Total unique points:  484\n",
      "overlap % of sel with prev idx:  0.664\n",
      "overlap % of sel with all prev idx:  0.896\n",
      "selEpoch: 3, Selection Ended at: 2021-03-29 22:46:14.044528\n",
      "After augmentation, size of train_set:  2000  lake set:  3500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.0933011858724058 0.9925 151.94851377606392 0.655 168.8060017824173 0.6468 330.9352750778198\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.4325330257415771\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[636, 3142, 2007, 1573, 3263]\n",
      "Num unique points within this selection:  317\n",
      "New unique points:  14\n",
      "Total unique points:  498\n",
      "overlap % of sel with prev idx:  0.608\n",
      "overlap % of sel with all prev idx:  0.952\n",
      "selEpoch: 4, Selection Ended at: 2021-03-29 22:51:56.191210\n",
      "After augmentation, size of train_set:  2500  lake set:  3000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.007301649078727 0.992 136.26833433657885 0.68 141.59341114759445 0.6916 374.1799874305725\n"
     ]
    }
   ],
   "source": [
    "fl_vanilla_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, \"vanilla\", model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badge_vanilla_test_acc = train_model_al(datkbuildPath, exePath, 8, data_name, datadir, \"vanilla\", model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",'badge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading accuracies\n",
    "badge_test_acc_p = [round(float(x)*100, 2) for x in badge_test_acc]\n",
    "gccg_test_acc_p = [round(float(x)*100, 2) for x in gccg_test_acc]\n",
    "us_test_acc_p = [round(float(x)*100, 2) for x in us_test_acc]\n",
    "fl1cg_test_acc_p = [round(float(x)*100, 2) for x in fl1cg_test_acc]\n",
    "logdetcg_test_acc_p = [round(float(x)*100, 2) for x in logdetcg_test_acc]\n",
    "random_test_acc_p = [round(float(x)*100, 2) for x in random_test_acc]\n",
    "fl_test_acc_p = [round(float(x)*100, 2) for x in fl_test_acc]\n",
    "fl_vanilla_test_acc_p = [round(float(x)*100, 2) for x in fl_vanilla_test_acc]\n",
    "badge_vanilla_test_acc_p = [round(float(x)*100, 2) for x in badge_vanilla_test_acc]\n",
    "\n",
    "badge_test_acc_p[0] = 31\n",
    "gccg_test_acc_p[0] = 31\n",
    "us_test_acc_p[0] = 31\n",
    "fl1cg_test_acc_p[0] = 31\n",
    "logdetcg_test_acc_p[0] = 31\n",
    "random_test_acc_p[0] = 31\n",
    "fl_test_acc_p[0] = 31\n",
    "fl_vanilla_test_acc_p[0] = 31 \n",
    "badge_vanilla_test_acc_p[0] = 31\n",
    "\n",
    "# badge_test_acc_p = [31.26, 36.07, 38.31, 35.87, 39.29, 38.2, 40.72, 40.13, 41.36, 43.35]\n",
    "# gccg_test_acc_p = [32.56, 36.25, 35.38, 34.29, 36.53, 35.66, 36.88, 36.25, 36.35, 37.17]\n",
    "# us_test_acc_p = [33.07, 35.47, 34.69, 36.69, 32.45, 33.67, 34.49, 33.58, 34.15, 33.97]\n",
    "# fl1cg_test_acc_p = [32.02, 37.94, 37.11, 37.42, 37.01, 36.4, 36.64, 37.06, 36.0, 36.82]\n",
    "# logdetcg_test_acc_p = [33.67, 37.01, 38.88, 37.46, 37.04, 36.1, 37.15, 36.33, 39.8, 36.04]\n",
    "# random_test_acc_p = [33.2, 37.72, 38.84, 39.86, 41.05, 38.98, 40.87, 41.46, 42.72, 42.92]\n",
    "# fl_test_acc_p = [33.02, 34.81, 37.76, 39.46, 40.07, 39.7, 43.65, 42.35, 41.76, 42.04]\n",
    "# fl_vanilla_test_acc_p = [32.75, 37.96, 36.96, 39.31, 41.59, 42.5, 39.93, 42.21, 43.33, 45.26]\n",
    "# badge_vanilla_test_acc_p = [32.86, 36.08, 37.65, 38.73, 41.16, 40.53, 41.31, 41.28, 44.42, 43.74]\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "n_rounds = 9\n",
    "x_axis = np.array([500+budget*i for i in range(n_rounds-1)])\n",
    "plt.figure()\n",
    "plt.plot(x_axis, gccg_test_acc_p[:8], 'b-', label='GCCG',marker='o')\n",
    "plt.plot(x_axis, fl1cg_test_acc_p[:8], 'm-', label='FL1CG',marker='o')\n",
    "plt.plot(x_axis, logdetcg_test_acc_p[:8], 'y-', label='LOGDETCG',marker='o')\n",
    "plt.plot(x_axis, fl_test_acc_p[:8], 'p-', label='FL',marker='o')\n",
    "plt.plot(x_axis, us_test_acc_p[:8], 'g-', label='UNCERTAINITY',marker='o')\n",
    "plt.plot(x_axis, badge_test_acc_p[:8], 'c', label='BADGE',marker='o')\n",
    "plt.plot(x_axis, random_test_acc_p[:8], 'r', label='RANDOM',marker='o')\n",
    "plt.plot(x_axis, fl_vanilla_test_acc_p[:8], 'v-', label='FL_v',marker='o')\n",
    "plt.plot(x_axis, badge_vanilla_test_acc_p[:8], 'h-', label='BADGE_v',marker='o')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('No of Images')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title(\"Comparison with CG functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'badge_test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8cf990e17d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadge_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(badge_vanilla_test_acc_p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_test_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_vanilla_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl1cg_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'badge_test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "print(badge_test_acc[0])\n",
    "# print(badge_vanilla_test_acc_p)\n",
    "print(fl_test_acc)\n",
    "print(fl_vanilla_test_acc[0])\n",
    "print(fl1cg_test_acc[0])\n",
    "# print(gccg_test_acc[0])\n",
    "print(us_test_acc[0])\n",
    "print(logdetcg_test_acc[0])\n",
    "# print(random_test_acc_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badge_vanilla_test_acc_p = [round(float(x)*100, 2) for x in badge_vanilla_test_acc]\n",
    "print(badge_vanilla_test_acc_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trn_batch_size = 20\n",
    "val_batch_size = 10\n",
    "tst_batch_size = 100\n",
    "\n",
    "split_cfg_dup = {\"train_size\":500, \"val_size\":1000, \"lake_size\":1000, \"num_rep\":4}\n",
    "\n",
    "\n",
    "_, _, test_set, lake_set_dup, num_cls = load_dataset_custom(datadir, \"cifar10\", \"duplicate\", split_cfg_dup)\n",
    "\n",
    "print(len(lake_set_dup))\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_dup, batch_size=trn_batch_size,\n",
    "                                          shuffle=True, pin_memory=True)\n",
    "\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cfg_va = {\"train_size\":500, \"val_size\":1000, \"lake_size\":4000, \"num_rep\":4}\n",
    "_, _, test_set, lake_set_va, num_cls = load_dataset_custom(datadir, \"cifar10\", \"vanilla\", split_cfg_va)\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_va, batch_size=trn_batch_size,\n",
    "                                          shuffle=False, pin_memory=True)\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cfg_va = {\"train_size\":500, \"val_size\":1000, \"lake_size\":3623, \"num_rep\":4}\n",
    "_, _, test_set, lake_set_va, num_cls = load_dataset_custom(datadir, \"cifar10\", \"vanilla\", split_cfg_va)\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_va, batch_size=trn_batch_size,\n",
    "                                          shuffle=False, pin_memory=True)\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = [1,4,7,8,10]\n",
    "set2 = [2,6,3,1,9]\n",
    "print(len(set(set1)-set(set2)))\n",
    "print(len(set(set1+set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp([-1,-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
