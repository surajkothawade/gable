{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from cords.cords.selectionstrategies.supervisedlearning import DataSelectionStrategy\n",
    "from cords.cords.utils.models import ResNet18\n",
    "from gable.gable.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "# for cuda\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class custom_subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "        labels(sequence) : targets as required for the indices. will be the same length as indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, labels):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.targets = labels.type(torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target = self.targets[idx]\n",
    "        return (image, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device):\n",
    "    if name == 'ResNet18':\n",
    "        model = ResNet18(num_cls)\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def kernel(x, y, measure=\"cosine\", exp=2):\n",
    "    if(measure==\"eu_sim\"):\n",
    "        lam = 0.25\n",
    "        dist = pairwise_distances(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = max(dist.ravel()) - dist\n",
    "        sim *= lam\n",
    "        sim = np.exp(sim)\n",
    "#         n = x.size(0)\n",
    "#         m = y.size(0)\n",
    "#         d = x.size(1)\n",
    "#         x = x.unsqueeze(1).expand(n, m, d)\n",
    "#         y = y.unsqueeze(0).expand(n, m, d)\n",
    "#         dist = torch.pow(x - y, exp).sum(2)\n",
    "#         const = torch.max(dist).item()\n",
    "#         sim = (const - dist)\n",
    "    \n",
    "        #dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))\n",
    "    if(measure==\"cosine\"):\n",
    "        sim = cosine_similarity(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = np.exp(sim)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def save_kernel_hdf5(lake_kernel, lake_target_kernel, target_kernel=[], numpy=True):\n",
    "    if(not(numpy)):\n",
    "        lake_kernel = lake_kernel.cpu().numpy()\n",
    "    with h5py.File(\"lake_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_target_kernel = lake_target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"lake_target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_target_kernel)\n",
    "    if(not(numpy)):\n",
    "        target_kernel = target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=target_kernel)\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "    saveDir = os.path.join(saveDir, prefix)\n",
    "    if(not(os.path.exists(saveDir))):\n",
    "        os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    class_err_log = []\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        print(\"val, test error% for class \", i, \" : \", round((len(val_err_class_idx)/len(val_class_idxs))*100,2), round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        class_err_log.append(\"val, test error% for class \"+ str(i) + \" : \"+ str(round((len(val_err_class_idx)/len(val_class_idxs))*100,2)) + \", \" + str(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)))\n",
    "        tst_err_log.append(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        \n",
    "    return tst_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = custom_subset(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = custom_subset(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    remain_true_lake_set = custom_subset(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "    assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    return aug_train_set, remain_lake_set, remain_true_lake_set, subset\n",
    "                        \n",
    "def getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def getSMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, sf):\n",
    "    if(sf==\"fl1mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\")\n",
    "    elif(sf == \"logdetmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \"  -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\") + \" -queryqueryKernelFile \" + os.path.join(hdf5Path, \"target_kernel.hdf5\")\n",
    "    elif(sf==\"fl2mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gcmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gccg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl1cg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"logdetcg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\") + \" -privateprivateKernelFile \" + os.path.join(hdf5Path, \"target_kernel.hdf5\")\n",
    "    elif(sf==\"fl\" or sf==\"logdet\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\")\n",
    "    elif(sf ==\"gc\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\")\n",
    "    print(\"Executing SIM command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "# def getDuplicates(subset):\n",
    "    \n",
    "#check overlap with prev selections\n",
    "def check_overlap(prev_idx, prev_idx_hist, idx):\n",
    "    prev_idx = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in prev_idx ]\n",
    "    prev_idx_hist = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in prev_idx_hist]\n",
    "    idx = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in idx]\n",
    "    # overlap = set(prev_idx).intersection(set(idx))\n",
    "    overlap = [value for value in idx if value in prev_idx] \n",
    "    # overlap_hist = set(prev_idx_hist).intersection(set(idx))\n",
    "    overlap_hist = [value for value in idx if value in prev_idx_hist]\n",
    "    new_points = set(idx) - set(prev_idx_hist)\n",
    "    total_unique_points = set(idx+prev_idx_hist)\n",
    "    print(\"Num unique points within this selection: \", len(set(idx)))\n",
    "    print(\"New unique points: \", len(new_points))\n",
    "    print(\"Total unique points: \", len(total_unique_points))\n",
    "    print(\"overlap % of sel with prev idx: \", len(overlap)/len(idx))\n",
    "    print(\"overlap % of sel with all prev idx: \", len(overlap_hist)/len(idx))\n",
    "#     return len(overlap)/len(idx), len(overlap_hist)/len(idx)\n",
    "    return len(total_unique_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "datadir = 'data/'\n",
    "data_name = 'cifar100'\n",
    "num_cls=100\n",
    "fraction = float(0.1)\n",
    "budget=500\n",
    "num_epochs = int(10)\n",
    "num_rep = 10\n",
    "# feature='vanilla'\n",
    "feature = 'duplicate'\n",
    "# feature = 'classimb'\n",
    "num_runs = 1  # number of random runs\n",
    "learning_rate = 0.01\n",
    "model_name = 'ResNet18'\n",
    "magnification = 10\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# split_cfg = {\"num_cls_imbalance\":2, \"per_imbclass_train\":50, \"per_imbclass_val\":25, \"per_imbclass_lake\":150, \"per_class_train\":1000, \"per_class_val\":25, \"per_class_lake\":3000}\n",
    "split_cfg = {\"train_size\":500, \"val_size\":1000, \"lake_size\":5000, \"num_rep\":num_rep, \"lake_subset_repeat_size\":1000}\n",
    "datkbuildPath = \"/home/snk170001/bioml/dss/notebooks/datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "initModelPath = \"weights/cg_\" + data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"train_size\"])\n",
    "print(\"Using Device:\", device)\n",
    "doublePrecision = False\n",
    "linearLayer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distil.distil.active_learning_strategies import BADGE, EntropySampling, GLISTER\n",
    "from distil.distil.utils.DataHandler import DataHandler_CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    if(feature==\"classimb\"):\n",
    "        train_set, val_set, test_set, lake_set, imb_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        print(\"imbalanced classes are: \", imb_cls_idx)\n",
    "    if(feature==\"duplicate\" or feature==\"vanilla\"):\n",
    "        sel_cls_idx = None\n",
    "        if(strategy == \"SIM\" or strategy==\"random\"):\n",
    "            train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        elif(strategy==\"AL\"):\n",
    "            X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "        \n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    \n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "    print(\"Budget: \", bud)\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "    all_logs_dir = 'CG_active_learning_results_woVal/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "#     path_logfile = os.path.join(all_logs_dir, dataset_name + '.txt')\n",
    "#     logfile = open(path_logfile, 'w')\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \"feature\":feature, \"sel_func\":sf, \"sel_budget\":budget, \"num_selections\":num_epochs, \"model\":model_name, \"learning_rate\":learning_rate, \"setting\":split_cfg, \"test_acc\":[], \"num_unique_samples\":[], \"sel_cls_idx\":sel_cls_idx}\n",
    "\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device)\n",
    "    model1 = create_model(model_name, num_cls, device)\n",
    "    if(strategy == \"AL\"):\n",
    "        strategy_args = {'batch_size' : 100, 'lr':float(0.001)}\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"glister\"):\n",
    "            strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args, valid=False, typeOf='rand', lam=0.1, linear_layer=False)\n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "#     optimizer, scheduler = optimizer_with_scheduler(model, num_epochs, learning_rate)\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "    #overlap vars\n",
    "    prev_idx = []\n",
    "    prev_idx_hist = []\n",
    "    per_ep_overlap = []\n",
    "    overall_overlap = []\n",
    "    idx_tracker = np.array(list(range(len(lake_set))))\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training\")\n",
    "                for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    tst_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    tst_total += targets.size(0)\n",
    "                    tst_correct += predicted.eq(targets).sum().item()\n",
    "                tst_acc[i] = tst_correct / tst_total\n",
    "                res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "                continue\n",
    "        else:\n",
    "#             if(full_trn_acc[i-1] >= 0.99): #The model has already trained on the seed dataset\n",
    "            #use misclassifications on validation set as queries\n",
    "            #compute hypothesized labels\n",
    "            hyp_lake_labels = []\n",
    "            for batch_idx, (inputs, _) in enumerate(lakeloader):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                hyp_lake_labels += list(predicted)\n",
    "            print(len(hyp_lake_labels))\n",
    "            lake_set = custom_subset(lake_set, list(range(len(hyp_lake_labels))), torch.Tensor(hyp_lake_labels))\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append([\"epoch \"+str(i+1)]+tst_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                    miscls_set = getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx)\n",
    "                    misclsloader = torch.utils.data.DataLoader(miscls_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, misclsloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                elif(sf.endswith(\"cg\")): #atleast one selection must be done for private set in cond gain functions\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, privateSetloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        #compute subset with private set a NULL\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                else:\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                start_time = time.time()\n",
    "                cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "                clone_dict = copy.deepcopy(model.state_dict())\n",
    "                #update the selection strategy model with new params for gradient computation\n",
    "                setf_model.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mi\")): #SMI functions need the target set gradients\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                    if(doublePrecision):\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    else:\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                    numQueryPrivate = train_val_kernel.shape[1]\n",
    "                elif(sf.endswith(\"cg\")):\n",
    "                    if(len(private_set)!=0):\n",
    "                        setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                        print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                        print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                        if(doublePrecision):\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_private_kernel\n",
    "                        else:\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_private_kernel\n",
    "                        numQueryPrivate = train_val_kernel.shape[1]\n",
    "                    else:\n",
    "#                         assert(((i + 1)/select_every)==1)\n",
    "                        setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        train_val_kernel = []\n",
    "                        numQueryPrivate = 0\n",
    "                else: # For other submodular functions needing only image kernel\n",
    "                    setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                    train_val_kernel = []\n",
    "                    numQueryPrivate = 0\n",
    "\n",
    "                kernel_time = time.time()\n",
    "                if(doublePrecision):\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.grads_per_elem.double()) #img_img_kernel\n",
    "                else:\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem, setf_model.grads_per_elem) #img_img_kernel\n",
    "\n",
    "                if(sf==\"logdetmi\" or sf==\"logdetcg\"):\n",
    "                    if(len(private_set)!=0):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                    else:\n",
    "                        val_kernel = []\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel, val_kernel)\n",
    "                else:\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel)\n",
    "                print(\"kernel compute time: \", time.time()-kernel_time)\n",
    "                #call the c++ exec to read kernel and compute subset of selected minibatches\n",
    "                subset = getSMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQueryPrivate), sf)\n",
    "                print(subset[:5])\n",
    "                model.load_state_dict(cached_state_dict)\n",
    "                if(sf.endswith(\"cg\")): #for first selection\n",
    "                    if(len(private_set)==0):\n",
    "                        private_set = custom_subset(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "                    else:\n",
    "                        private_set = getPrivateSet(true_lake_set, subset, private_set)\n",
    "                    print(\"size of private set: \", len(private_set))\n",
    "\n",
    "    #           temp = np.array(list(trainloader.batch_sampler))[subset] #if per batch\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                strategy_sel.update_model(model)\n",
    "                if(sf==\"badge\" or sf==\"glister\"):\n",
    "                    subset = strategy_sel.select(budget)\n",
    "                if(sf==\"us\"):\n",
    "                    subset = list(strategy_sel.select(budget).cpu().numpy())\n",
    "                print(len(subset), \" samples selected\")\n",
    "                X_tr = np.concatenate((X_tr, X_unlabeled[subset]), axis=0)\n",
    "                X_unlabeled = np.delete(X_unlabeled, subset, axis = 0)\n",
    "                y_tr = np.concatenate((y_tr, y_unlabeled[subset]), axis = 0)\n",
    "                y_unlabeled = np.delete(y_unlabeled, subset, axis = 0)\n",
    "                strategy_sel.update_data(X_tr, y_tr, X_unlabeled)\n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            if(i>0):\n",
    "                curr_unique_points = check_overlap(prev_idx, prev_idx_hist, list(idx_tracker[subset]))\n",
    "                res_dict[\"num_unique_samples\"].append(curr_unique_points)\n",
    "#                 per_ep_overlap.append(per_ep)\n",
    "#                 overall_overlap.append(overall)\n",
    "\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "\n",
    "            #augment the train_set with selected indices from the lake\n",
    "            train_set, lake_set, true_lake_set, subset = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "            assert(len(lake_set)==len(true_lake_set))\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set))\n",
    "            prev_idx = list(idx_tracker[subset])\n",
    "            prev_idx_hist += list(idx_tracker[subset])\n",
    "            idx_tracker = np.delete(idx_tracker, subset, axis=0)\n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            assert(len(idx_tracker)==len(lake_set))\n",
    "#             model =  model.apply(weight_reset).cuda()\n",
    "            model = create_model(model_name, num_cls, device)\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<150):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # print(batch_idx)\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "#                 if(i == (num_epochs-1)):\n",
    "            final_val_predictions += list(predicted.cpu().numpy())\n",
    "            final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "            # sys.exit()\n",
    "\n",
    "#         if((val_correct/val_total) > best_val_acc):\n",
    "        final_tst_predictions = []\n",
    "        final_tst_classifications = []\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "#             if((val_correct/val_total) > best_val_acc):\n",
    "#                 if(i == (num_epochs-1)):\n",
    "            final_tst_predictions += list(predicted.cpu().numpy())\n",
    "            final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "#         if((val_correct/val_total) > best_val_acc):\n",
    "#             best_val_acc = (val_correct/val_total)\n",
    "        val_acc[i] = val_correct / val_total\n",
    "        tst_acc[i] = tst_correct / tst_total\n",
    "        val_losses[i] = val_loss\n",
    "        fulltrn_losses[i] = full_trn_loss\n",
    "        tst_losses[i] = tst_loss\n",
    "        full_val_acc = list(np.array(val_acc))\n",
    "        full_timing = list(np.array(timing))\n",
    "        res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "        print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_class_err_idxs = find_err_per_class(test_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append([\"final\"]+tst_err_log)\n",
    "        print(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    return tst_acc, prev_idx_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n",
    "badge_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL us\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/us/500/1\n",
      "cifar100_duplicate_AL_us_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "500  samples selected\n",
      "Num unique points within this selection:  151\n",
      "New unique points:  151\n",
      "Total unique points:  151\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-04 01:24:02.286599\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.240939344279468 0.997 658.2571821212769 0.089 711.1595673561096 0.0882 74.23673248291016\n",
      "AL epoch:  2\n",
      "500  samples selected\n",
      "Num unique points within this selection:  189\n",
      "New unique points:  189\n",
      "Total unique points:  340\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 2, Selection Ended at: 2021-04-04 01:25:26.401970\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.16006351262331 0.9913333333333333 615.3531317710876 0.128 664.870491027832 0.1169 102.37509393692017\n",
      "AL epoch:  3\n",
      "500  samples selected\n",
      "Num unique points within this selection:  150\n",
      "New unique points:  150\n",
      "Total unique points:  490\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 3, Selection Ended at: 2021-04-04 01:27:17.894467\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.274039593525231 0.9905 603.4893250465393 0.128 654.3305010795593 0.1185 132.53785228729248\n",
      "AL epoch:  4\n",
      "500  samples selected\n",
      "Num unique points within this selection:  167\n",
      "New unique points:  167\n",
      "Total unique points:  657\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 4, Selection Ended at: 2021-04-04 01:29:39.379376\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.701643437845632 0.9924 588.9568116664886 0.137 622.3361401557922 0.1362 156.42200803756714\n",
      "AL epoch:  5\n",
      "500  samples selected\n",
      "Num unique points within this selection:  178\n",
      "New unique points:  177\n",
      "Total unique points:  834\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.004\n",
      "selEpoch: 5, Selection Ended at: 2021-04-04 01:32:24.683909\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.467213258612901 0.991 570.7126858234406 0.14 613.1549320220947 0.1392 184.84537363052368\n",
      "AL epoch:  6\n",
      "500  samples selected\n",
      "Num unique points within this selection:  167\n",
      "New unique points:  167\n",
      "Total unique points:  1001\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 6, Selection Ended at: 2021-04-04 01:35:39.292065\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.061284890631214 0.9928571428571429 550.5096302032471 0.155 584.7115941047668 0.1446 214.84882140159607\n",
      "AL epoch:  7\n",
      "500  samples selected\n",
      "Num unique points within this selection:  174\n",
      "New unique points:  172\n",
      "Total unique points:  1173\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.008\n",
      "selEpoch: 7, Selection Ended at: 2021-04-04 01:39:23.256998\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.889599953661673 0.99575 521.8825106620789 0.166 558.6890826225281 0.1642 241.09412050247192\n",
      "AL epoch:  8\n",
      "500  samples selected\n",
      "Num unique points within this selection:  198\n",
      "New unique points:  197\n",
      "Total unique points:  1370\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.008\n",
      "selEpoch: 8, Selection Ended at: 2021-04-04 01:43:32.510014\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.796705823158845 0.9908888888888889 529.4440639019012 0.151 546.1296682357788 0.161 276.8809952735901\n",
      "AL epoch:  9\n",
      "500  samples selected\n",
      "Num unique points within this selection:  165\n",
      "New unique points:  164\n",
      "Total unique points:  1534\n",
      "overlap % of sel with prev idx:  0.014\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 9, Selection Ended at: 2021-04-04 01:48:18.163846\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.7739217802882195 0.9958 499.0795795917511 0.178 509.29580068588257 0.1926 315.531512260437\n"
     ]
    }
   ],
   "source": [
    "us_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL glister\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/glister/500/1\n",
      "cifar100_duplicate_AL_glister_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snk170001/bioml/dss/notebooks/distil/distil/active_learning_strategies/strategy.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_trn = torch.tensor(Y[idxs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([100, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [100, 1] doesn't match the broadcast shape [100, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ee48ee3860ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglister_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_al\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatkbuildPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AL\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"glister\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-087c9ff3db3d>\u001b[0m in \u001b[0;36mtrain_model_al\u001b[0;34m(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mstrategy_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"badge\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"glister\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbudget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"us\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbudget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bioml/dss/notebooks/distil/distil/active_learning_strategies/glister.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, budget)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_per_element_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grads_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mnumSelected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bioml/dss/notebooks/distil/distil/active_learning_strategies/glister.py\u001b[0m in \u001b[0;36m_update_grads_val\u001b[0;34m(self, grads_currX, first_init)\u001b[0m\n\u001b[1;32m    180\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads_val_curr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml0_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads_val_curr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml0_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [100, 1] doesn't match the broadcast shape [100, 100]"
     ]
    }
   ],
   "source": [
    "glister_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"glister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM gccg\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results_woVal/cifar10/duplicate/gccg/500/1\n",
      "cifar10_duplicate_SIM_gccg_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "14000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.694102048873901\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 0 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[1436, 11369, 4467, 3503, 8035]\n",
      "size of private set:  500\n",
      "Num unique points within this selection:  292\n",
      "New unique points:  292\n",
      "Total unique points:  292\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-10 10:38:10.872724\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.071206506807357 0.99 289.3527777791023 0.464 308.18728256225586 0.4711 167.04383039474487\n",
      "AL epoch:  2\n",
      "13500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13500, 10])\n",
      "val minibatch gradients shape  torch.Size([500, 10])\n",
      "kernel compute time:  7.403152704238892\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[3521, 7898, 5820, 851, 10060]\n",
      "size of private set:  1000\n",
      "Num unique points within this selection:  275\n",
      "New unique points:  272\n",
      "Total unique points:  564\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.012\n",
      "selEpoch: 2, Selection Ended at: 2021-04-10 10:42:19.496600\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.8635153230279684 0.9933333333333333 238.6865655183792 0.494 271.70175290107727 0.4987 197.92495679855347\n",
      "AL epoch:  3\n",
      "13000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13000, 10])\n",
      "val minibatch gradients shape  torch.Size([1000, 10])\n",
      "kernel compute time:  7.109576940536499\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 1000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[12717]\n",
      "size of private set:  1001\n",
      "Num unique points within this selection:  1\n",
      "New unique points:  1\n",
      "Total unique points:  565\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 3, Selection Ended at: 2021-04-10 10:46:27.563932\n",
      "After augmentation, size of train_set:  1501  lake set:  12999\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.089557373255957 0.9906728847435043 284.5171053111553 0.497 311.1790130138397 0.4932 225.93540120124817\n",
      "AL epoch:  4\n",
      "12999\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12999, 10])\n",
      "val minibatch gradients shape  torch.Size([1001, 10])\n",
      "kernel compute time:  7.058370351791382\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 1001 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[9553]\n",
      "size of private set:  1002\n",
      "Num unique points within this selection:  1\n",
      "New unique points:  1\n",
      "Total unique points:  566\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 4, Selection Ended at: 2021-04-10 10:51:13.478097\n",
      "After augmentation, size of train_set:  1502  lake set:  12998\n",
      "Selection Epoch  4  Training epoch [ 44 ]  Training Acc:  0.9047936085219707\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5cfef738f215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgccg_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_al\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatkbuildPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SIM\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gccg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7f6eacb3a47f>\u001b[0m in \u001b[0;36mtrain_model_al\u001b[0;34m(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;31m#             scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dependencies/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dependencies/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    110\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gccg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL1CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl1cg\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results_woVal/cifar100/duplicate/fl1cg/500/2\n",
      "cifar100_duplicate_SIM_fl1cg_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "14000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  8.054877996444702\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 0 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[6348, 12432, 13673, 12045, 8999]\n",
      "size of private set:  500\n",
      "Num unique points within this selection:  264\n",
      "New unique points:  264\n",
      "Total unique points:  264\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-10 16:04:15.642687\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.6336967828683555 0.997 628.201961517334 0.103 679.6906609535217 0.097 86.20404958724976\n",
      "AL epoch:  2\n",
      "13500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13500, 100])\n",
      "val minibatch gradients shape  torch.Size([500, 100])\n",
      "kernel compute time:  7.549072980880737\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[13013, 13014, 13015, 13016, 13017]\n",
      "size of private set:  1000\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  496\n",
      "Total unique points:  760\n",
      "overlap % of sel with prev idx:  0.004\n",
      "overlap % of sel with all prev idx:  0.004\n",
      "selEpoch: 2, Selection Ended at: 2021-04-10 16:06:35.938322\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.853348811157048 0.9926666666666667 569.0106327533722 0.131 598.6313042640686 0.1561 120.06070566177368\n",
      "AL epoch:  3\n",
      "13000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13000, 100])\n",
      "val minibatch gradients shape  torch.Size([1000, 100])\n",
      "kernel compute time:  7.219408988952637\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 1000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[12513, 12514, 12515, 12516, 12517]\n",
      "size of private set:  1500\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  494\n",
      "Total unique points:  1254\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.012\n",
      "selEpoch: 3, Selection Ended at: 2021-04-10 16:09:32.200993\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.674508307129145 0.9935 522.0281326770782 0.158 550.3274331092834 0.1655 187.54233622550964\n",
      "AL epoch:  4\n",
      "12500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12500, 100])\n",
      "val minibatch gradients shape  torch.Size([1500, 100])\n",
      "kernel compute time:  6.859491348266602\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 1500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[12013, 12014, 12015, 12016, 12017]\n",
      "size of private set:  2000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  497\n",
      "Total unique points:  1751\n",
      "overlap % of sel with prev idx:  0.006\n",
      "overlap % of sel with all prev idx:  0.006\n",
      "selEpoch: 4, Selection Ended at: 2021-04-10 16:13:35.644468\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.692299484275281 0.9908 513.1702976226807 0.2 550.2669086456299 0.1989 272.18541264533997\n",
      "AL epoch:  5\n",
      "12000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12000, 100])\n",
      "val minibatch gradients shape  torch.Size([2000, 100])\n",
      "kernel compute time:  6.400182485580444\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 2000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[11513, 11514, 11515, 11516, 11517]\n",
      "size of private set:  2500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  493\n",
      "Total unique points:  2244\n",
      "overlap % of sel with prev idx:  0.014\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 5, Selection Ended at: 2021-04-10 16:19:04.736964\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.574034636840224 0.994 443.86816811561584 0.226 466.7226846218109 0.2539 324.42650628089905\n",
      "AL epoch:  6\n",
      "11500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11500, 100])\n",
      "val minibatch gradients shape  torch.Size([2500, 100])\n",
      "kernel compute time:  6.180262804031372\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 2500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[11013, 11014, 11015, 11016, 11017]\n",
      "size of private set:  3000\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  495\n",
      "Total unique points:  2739\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.01\n",
      "selEpoch: 6, Selection Ended at: 2021-04-10 16:25:26.914018\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.352851596660912 0.9942857142857143 449.0076951980591 0.251 462.51014828681946 0.2674 388.0721333026886\n",
      "AL epoch:  7\n",
      "11000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11000, 100])\n",
      "val minibatch gradients shape  torch.Size([3000, 100])\n",
      "kernel compute time:  5.9094462394714355\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 3000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[10513, 10514, 10515, 10516, 10517]\n",
      "size of private set:  3500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  495\n",
      "Total unique points:  3234\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.01\n",
      "selEpoch: 7, Selection Ended at: 2021-04-10 16:32:56.421704\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.919254872016609 0.99325 403.5112873315811 0.275 426.7444579601288 0.2958 446.8390431404114\n",
      "AL epoch:  8\n",
      "10500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10500, 100])\n",
      "val minibatch gradients shape  torch.Size([3500, 100])\n",
      "kernel compute time:  5.626122951507568\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 3500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[10013, 10014, 10015, 10016, 10017]\n",
      "size of private set:  4000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  492\n",
      "Total unique points:  3726\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.016\n",
      "selEpoch: 8, Selection Ended at: 2021-04-10 16:41:27.637445\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 12.028131071478128 0.9902222222222222 393.31665575504303 0.308 402.63274908065796 0.3264 549.6608009338379\n",
      "AL epoch:  9\n",
      "10000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10000, 100])\n",
      "val minibatch gradients shape  torch.Size([4000, 100])\n",
      "kernel compute time:  5.180488586425781\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 4000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[9513, 9514, 9515, 9516, 9517]\n",
      "size of private set:  4500\n",
      "Num unique points within this selection:  481\n",
      "New unique points:  476\n",
      "Total unique points:  4202\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.01\n",
      "selEpoch: 9, Selection Ended at: 2021-04-10 16:51:39.676021\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.683835399337113 0.9916 374.2288453578949 0.328 390.4842083454132 0.3367 574.8111021518707\n"
     ]
    }
   ],
   "source": [
    "fl1cg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl1cg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDETCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdetcg\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results_woVal/cifar100/duplicate/logdetcg/500/2\n",
      "cifar100_duplicate_SIM_logdetcg_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "14000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  17.524158000946045\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 0 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[2331, 4722, 9634, 9571, 9631]\n",
      "size of private set:  500\n",
      "Num unique points within this selection:  431\n",
      "New unique points:  431\n",
      "Total unique points:  431\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-10 17:02:32.528265\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.2026773169636726 0.991 604.4023082256317 0.121 653.193941116333 0.1239 96.24961447715759\n",
      "AL epoch:  2\n",
      "13500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13500, 100])\n",
      "val minibatch gradients shape  torch.Size([500, 100])\n",
      "kernel compute time:  3.9941787719726562\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[13013, 13014, 13015, 13016, 13017]\n",
      "size of private set:  1000\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  497\n",
      "Total unique points:  928\n",
      "overlap % of sel with prev idx:  0.002\n",
      "overlap % of sel with all prev idx:  0.002\n",
      "selEpoch: 2, Selection Ended at: 2021-04-10 17:05:10.311060\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.1685490990057588 0.996 548.9077960252762 0.171 586.6838140487671 0.1579 158.04419207572937\n",
      "AL epoch:  3\n",
      "13000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13000, 100])\n",
      "val minibatch gradients shape  torch.Size([1000, 100])\n",
      "kernel compute time:  3.8693249225616455\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 1000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[12513, 12514, 12515, 12516, 12517]\n",
      "size of private set:  1500\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  492\n",
      "Total unique points:  1420\n",
      "overlap % of sel with prev idx:  0.014\n",
      "overlap % of sel with all prev idx:  0.016\n",
      "selEpoch: 3, Selection Ended at: 2021-04-10 17:09:17.357929\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.9281935496255755 0.992 504.9408686161041 0.193 558.8992147445679 0.1856 230.7625596523285\n",
      "AL epoch:  4\n",
      "12500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12500, 100])\n",
      "val minibatch gradients shape  torch.Size([1500, 100])\n",
      "kernel compute time:  3.726047992706299\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 1500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[12013, 12014, 12015, 12016, 12017]\n",
      "size of private set:  2000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  495\n",
      "Total unique points:  1915\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.01\n",
      "selEpoch: 4, Selection Ended at: 2021-04-10 17:15:26.997342\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.830914715770632 0.9936 490.57102823257446 0.234 527.4489045143127 0.2112 276.0806837081909\n",
      "AL epoch:  5\n",
      "12000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12000, 100])\n",
      "val minibatch gradients shape  torch.Size([2000, 100])\n",
      "kernel compute time:  3.7434566020965576\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 2000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[11513, 11514, 11515, 11516, 11517]\n",
      "size of private set:  2500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  493\n",
      "Total unique points:  2408\n",
      "overlap % of sel with prev idx:  0.01\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 5, Selection Ended at: 2021-04-10 17:23:20.507415\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.37107607210055 0.991 438.08856666088104 0.254 472.8110342025757 0.2569 350.180783033371\n",
      "AL epoch:  6\n",
      "11500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11500, 100])\n",
      "val minibatch gradients shape  torch.Size([2500, 100])\n",
      "kernel compute time:  3.641486167907715\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 2500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[11013, 11014, 11015, 11016, 11017]\n",
      "size of private set:  3000\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  492\n",
      "Total unique points:  2900\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.016\n",
      "selEpoch: 6, Selection Ended at: 2021-04-10 17:33:40.876962\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.73489628592506 0.9902857142857143 422.2162597179413 0.266 449.2084047794342 0.2846 402.0306613445282\n",
      "AL epoch:  7\n",
      "11000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11000, 100])\n",
      "val minibatch gradients shape  torch.Size([3000, 100])\n",
      "kernel compute time:  3.5421371459960938\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 3000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10513, 10514, 10515, 10516, 10517]\n",
      "size of private set:  3500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  493\n",
      "Total unique points:  3393\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 7, Selection Ended at: 2021-04-10 17:46:21.275482\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.433394817169756 0.99325 382.7219978570938 0.309 409.20942187309265 0.3191 463.1431243419647\n",
      "AL epoch:  8\n",
      "10500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10500, 100])\n",
      "val minibatch gradients shape  torch.Size([3500, 100])\n",
      "kernel compute time:  3.391935110092163\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 3500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[10013, 10014, 10015, 10016, 10017]\n",
      "size of private set:  4000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  493\n",
      "Total unique points:  3886\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 8, Selection Ended at: 2021-04-10 18:01:43.739385\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.280881775775924 0.9933333333333333 364.73486959934235 0.312 389.80744647979736 0.3344 623.0360462665558\n",
      "AL epoch:  9\n",
      "10000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10000, 100])\n",
      "val minibatch gradients shape  torch.Size([4000, 100])\n",
      "kernel compute time:  3.4157614707946777\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 4000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[9513, 9514, 9515, 9516, 9517]\n",
      "size of private set:  4500\n",
      "Num unique points within this selection:  475\n",
      "New unique points:  466\n",
      "Total unique points:  4352\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.034\n",
      "selEpoch: 9, Selection Ended at: 2021-04-10 18:21:21.063905\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.43106986535713 0.9908 367.07449066638947 0.329 383.7601878643036 0.3434 616.2850115299225\n"
     ]
    }
   ],
   "source": [
    "logdetcg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'logdetcg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results_woVal/cifar10/duplicate/fl/500/2\n",
      "cifar10_duplicate_SIM_fl_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "14000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  18.145132303237915\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[6814, 12456, 12889, 7356, 4686]\n",
      "Num unique points within this selection:  441\n",
      "New unique points:  441\n",
      "Total unique points:  441\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-10 05:23:45.718884\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.7263385225087404 0.992 249.95113703608513 0.495 277.82412576675415 0.4964 144.30786323547363\n",
      "AL epoch:  2\n",
      "13500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.298890829086304\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[12973, 4052, 11946, 9392, 330]\n",
      "Num unique points within this selection:  437\n",
      "New unique points:  330\n",
      "Total unique points:  771\n",
      "overlap % of sel with prev idx:  0.258\n",
      "overlap % of sel with all prev idx:  0.258\n",
      "selEpoch: 2, Selection Ended at: 2021-04-10 05:28:19.088674\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.560766439884901 0.992 233.10959112644196 0.521 254.8431304693222 0.5221 205.30512499809265\n",
      "AL epoch:  3\n",
      "13000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.033515214920044\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3536, 4259, 11307, 10320, 5429]\n",
      "Num unique points within this selection:  427\n",
      "New unique points:  254\n",
      "Total unique points:  1025\n",
      "overlap % of sel with prev idx:  0.27\n",
      "overlap % of sel with all prev idx:  0.428\n",
      "selEpoch: 3, Selection Ended at: 2021-04-10 05:33:43.733578\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.6431667499709874 0.9955 193.13737926073372 0.563 208.391131401062 0.5783 279.54568433761597\n",
      "AL epoch:  4\n",
      "12500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.3342273235321045\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[734, 651, 2701, 9095, 9516]\n",
      "Num unique points within this selection:  429\n",
      "New unique points:  199\n",
      "Total unique points:  1224\n",
      "overlap % of sel with prev idx:  0.26\n",
      "overlap % of sel with all prev idx:  0.56\n",
      "selEpoch: 4, Selection Ended at: 2021-04-10 05:40:08.850503\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.4342861981131136 0.9928 200.43918212503195 0.607 213.7650889158249 0.607 345.4005129337311\n",
      "AL epoch:  5\n",
      "12000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.89043116569519\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[6129, 7276, 9696, 797, 1273]\n",
      "Num unique points within this selection:  425\n",
      "New unique points:  180\n",
      "Total unique points:  1404\n",
      "overlap % of sel with prev idx:  0.284\n",
      "overlap % of sel with all prev idx:  0.606\n",
      "selEpoch: 5, Selection Ended at: 2021-04-10 05:47:34.226447\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.349379066377878 0.9916666666666667 175.64180599153042 0.609 191.4618810415268 0.603 323.9938611984253\n",
      "AL epoch:  6\n",
      "11500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.653850317001343\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[7029, 9921, 362, 6613, 8049]\n",
      "Num unique points within this selection:  435\n",
      "New unique points:  153\n",
      "Total unique points:  1557\n",
      "overlap % of sel with prev idx:  0.274\n",
      "overlap % of sel with all prev idx:  0.666\n",
      "selEpoch: 6, Selection Ended at: 2021-04-10 05:54:27.120528\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.114744231919758 0.9937142857142857 195.20828711986542 0.62 198.62399101257324 0.6085 402.6337237358093\n",
      "AL epoch:  7\n",
      "11000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.933864116668701\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[671, 7867, 7975, 4390, 5824]\n",
      "Num unique points within this selection:  434\n",
      "New unique points:  146\n",
      "Total unique points:  1703\n",
      "overlap % of sel with prev idx:  0.246\n",
      "overlap % of sel with all prev idx:  0.694\n",
      "selEpoch: 7, Selection Ended at: 2021-04-10 06:02:40.036323\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.167494133929722 0.9925 166.92864950746298 0.616 184.00139391422272 0.6258 432.5574469566345\n",
      "AL epoch:  8\n",
      "10500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.463352918624878\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[10086, 5158, 7025, 6013, 9469]\n",
      "Num unique points within this selection:  434\n",
      "New unique points:  145\n",
      "Total unique points:  1848\n",
      "overlap % of sel with prev idx:  0.248\n",
      "overlap % of sel with all prev idx:  0.7\n",
      "selEpoch: 8, Selection Ended at: 2021-04-10 06:11:15.829465\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.279244801378809 0.9955555555555555 157.80533568561077 0.632 162.94224452972412 0.6542 552.143411397934\n",
      "AL epoch:  9\n",
      "10000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  18.41664457321167\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[9703, 3095, 3811, 1342, 3927]\n",
      "Num unique points within this selection:  431\n",
      "New unique points:  138\n",
      "Total unique points:  1986\n",
      "overlap % of sel with prev idx:  0.262\n",
      "overlap % of sel with all prev idx:  0.716\n",
      "selEpoch: 9, Selection Ended at: 2021-04-10 06:22:02.255741\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.774977863649838 0.9922 153.7864185348153 0.648 173.33020734786987 0.6473 538.4739992618561\n"
     ]
    }
   ],
   "source": [
    "fl_test_acc, selected_idx = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM gc\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results_woVal/cifar10/duplicate/gc/500/1\n",
      "cifar10_duplicate_SIM_gc_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "14000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.741007566452026\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[10714, 3020, 12098, 6634, 1134]\n",
      "Num unique points within this selection:  311\n",
      "New unique points:  311\n",
      "Total unique points:  311\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-10 06:31:52.895601\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.9246445114258677 0.991 277.0874629020691 0.469 316.6169831752777 0.4585 138.2511670589447\n",
      "AL epoch:  2\n",
      "13500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.400704383850098\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[7237, 3313, 5558, 2770, 3416]\n",
      "Num unique points within this selection:  306\n",
      "New unique points:  275\n",
      "Total unique points:  586\n",
      "overlap % of sel with prev idx:  0.164\n",
      "overlap % of sel with all prev idx:  0.164\n",
      "selEpoch: 2, Selection Ended at: 2021-04-10 06:34:54.969588\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.255592941539362 0.9933333333333333 263.97860473394394 0.48 292.2643518447876 0.5016 191.27092719078064\n",
      "AL epoch:  3\n",
      "13000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.850780010223389\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[11771, 10380, 513, 10843, 9948]\n",
      "Num unique points within this selection:  289\n",
      "New unique points:  244\n",
      "Total unique points:  830\n",
      "overlap % of sel with prev idx:  0.088\n",
      "overlap % of sel with all prev idx:  0.152\n",
      "selEpoch: 3, Selection Ended at: 2021-04-10 06:38:48.008624\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.2398528549820185 0.99 218.76564943790436 0.548 245.86097395420074 0.5441 231.67555785179138\n",
      "AL epoch:  4\n",
      "12500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.387898206710815\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[1737, 6687, 10361, 269, 11724]\n",
      "Num unique points within this selection:  318\n",
      "New unique points:  240\n",
      "Total unique points:  1070\n",
      "overlap % of sel with prev idx:  0.128\n",
      "overlap % of sel with all prev idx:  0.334\n",
      "selEpoch: 4, Selection Ended at: 2021-04-10 06:43:26.646392\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.295852335519157 0.9916 205.86919516324997 0.566 220.55667102336884 0.5696 288.04148721694946\n",
      "AL epoch:  5\n",
      "12000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.814180850982666\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[6398, 9093, 11035, 11033, 1605]\n",
      "Num unique points within this selection:  328\n",
      "New unique points:  255\n",
      "Total unique points:  1325\n",
      "overlap % of sel with prev idx:  0.128\n",
      "overlap % of sel with all prev idx:  0.33\n",
      "selEpoch: 5, Selection Ended at: 2021-04-10 06:48:53.892891\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.532676886883564 0.9923333333333333 209.88697834312916 0.556 220.6865062713623 0.5683 368.0031349658966\n",
      "AL epoch:  6\n",
      "11500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.481351613998413\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2956, 2113, 9890, 1011, 2884]\n",
      "Num unique points within this selection:  330\n",
      "New unique points:  222\n",
      "Total unique points:  1547\n",
      "overlap % of sel with prev idx:  0.132\n",
      "overlap % of sel with all prev idx:  0.402\n",
      "selEpoch: 6, Selection Ended at: 2021-04-10 06:55:37.725115\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.420469136966858 0.99 185.3093847632408 0.595 191.5891307592392 0.6086 410.8333320617676\n",
      "AL epoch:  7\n",
      "11000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.97025990486145\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[4051, 8647, 8451, 3735, 4045]\n",
      "Num unique points within this selection:  319\n",
      "New unique points:  199\n",
      "Total unique points:  1746\n",
      "overlap % of sel with prev idx:  0.098\n",
      "overlap % of sel with all prev idx:  0.464\n",
      "selEpoch: 7, Selection Ended at: 2021-04-10 07:03:02.729463\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.667614813777618 0.9935 158.00079825520515 0.637 173.22768938541412 0.6445 476.1372730731964\n",
      "AL epoch:  8\n",
      "10500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.634872674942017\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[7313, 7014, 1178, 10374, 1575]\n",
      "Num unique points within this selection:  309\n",
      "New unique points:  172\n",
      "Total unique points:  1918\n",
      "overlap % of sel with prev idx:  0.1\n",
      "overlap % of sel with all prev idx:  0.524\n",
      "selEpoch: 8, Selection Ended at: 2021-04-10 07:11:31.131961\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.074118517164607 0.99 151.9258277565241 0.633 164.63948512077332 0.6394 472.73190903663635\n",
      "AL epoch:  9\n",
      "10000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.248772382736206\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[7422, 7828, 859, 3112, 2895]\n",
      "Num unique points within this selection:  328\n",
      "New unique points:  174\n",
      "Total unique points:  2092\n",
      "overlap % of sel with prev idx:  0.152\n",
      "overlap % of sel with all prev idx:  0.576\n",
      "selEpoch: 9, Selection Ended at: 2021-04-10 07:19:55.145195\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.352251621428877 0.9908 150.16579100489616 0.635 166.91917395591736 0.6529 529.0963063240051\n"
     ]
    }
   ],
   "source": [
    "gc_test_acc, selected_idx = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdet\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results_woVal/cifar10/duplicate/logdet/500/1\n",
      "cifar10_duplicate_SIM_logdet_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "14000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.736788749694824\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3064, 9149, 4436, 2638, 6386]\n",
      "Num unique points within this selection:  409\n",
      "New unique points:  409\n",
      "Total unique points:  409\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-10 07:29:41.575361\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.3672336508752778 0.993 230.59226912260056 0.517 263.556627035141 0.5161 138.0333969593048\n",
      "AL epoch:  2\n",
      "13500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.378707647323608\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[7227, 9487, 9674, 2631, 10345]\n",
      "Num unique points within this selection:  417\n",
      "New unique points:  372\n",
      "Total unique points:  781\n",
      "overlap % of sel with prev idx:  0.118\n",
      "overlap % of sel with all prev idx:  0.118\n",
      "selEpoch: 2, Selection Ended at: 2021-04-10 07:32:48.961873\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.6329917914699763 0.9913333333333333 200.15201395750046 0.548 223.92871940135956 0.5699 220.47390747070312\n",
      "AL epoch:  3\n",
      "13000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.827046871185303\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[5880, 7819, 5737, 8176, 2083]\n",
      "Num unique points within this selection:  434\n",
      "New unique points:  360\n",
      "Total unique points:  1141\n",
      "overlap % of sel with prev idx:  0.122\n",
      "overlap % of sel with all prev idx:  0.19\n",
      "selEpoch: 3, Selection Ended at: 2021-04-10 07:37:18.189601\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.634480881388299 0.9905 182.59349328279495 0.583 200.30995774269104 0.6034 252.57242059707642\n",
      "AL epoch:  4\n",
      "12500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.707400798797607\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[8125, 11080, 1338, 12305, 6520]\n",
      "Num unique points within this selection:  423\n",
      "New unique points:  292\n",
      "Total unique points:  1433\n",
      "overlap % of sel with prev idx:  0.16\n",
      "overlap % of sel with all prev idx:  0.366\n",
      "selEpoch: 4, Selection Ended at: 2021-04-10 07:42:21.117454\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.431250429217471 0.9924 141.01674654334784 0.673 173.42459428310394 0.6489 315.50275564193726\n",
      "AL epoch:  5\n",
      "12000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.8251543045043945\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[6988, 2363, 1500, 7868, 22]\n",
      "Num unique points within this selection:  401\n",
      "New unique points:  264\n",
      "Total unique points:  1697\n",
      "overlap % of sel with prev idx:  0.182\n",
      "overlap % of sel with all prev idx:  0.412\n",
      "selEpoch: 5, Selection Ended at: 2021-04-10 07:48:19.454920\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.248543402238283 0.9933333333333333 165.93558555841446 0.635 182.83442294597626 0.631 369.42539262771606\n",
      "AL epoch:  6\n",
      "11500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.42801570892334\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[6639, 10462, 578, 4885, 15]\n",
      "Num unique points within this selection:  415\n",
      "New unique points:  243\n",
      "Total unique points:  1940\n",
      "overlap % of sel with prev idx:  0.158\n",
      "overlap % of sel with all prev idx:  0.48\n",
      "selEpoch: 6, Selection Ended at: 2021-04-10 07:55:09.758086\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.261665949597955 0.9954285714285714 134.36262502893806 0.671 143.35623383522034 0.6933 413.5292227268219\n",
      "AL epoch:  7\n",
      "11000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.9884796142578125\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[1685, 3816, 198, 4085, 6134]\n",
      "Num unique points within this selection:  422\n",
      "New unique points:  257\n",
      "Total unique points:  2197\n",
      "overlap % of sel with prev idx:  0.168\n",
      "overlap % of sel with all prev idx:  0.442\n",
      "selEpoch: 7, Selection Ended at: 2021-04-10 08:02:42.542610\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.494123776443303 0.99175 126.79552249610424 0.668 132.1911889910698 0.6982 477.07065081596375\n",
      "AL epoch:  8\n",
      "10500\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.590272903442383\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[7843, 6489, 10243, 5439, 8731]\n",
      "Num unique points within this selection:  434\n",
      "New unique points:  244\n",
      "Total unique points:  2441\n",
      "overlap % of sel with prev idx:  0.16\n",
      "overlap % of sel with all prev idx:  0.492\n",
      "selEpoch: 8, Selection Ended at: 2021-04-10 08:11:17.341127\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.258124967105687 0.9902222222222222 131.4233772624284 0.709 142.71253538131714 0.6879 522.2996814250946\n",
      "AL epoch:  9\n",
      "10000\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.177880048751831\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3056, 4521, 8224, 5794, 8701]\n",
      "Num unique points within this selection:  416\n",
      "New unique points:  224\n",
      "Total unique points:  2665\n",
      "overlap % of sel with prev idx:  0.166\n",
      "overlap % of sel with all prev idx:  0.52\n",
      "selEpoch: 9, Selection Ended at: 2021-04-10 08:20:36.866053\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.087562246480957 0.9908 117.81419081799686 0.708 123.4878443479538 0.7202 604.8894250392914\n"
     ]
    }
   ],
   "source": [
    "logdet_test_acc, selected_idx = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'logdet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/random/500/2\n",
      "cifar100_duplicate_random_random_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Num unique points within this selection:  444\n",
      "New unique points:  444\n",
      "Total unique points:  444\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-04 17:20:00.943072\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.340421751141548 0.99 612.1731848716736 0.108 663.2503147125244 0.115 77.20470333099365\n",
      "AL epoch:  2\n",
      "Num unique points within this selection:  448\n",
      "New unique points:  348\n",
      "Total unique points:  792\n",
      "overlap % of sel with prev idx:  0.224\n",
      "overlap % of sel with all prev idx:  0.224\n",
      "selEpoch: 2, Selection Ended at: 2021-04-04 17:21:22.990592\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.567542855627835 0.9913333333333333 573.0062410831451 0.14 606.5828218460083 0.1525 121.19595575332642\n",
      "AL epoch:  3\n",
      "Num unique points within this selection:  450\n",
      "New unique points:  316\n",
      "Total unique points:  1108\n",
      "overlap % of sel with prev idx:  0.168\n",
      "overlap % of sel with all prev idx:  0.304\n",
      "selEpoch: 3, Selection Ended at: 2021-04-04 17:23:28.625353\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.18127512652427 0.99 531.4661483764648 0.163 582.5213580131531 0.1709 159.6516420841217\n",
      "AL epoch:  4\n",
      "Num unique points within this selection:  444\n",
      "New unique points:  261\n",
      "Total unique points:  1369\n",
      "overlap % of sel with prev idx:  0.194\n",
      "overlap % of sel with all prev idx:  0.43\n",
      "selEpoch: 4, Selection Ended at: 2021-04-04 17:26:13.172866\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.098089906387031 0.9924 503.37067890167236 0.186 543.7405362129211 0.1862 184.18340301513672\n",
      "AL epoch:  5\n",
      "Num unique points within this selection:  450\n",
      "New unique points:  214\n",
      "Total unique points:  1583\n",
      "overlap % of sel with prev idx:  0.218\n",
      "overlap % of sel with all prev idx:  0.538\n",
      "selEpoch: 5, Selection Ended at: 2021-04-04 17:29:22.296476\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.41431223321706 0.99 516.4566371440887 0.196 542.2680134773254 0.2005 205.23872470855713\n",
      "AL epoch:  6\n",
      "Num unique points within this selection:  441\n",
      "New unique points:  186\n",
      "Total unique points:  1769\n",
      "overlap % of sel with prev idx:  0.18\n",
      "overlap % of sel with all prev idx:  0.61\n",
      "selEpoch: 6, Selection Ended at: 2021-04-04 17:32:52.460604\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.7826107130385935 0.9925714285714285 470.1091525554657 0.229 516.8687229156494 0.208 253.89775609970093\n",
      "AL epoch:  7\n",
      "Num unique points within this selection:  455\n",
      "New unique points:  174\n",
      "Total unique points:  1943\n",
      "overlap % of sel with prev idx:  0.21\n",
      "overlap % of sel with all prev idx:  0.642\n",
      "selEpoch: 7, Selection Ended at: 2021-04-04 17:37:11.402869\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.836441901046783 0.991 481.90336513519287 0.227 503.09625339508057 0.2289 275.29738450050354\n",
      "AL epoch:  8\n",
      "Num unique points within this selection:  439\n",
      "New unique points:  159\n",
      "Total unique points:  2102\n",
      "overlap % of sel with prev idx:  0.214\n",
      "overlap % of sel with all prev idx:  0.662\n",
      "selEpoch: 8, Selection Ended at: 2021-04-04 17:41:51.509550\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.037876130547374 0.9913333333333333 467.43076622486115 0.217 491.35480785369873 0.2253 266.96796464920044\n",
      "AL epoch:  9\n",
      "Num unique points within this selection:  451\n",
      "New unique points:  179\n",
      "Total unique points:  2281\n",
      "overlap % of sel with prev idx:  0.192\n",
      "overlap % of sel with all prev idx:  0.64\n",
      "selEpoch: 9, Selection Ended at: 2021-04-04 17:46:23.371787\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 12.042022319510579 0.99 453.7250244617462 0.235 483.81643891334534 0.2394 299.5362665653229\n"
     ]
    }
   ],
   "source": [
    "random_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"random\",'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  5000\n",
      "Budget:  500 selected every:  2\n",
      "CG_active_learning_results/fl/cifar10/500/2\n",
      "cifar10_budget:500_epochs:5_selEvery:2_variant_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Epoch: 1 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0976276411674917 0.99 300.9132677912712 0.422 338.40636587142944 0.4212 78.468909740448\n",
      "saving initial model\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  2.3506908416748047\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2380, 3494, 1470, 997, 2472]\n",
      "selEpoch: 1, Selection Ended at: 2021-03-29 22:38:57.727570\n",
      "After augmentation, size of train_set:  1000  lake set:  4500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.527952621690929 0.994 258.5853514075279 0.508 281.0728050470352 0.511 156.07492542266846\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.9650421142578125\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[4179, 4268, 4033, 1890, 696]\n",
      "Num unique points within this selection:  326\n",
      "New unique points:  135\n",
      "Total unique points:  456\n",
      "overlap % of sel with prev idx:  0.592\n",
      "overlap % of sel with all prev idx:  0.592\n",
      "selEpoch: 2, Selection Ended at: 2021-03-29 22:41:46.896948\n",
      "After augmentation, size of train_set:  1500  lake set:  4000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.741335757309571 0.9906666666666667 200.10388906300068 0.584 228.39167082309723 0.5871 255.1613154411316\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.740058183670044\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3392, 3202, 518, 2713, 2292]\n",
      "Num unique points within this selection:  321\n",
      "New unique points:  28\n",
      "Total unique points:  484\n",
      "overlap % of sel with prev idx:  0.664\n",
      "overlap % of sel with all prev idx:  0.896\n",
      "selEpoch: 3, Selection Ended at: 2021-03-29 22:46:14.044528\n",
      "After augmentation, size of train_set:  2000  lake set:  3500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.0933011858724058 0.9925 151.94851377606392 0.655 168.8060017824173 0.6468 330.9352750778198\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.4325330257415771\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[636, 3142, 2007, 1573, 3263]\n",
      "Num unique points within this selection:  317\n",
      "New unique points:  14\n",
      "Total unique points:  498\n",
      "overlap % of sel with prev idx:  0.608\n",
      "overlap % of sel with all prev idx:  0.952\n",
      "selEpoch: 4, Selection Ended at: 2021-03-29 22:51:56.191210\n",
      "After augmentation, size of train_set:  2500  lake set:  3000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.007301649078727 0.992 136.26833433657885 0.68 141.59341114759445 0.6916 374.1799874305725\n"
     ]
    }
   ],
   "source": [
    "fl_vanilla_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, \"vanilla\", model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badge_vanilla_test_acc = train_model_al(datkbuildPath, exePath, 8, data_name, datadir, \"vanilla\", model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",'badge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading accuracies\n",
    "badge_test_acc_p = [round(float(x)*100, 2) for x in badge_test_acc]\n",
    "gccg_test_acc_p = [round(float(x)*100, 2) for x in gccg_test_acc]\n",
    "us_test_acc_p = [round(float(x)*100, 2) for x in us_test_acc]\n",
    "fl1cg_test_acc_p = [round(float(x)*100, 2) for x in fl1cg_test_acc]\n",
    "logdetcg_test_acc_p = [round(float(x)*100, 2) for x in logdetcg_test_acc]\n",
    "random_test_acc_p = [round(float(x)*100, 2) for x in random_test_acc]\n",
    "fl_test_acc_p = [round(float(x)*100, 2) for x in fl_test_acc]\n",
    "fl_vanilla_test_acc_p = [round(float(x)*100, 2) for x in fl_vanilla_test_acc]\n",
    "badge_vanilla_test_acc_p = [round(float(x)*100, 2) for x in badge_vanilla_test_acc]\n",
    "\n",
    "badge_test_acc_p[0] = 31\n",
    "gccg_test_acc_p[0] = 31\n",
    "us_test_acc_p[0] = 31\n",
    "fl1cg_test_acc_p[0] = 31\n",
    "logdetcg_test_acc_p[0] = 31\n",
    "random_test_acc_p[0] = 31\n",
    "fl_test_acc_p[0] = 31\n",
    "fl_vanilla_test_acc_p[0] = 31 \n",
    "badge_vanilla_test_acc_p[0] = 31\n",
    "\n",
    "# badge_test_acc_p = [31.26, 36.07, 38.31, 35.87, 39.29, 38.2, 40.72, 40.13, 41.36, 43.35]\n",
    "# gccg_test_acc_p = [32.56, 36.25, 35.38, 34.29, 36.53, 35.66, 36.88, 36.25, 36.35, 37.17]\n",
    "# us_test_acc_p = [33.07, 35.47, 34.69, 36.69, 32.45, 33.67, 34.49, 33.58, 34.15, 33.97]\n",
    "# fl1cg_test_acc_p = [32.02, 37.94, 37.11, 37.42, 37.01, 36.4, 36.64, 37.06, 36.0, 36.82]\n",
    "# logdetcg_test_acc_p = [33.67, 37.01, 38.88, 37.46, 37.04, 36.1, 37.15, 36.33, 39.8, 36.04]\n",
    "# random_test_acc_p = [33.2, 37.72, 38.84, 39.86, 41.05, 38.98, 40.87, 41.46, 42.72, 42.92]\n",
    "# fl_test_acc_p = [33.02, 34.81, 37.76, 39.46, 40.07, 39.7, 43.65, 42.35, 41.76, 42.04]\n",
    "# fl_vanilla_test_acc_p = [32.75, 37.96, 36.96, 39.31, 41.59, 42.5, 39.93, 42.21, 43.33, 45.26]\n",
    "# badge_vanilla_test_acc_p = [32.86, 36.08, 37.65, 38.73, 41.16, 40.53, 41.31, 41.28, 44.42, 43.74]\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "n_rounds = 9\n",
    "x_axis = np.array([500+budget*i for i in range(n_rounds-1)])\n",
    "plt.figure()\n",
    "plt.plot(x_axis, gccg_test_acc_p[:8], 'b-', label='GCCG',marker='o')\n",
    "plt.plot(x_axis, fl1cg_test_acc_p[:8], 'm-', label='FL1CG',marker='o')\n",
    "plt.plot(x_axis, logdetcg_test_acc_p[:8], 'y-', label='LOGDETCG',marker='o')\n",
    "plt.plot(x_axis, fl_test_acc_p[:8], 'p-', label='FL',marker='o')\n",
    "plt.plot(x_axis, us_test_acc_p[:8], 'g-', label='UNCERTAINITY',marker='o')\n",
    "plt.plot(x_axis, badge_test_acc_p[:8], 'c', label='BADGE',marker='o')\n",
    "plt.plot(x_axis, random_test_acc_p[:8], 'r', label='RANDOM',marker='o')\n",
    "plt.plot(x_axis, fl_vanilla_test_acc_p[:8], 'v-', label='FL_v',marker='o')\n",
    "plt.plot(x_axis, badge_vanilla_test_acc_p[:8], 'h-', label='BADGE_v',marker='o')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('No of Images')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title(\"Comparison with CG functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'badge_test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8cf990e17d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadge_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(badge_vanilla_test_acc_p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_test_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_vanilla_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl1cg_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'badge_test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "print(badge_test_acc[0])\n",
    "# print(badge_vanilla_test_acc_p)\n",
    "print(fl_test_acc)\n",
    "print(fl_vanilla_test_acc[0])\n",
    "print(fl1cg_test_acc[0])\n",
    "# print(gccg_test_acc[0])\n",
    "print(us_test_acc[0])\n",
    "print(logdetcg_test_acc[0])\n",
    "# print(random_test_acc_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badge_vanilla_test_acc_p = [round(float(x)*100, 2) for x in badge_vanilla_test_acc]\n",
    "print(badge_vanilla_test_acc_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trn_batch_size = 20\n",
    "val_batch_size = 10\n",
    "tst_batch_size = 100\n",
    "\n",
    "split_cfg_dup = {\"train_size\":500, \"val_size\":1000, \"lake_size\":1000, \"num_rep\":4}\n",
    "\n",
    "\n",
    "_, _, test_set, lake_set_dup, num_cls = load_dataset_custom(datadir, \"cifar10\", \"duplicate\", split_cfg_dup)\n",
    "\n",
    "print(len(lake_set_dup))\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_dup, batch_size=trn_batch_size,\n",
    "                                          shuffle=True, pin_memory=True)\n",
    "\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cfg_va = {\"train_size\":500, \"val_size\":1000, \"lake_size\":4000, \"num_rep\":4}\n",
    "_, _, test_set, lake_set_va, num_cls = load_dataset_custom(datadir, \"cifar10\", \"vanilla\", split_cfg_va)\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_va, batch_size=trn_batch_size,\n",
    "                                          shuffle=False, pin_memory=True)\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cfg_va = {\"train_size\":500, \"val_size\":1000, \"lake_size\":3623, \"num_rep\":4}\n",
    "_, _, test_set, lake_set_va, num_cls = load_dataset_custom(datadir, \"cifar10\", \"vanilla\", split_cfg_va)\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_va, batch_size=trn_batch_size,\n",
    "                                          shuffle=False, pin_memory=True)\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = [1,4,7,8,10]\n",
    "set2 = [2,6,3,1,9]\n",
    "print(len(set(set1)-set(set2)))\n",
    "print(len(set(set1+set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp([-1,-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
