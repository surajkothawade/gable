{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from cords.cords.selectionstrategies.supervisedlearning import DataSelectionStrategy\n",
    "from cords.cords.utils.models import ResNet18\n",
    "from gable.gable.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "# for cuda\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class custom_subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "        labels(sequence) : targets as required for the indices. will be the same length as indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, labels):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.targets = labels.type(torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target = self.targets[idx]\n",
    "        return (image, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device):\n",
    "    if name == 'ResNet18':\n",
    "        model = ResNet18(num_cls)\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def kernel(x, y, measure=\"cosine\", exp=2):\n",
    "    if(measure==\"eu_sim\"):\n",
    "        lam = 0.25\n",
    "        dist = pairwise_distances(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = max(dist.ravel()) - dist\n",
    "        sim *= lam\n",
    "        sim = np.exp(sim)\n",
    "#         n = x.size(0)\n",
    "#         m = y.size(0)\n",
    "#         d = x.size(1)\n",
    "#         x = x.unsqueeze(1).expand(n, m, d)\n",
    "#         y = y.unsqueeze(0).expand(n, m, d)\n",
    "#         dist = torch.pow(x - y, exp).sum(2)\n",
    "#         const = torch.max(dist).item()\n",
    "#         sim = (const - dist)\n",
    "    \n",
    "        #dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))\n",
    "    if(measure==\"cosine\"):\n",
    "        sim = cosine_similarity(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = np.exp(sim)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def save_kernel_hdf5(lake_kernel, lake_target_kernel, target_kernel=[], numpy=True):\n",
    "    if(not(numpy)):\n",
    "        lake_kernel = lake_kernel.cpu().numpy()\n",
    "    with h5py.File(\"lake_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_target_kernel = lake_target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"lake_target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_target_kernel)\n",
    "    if(not(numpy)):\n",
    "        target_kernel = target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=target_kernel)\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "    saveDir = os.path.join(saveDir, prefix)\n",
    "    if(not(os.path.exists(saveDir))):\n",
    "        os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    class_err_log = []\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        print(\"val, test error% for class \", i, \" : \", round((len(val_err_class_idx)/len(val_class_idxs))*100,2), round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        class_err_log.append(\"val, test error% for class \"+ str(i) + \" : \"+ str(round((len(val_err_class_idx)/len(val_class_idxs))*100,2)) + \", \" + str(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)))\n",
    "        tst_err_log.append(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        \n",
    "    return tst_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget)):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = custom_subset(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    return aug_train_set, remain_lake_set, subset\n",
    "                        \n",
    "def getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def getSMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, sf):\n",
    "    if(sf==\"fl1mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\")\n",
    "    elif(sf == \"logdetmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \"  -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\") + \" -queryqueryKernelFile \" + os.path.join(hdf5Path, \"target_kernel.hdf5\")\n",
    "    elif(sf==\"fl2mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gcmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gccg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl1cg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"logdetcg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda \" + str(magnification) + \" -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"lake_target_kernel.hdf5\") + \" -privateprivateKernelFile \" + os.path.join(hdf5Path, \"target_kernel.hdf5\")\n",
    "    elif(sf==\"fl\" or sf==\"logdet\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\")\n",
    "    elif(sf ==\"gc\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"lake_kernel.hdf5\")\n",
    "    print(\"Executing SIM command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "# def getDuplicates(subset):\n",
    "    \n",
    "#check overlap with prev selections\n",
    "def check_overlap(prev_idx, prev_idx_hist, idx):\n",
    "    prev_idx = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in prev_idx ]\n",
    "    prev_idx_hist = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in prev_idx_hist]\n",
    "    idx = [int(x/num_rep) if x < ((split_cfg[\"num_rep\"] * split_cfg[\"lake_subset_repeat_size\"])-1) else x for x in idx]\n",
    "    # overlap = set(prev_idx).intersection(set(idx))\n",
    "    overlap = [value for value in idx if value in prev_idx] \n",
    "    # overlap_hist = set(prev_idx_hist).intersection(set(idx))\n",
    "    overlap_hist = [value for value in idx if value in prev_idx_hist]\n",
    "    new_points = set(idx) - set(prev_idx_hist)\n",
    "    total_unique_points = set(idx+prev_idx_hist)\n",
    "    print(\"Num unique points within this selection: \", len(set(idx)))\n",
    "    print(\"New unique points: \", len(new_points))\n",
    "    print(\"Total unique points: \", len(total_unique_points))\n",
    "    print(\"overlap % of sel with prev idx: \", len(overlap)/len(idx))\n",
    "    print(\"overlap % of sel with all prev idx: \", len(overlap_hist)/len(idx))\n",
    "#     return len(overlap)/len(idx), len(overlap_hist)/len(idx)\n",
    "    return len(total_unique_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "datadir = 'data/'\n",
    "data_name = 'cifar100'\n",
    "num_cls=10\n",
    "fraction = float(0.1)\n",
    "budget=500\n",
    "num_epochs = int(10)\n",
    "num_rep = 10\n",
    "# feature='vanilla'\n",
    "feature = 'duplicate'\n",
    "# feature = 'classimb'\n",
    "num_runs = 1  # number of random runs\n",
    "learning_rate = 0.01\n",
    "model_name = 'ResNet18'\n",
    "magnification = 10\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# split_cfg = {\"num_cls_imbalance\":2, \"per_imbclass_train\":50, \"per_imbclass_val\":25, \"per_imbclass_lake\":150, \"per_class_train\":1000, \"per_class_val\":25, \"per_class_lake\":3000}\n",
    "split_cfg = {\"train_size\":500, \"val_size\":1000, \"lake_size\":5000, \"num_rep\":num_rep, \"lake_subset_repeat_size\":1000}\n",
    "datkbuildPath = \"/home/snk170001/bioml/dss/notebooks/datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "initModelPath = \"weights/cg_\" + data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"train_size\"])\n",
    "print(\"Using Device:\", device)\n",
    "doublePrecision = False\n",
    "linearLayer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distil.distil.active_learning_strategies import BADGE, EntropySampling, GLISTER\n",
    "from distil.distil.utils.DataHandler import DataHandler_CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    if(feature==\"classimb\"):\n",
    "        train_set, val_set, test_set, lake_set, imb_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        print(\"imbalanced classes are: \", imb_cls_idx)\n",
    "    if(feature==\"duplicate\" or feature==\"vanilla\"):\n",
    "        sel_cls_idx = None\n",
    "        if(strategy == \"SIM\" or strategy==\"random\"):\n",
    "            train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        elif(strategy==\"AL\"):\n",
    "            X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "        \n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "    print(\"Budget: \", bud)\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "    all_logs_dir = 'CG_active_learning_results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "#     path_logfile = os.path.join(all_logs_dir, dataset_name + '.txt')\n",
    "#     logfile = open(path_logfile, 'w')\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \"feature\":feature, \"sel_func\":sf, \"sel_budget\":budget, \"num_selections\":num_epochs, \"model\":model_name, \"learning_rate\":learning_rate, \"setting\":split_cfg, \"test_acc\":[], \"num_unique_samples\":[], \"sel_cls_idx\":sel_cls_idx}\n",
    "\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device)\n",
    "    model1 = create_model(model_name, num_cls, device)\n",
    "    if(strategy == \"AL\"):\n",
    "        strategy_args = {'batch_size' : 100, 'lr':float(0.001)}\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"glister\"):\n",
    "            strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args, valid=False, typeOf='rand', lam=0.1, linear_layer=False)\n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "#     optimizer, scheduler = optimizer_with_scheduler(model, num_epochs, learning_rate)\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "    #overlap vars\n",
    "    prev_idx = []\n",
    "    prev_idx_hist = []\n",
    "    per_ep_overlap = []\n",
    "    overall_overlap = []\n",
    "    idx_tracker = np.array(list(range(len(lake_set))))\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training\")\n",
    "                for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    tst_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    tst_total += targets.size(0)\n",
    "                    tst_correct += predicted.eq(targets).sum().item()\n",
    "                tst_acc[i] = tst_correct / tst_total\n",
    "                res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "                continue\n",
    "        else:\n",
    "#             if(full_trn_acc[i-1] >= 0.99): #The model has already trained on the seed dataset\n",
    "            #use misclassifications on validation set as queries\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append([\"epoch \"+str(i+1)]+tst_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                    miscls_set = getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx)\n",
    "                    misclsloader = torch.utils.data.DataLoader(miscls_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, misclsloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                elif(sf.endswith(\"cg\")): #atleast one selection must be done for private set in cond gain functions\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, privateSetloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        #compute subset with private set a NULL\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                else:\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                start_time = time.time()\n",
    "                cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "                clone_dict = copy.deepcopy(model.state_dict())\n",
    "                #update the selection strategy model with new params for gradient computation\n",
    "                setf_model.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mi\")): #SMI functions need the target set gradients\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                    if(doublePrecision):\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    else:\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                    numQueryPrivate = train_val_kernel.shape[1]\n",
    "                elif(sf.endswith(\"cg\")):\n",
    "                    if(len(private_set)!=0):\n",
    "                        setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                        print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                        print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                        if(doublePrecision):\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_private_kernel\n",
    "                        else:\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_private_kernel\n",
    "                        numQueryPrivate = train_val_kernel.shape[1]\n",
    "                    else:\n",
    "#                         assert(((i + 1)/select_every)==1)\n",
    "                        setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        train_val_kernel = []\n",
    "                        numQueryPrivate = 0\n",
    "                else: # For other submodular functions needing only image kernel\n",
    "                    setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                    train_val_kernel = []\n",
    "                    numQueryPrivate = 0\n",
    "\n",
    "                kernel_time = time.time()\n",
    "                if(doublePrecision):\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.grads_per_elem.double()) #img_img_kernel\n",
    "                else:\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem, setf_model.grads_per_elem) #img_img_kernel\n",
    "\n",
    "                if(sf==\"logdetmi\" or sf==\"logdetcg\"):\n",
    "                    if(len(private_set)!=0):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                    else:\n",
    "                        val_kernel = []\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel, val_kernel)\n",
    "                else:\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel)\n",
    "                print(\"kernel compute time: \", time.time()-kernel_time)\n",
    "                #call the c++ exec to read kernel and compute subset of selected minibatches\n",
    "                subset = getSMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQueryPrivate), sf)\n",
    "                print(subset[:5])\n",
    "                model.load_state_dict(cached_state_dict)\n",
    "                if(sf.endswith(\"cg\")): #for first selection\n",
    "                    if(len(private_set)==0):\n",
    "                        private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "                    else:\n",
    "                        private_set = getPrivateSet(lake_set, subset, private_set)\n",
    "                    print(\"size of private set: \", len(private_set))\n",
    "\n",
    "    #           temp = np.array(list(trainloader.batch_sampler))[subset] #if per batch\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                strategy_sel.update_model(model)\n",
    "                if(sf==\"badge\" or sf==\"glister\"):\n",
    "                    subset = strategy_sel.select(budget)\n",
    "                if(sf==\"us\"):\n",
    "                    subset = list(strategy_sel.select(budget).cpu().numpy())\n",
    "                print(len(subset), \" samples selected\")\n",
    "                X_tr = np.concatenate((X_tr, X_unlabeled[subset]), axis=0)\n",
    "                X_unlabeled = np.delete(X_unlabeled, subset, axis = 0)\n",
    "                y_tr = np.concatenate((y_tr, y_unlabeled[subset]), axis = 0)\n",
    "                y_unlabeled = np.delete(y_unlabeled, subset, axis = 0)\n",
    "                strategy_sel.update_data(X_tr, y_tr, X_unlabeled)\n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            if(i>0):\n",
    "                curr_unique_points = check_overlap(prev_idx, prev_idx_hist, list(idx_tracker[subset]))\n",
    "                res_dict[\"num_unique_samples\"].append(curr_unique_points)\n",
    "#                 per_ep_overlap.append(per_ep)\n",
    "#                 overall_overlap.append(overall)\n",
    "\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "\n",
    "            #augment the train_set with selected indices from the lake\n",
    "            train_set, lake_set, subset = aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set))\n",
    "            prev_idx = list(idx_tracker[subset])\n",
    "            prev_idx_hist += list(idx_tracker[subset])\n",
    "            idx_tracker = np.delete(idx_tracker, subset, axis=0)\n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            assert(len(idx_tracker)==len(lake_set))\n",
    "#             model =  model.apply(weight_reset).cuda()\n",
    "            model = create_model(model_name, num_cls, device)\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<150):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # print(batch_idx)\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "#                 if(i == (num_epochs-1)):\n",
    "            final_val_predictions += list(predicted.cpu().numpy())\n",
    "            final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "            # sys.exit()\n",
    "\n",
    "        if((val_correct/val_total) > best_val_acc):\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            if((val_correct/val_total) > best_val_acc):\n",
    "#                 if(i == (num_epochs-1)):\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "        if((val_correct/val_total) > best_val_acc):\n",
    "            best_val_acc = (val_correct/val_total)\n",
    "        val_acc[i] = val_correct / val_total\n",
    "        tst_acc[i] = tst_correct / tst_total\n",
    "        val_losses[i] = val_loss\n",
    "        fulltrn_losses[i] = full_trn_loss\n",
    "        tst_losses[i] = tst_loss\n",
    "        full_val_acc = list(np.array(val_acc))\n",
    "        full_timing = list(np.array(timing))\n",
    "        res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "        print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_class_err_idxs = find_err_per_class(test_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append([\"final\"]+tst_err_log)\n",
    "        print(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    return tst_acc, prev_idx_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL badge\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/badge/500/2\n",
      "cifar100_duplicate_AL_badge_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  500\n",
      "Total unique points:  500\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-04 02:09:07.980668\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.424920183606446 0.992 612.3125305175781 0.123 664.2775421142578 0.1255 103.5102608203888\n",
      "AL epoch:  2\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  395\n",
      "Total unique points:  895\n",
      "overlap % of sel with prev idx:  0.21\n",
      "overlap % of sel with all prev idx:  0.21\n",
      "selEpoch: 2, Selection Ended at: 2021-04-04 02:25:19.607124\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.291963634546846 0.9906666666666667 558.521116733551 0.159 602.3568363189697 0.1546 148.06912684440613\n",
      "AL epoch:  3\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  314\n",
      "Total unique points:  1209\n",
      "overlap % of sel with prev idx:  0.202\n",
      "overlap % of sel with all prev idx:  0.372\n",
      "selEpoch: 3, Selection Ended at: 2021-04-04 02:40:53.947594\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.985971806570888 0.991 524.626079082489 0.177 553.0759162902832 0.1856 169.1079547405243\n",
      "AL epoch:  4\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  272\n",
      "Total unique points:  1481\n",
      "overlap % of sel with prev idx:  0.218\n",
      "overlap % of sel with all prev idx:  0.456\n",
      "selEpoch: 4, Selection Ended at: 2021-04-04 02:56:51.871738\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.692069679964334 0.9936 531.547149181366 0.173 548.9416007995605 0.1873 214.0593068599701\n",
      "AL epoch:  5\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  231\n",
      "Total unique points:  1712\n",
      "overlap % of sel with prev idx:  0.25\n",
      "overlap % of sel with all prev idx:  0.538\n",
      "selEpoch: 5, Selection Ended at: 2021-04-04 03:13:03.838807\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.973434795625508 0.994 485.94446778297424 0.217 512.4587044715881 0.2083 234.9548740386963\n",
      "AL epoch:  6\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  217\n",
      "Total unique points:  1929\n",
      "overlap % of sel with prev idx:  0.196\n",
      "overlap % of sel with all prev idx:  0.566\n",
      "selEpoch: 6, Selection Ended at: 2021-04-04 03:29:17.535507\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.862388708628714 0.9914285714285714 465.2426519393921 0.21 497.85299348831177 0.2185 255.46127676963806\n",
      "AL epoch:  7\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  194\n",
      "Total unique points:  2123\n",
      "overlap % of sel with prev idx:  0.22\n",
      "overlap % of sel with all prev idx:  0.612\n",
      "selEpoch: 7, Selection Ended at: 2021-04-04 03:44:48.000409\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.70483926567249 0.99125 496.91976618766785 0.196 506.7741458415985 0.2183 296.31937551498413\n",
      "AL epoch:  8\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  167\n",
      "Total unique points:  2290\n",
      "overlap % of sel with prev idx:  0.244\n",
      "overlap % of sel with all prev idx:  0.666\n",
      "selEpoch: 8, Selection Ended at: 2021-04-04 04:00:30.817321\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.915043524000794 0.9948888888888889 454.1595833301544 0.234 473.3086111545563 0.2451 313.76706624031067\n",
      "AL epoch:  9\n",
      "500  samples selected\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  187\n",
      "Total unique points:  2477\n",
      "overlap % of sel with prev idx:  0.258\n",
      "overlap % of sel with all prev idx:  0.626\n",
      "selEpoch: 9, Selection Ended at: 2021-04-04 04:15:51.771056\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.77622586954385 0.9958 423.633469581604 0.252 457.15879464149475 0.2581 341.5180048942566\n"
     ]
    }
   ],
   "source": [
    "# train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n",
    "badge_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL us\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/us/500/1\n",
      "cifar100_duplicate_AL_us_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "500  samples selected\n",
      "Num unique points within this selection:  151\n",
      "New unique points:  151\n",
      "Total unique points:  151\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-04 01:24:02.286599\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.240939344279468 0.997 658.2571821212769 0.089 711.1595673561096 0.0882 74.23673248291016\n",
      "AL epoch:  2\n",
      "500  samples selected\n",
      "Num unique points within this selection:  189\n",
      "New unique points:  189\n",
      "Total unique points:  340\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 2, Selection Ended at: 2021-04-04 01:25:26.401970\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.16006351262331 0.9913333333333333 615.3531317710876 0.128 664.870491027832 0.1169 102.37509393692017\n",
      "AL epoch:  3\n",
      "500  samples selected\n",
      "Num unique points within this selection:  150\n",
      "New unique points:  150\n",
      "Total unique points:  490\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 3, Selection Ended at: 2021-04-04 01:27:17.894467\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.274039593525231 0.9905 603.4893250465393 0.128 654.3305010795593 0.1185 132.53785228729248\n",
      "AL epoch:  4\n",
      "500  samples selected\n",
      "Num unique points within this selection:  167\n",
      "New unique points:  167\n",
      "Total unique points:  657\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 4, Selection Ended at: 2021-04-04 01:29:39.379376\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.701643437845632 0.9924 588.9568116664886 0.137 622.3361401557922 0.1362 156.42200803756714\n",
      "AL epoch:  5\n",
      "500  samples selected\n",
      "Num unique points within this selection:  178\n",
      "New unique points:  177\n",
      "Total unique points:  834\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.004\n",
      "selEpoch: 5, Selection Ended at: 2021-04-04 01:32:24.683909\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.467213258612901 0.991 570.7126858234406 0.14 613.1549320220947 0.1392 184.84537363052368\n",
      "AL epoch:  6\n",
      "500  samples selected\n",
      "Num unique points within this selection:  167\n",
      "New unique points:  167\n",
      "Total unique points:  1001\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 6, Selection Ended at: 2021-04-04 01:35:39.292065\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.061284890631214 0.9928571428571429 550.5096302032471 0.155 584.7115941047668 0.1446 214.84882140159607\n",
      "AL epoch:  7\n",
      "500  samples selected\n",
      "Num unique points within this selection:  174\n",
      "New unique points:  172\n",
      "Total unique points:  1173\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.008\n",
      "selEpoch: 7, Selection Ended at: 2021-04-04 01:39:23.256998\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.889599953661673 0.99575 521.8825106620789 0.166 558.6890826225281 0.1642 241.09412050247192\n",
      "AL epoch:  8\n",
      "500  samples selected\n",
      "Num unique points within this selection:  198\n",
      "New unique points:  197\n",
      "Total unique points:  1370\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.008\n",
      "selEpoch: 8, Selection Ended at: 2021-04-04 01:43:32.510014\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.796705823158845 0.9908888888888889 529.4440639019012 0.151 546.1296682357788 0.161 276.8809952735901\n",
      "AL epoch:  9\n",
      "500  samples selected\n",
      "Num unique points within this selection:  165\n",
      "New unique points:  164\n",
      "Total unique points:  1534\n",
      "overlap % of sel with prev idx:  0.014\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 9, Selection Ended at: 2021-04-04 01:48:18.163846\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.7739217802882195 0.9958 499.0795795917511 0.178 509.29580068588257 0.1926 315.531512260437\n"
     ]
    }
   ],
   "source": [
    "us_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL glister\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/glister/500/1\n",
      "cifar100_duplicate_AL_glister_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snk170001/bioml/dss/notebooks/distil/distil/active_learning_strategies/strategy.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_trn = torch.tensor(Y[idxs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([100, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [100, 1] doesn't match the broadcast shape [100, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ee48ee3860ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglister_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_al\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatkbuildPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AL\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"glister\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-087c9ff3db3d>\u001b[0m in \u001b[0;36mtrain_model_al\u001b[0;34m(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mstrategy_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"badge\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"glister\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbudget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"us\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbudget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bioml/dss/notebooks/distil/distil/active_learning_strategies/glister.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, budget)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_per_element_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grads_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mnumSelected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bioml/dss/notebooks/distil/distil/active_learning_strategies/glister.py\u001b[0m in \u001b[0;36m_update_grads_val\u001b[0;34m(self, grads_currX, first_init)\u001b[0m\n\u001b[1;32m    180\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads_val_curr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml0_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads_val_curr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml0_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [100, 1] doesn't match the broadcast shape [100, 100]"
     ]
    }
   ],
   "source": [
    "glister_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"glister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM gccg\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/gccg/500/1\n",
      "cifar100_duplicate_SIM_gccg_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.803529500961304\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 0 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[12738, 11545, 8924, 8806, 1379]\n",
      "size of private set:  500\n",
      "Num unique points within this selection:  253\n",
      "New unique points:  253\n",
      "Total unique points:  253\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 21:20:04.157817\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.34261145722121 0.993 616.6082186698914 0.118 687.1663932800293 0.1116 71.27564716339111\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13500, 100])\n",
      "val minibatch gradients shape  torch.Size([500, 100])\n",
      "kernel compute time:  7.473673343658447\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer gccg -numQueries 500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[898, 7875, 590, 5216, 8160]\n",
      "size of private set:  1000\n",
      "Num unique points within this selection:  247\n",
      "New unique points:  247\n",
      "Total unique points:  500\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 21:22:16.195703\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Selection Epoch  2  Training epoch [ 12 ]  Training Acc:  0.6013333333333334\r"
     ]
    }
   ],
   "source": [
    "gccg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL1CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl1cg\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/fl1cg/500/2\n",
      "cifar100_duplicate_SIM_fl1cg_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.729737758636475\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 0 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[10720, 12150, 2063, 140, 3909]\n",
      "size of private set:  500\n",
      "Num unique points within this selection:  224\n",
      "New unique points:  224\n",
      "Total unique points:  224\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 17:05:51.851750\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.0924987653270364 0.992 638.6907248497009 0.119 705.9181976318359 0.0957 58.653056383132935\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13500, 100])\n",
      "val minibatch gradients shape  torch.Size([500, 100])\n",
      "kernel compute time:  7.394716739654541\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[13013, 13014, 13015, 13016, 13017]\n",
      "size of private set:  1000\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  498\n",
      "Total unique points:  722\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 17:07:28.588388\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.493318657390773 0.9913333333333333 567.4956934452057 0.134 614.0177364349365 0.1376 101.9496521949768\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13000, 100])\n",
      "val minibatch gradients shape  torch.Size([1000, 100])\n",
      "kernel compute time:  6.812664985656738\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 1000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[12513, 12514, 12515, 12516, 12517]\n",
      "size of private set:  1500\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  493\n",
      "Total unique points:  1215\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 17:09:50.044009\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.0881102425046265 0.9955 528.746337890625 0.164 576.4281611442566 0.1633 162.95432829856873\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12500, 100])\n",
      "val minibatch gradients shape  torch.Size([1500, 100])\n",
      "kernel compute time:  6.727391481399536\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 1500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[12013, 12014, 12015, 12016, 12017]\n",
      "size of private set:  2000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  496\n",
      "Total unique points:  1711\n",
      "overlap % of sel with prev idx:  0.006\n",
      "overlap % of sel with all prev idx:  0.008\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 17:13:12.689709\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.072767330799252 0.9908 486.15298104286194 0.193 511.9260346889496 0.2104 179.7307755947113\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12000, 100])\n",
      "val minibatch gradients shape  torch.Size([2000, 100])\n",
      "kernel compute time:  6.539091348648071\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 2000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[11513, 11514, 11515, 11516, 11517]\n",
      "size of private set:  2500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  493\n",
      "Total unique points:  2204\n",
      "overlap % of sel with prev idx:  0.014\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 17:16:59.879293\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.624633080791682 0.997 452.1987042427063 0.237 491.0060420036316 0.2389 250.51068115234375\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11500, 100])\n",
      "val minibatch gradients shape  torch.Size([2500, 100])\n",
      "kernel compute time:  6.194093227386475\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 2500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[11013, 11014, 11015, 11016, 11017]\n",
      "size of private set:  3000\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  495\n",
      "Total unique points:  2699\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.01\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 17:21:52.130657\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.054121925495565 0.9962857142857143 435.64330887794495 0.268 450.7450387477875 0.2718 311.5632152557373\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11000, 100])\n",
      "val minibatch gradients shape  torch.Size([3000, 100])\n",
      "kernel compute time:  5.837984323501587\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 3000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[10513, 10514, 10515, 10516, 10517]\n",
      "size of private set:  3500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  495\n",
      "Total unique points:  3194\n",
      "overlap % of sel with prev idx:  0.01\n",
      "overlap % of sel with all prev idx:  0.01\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 17:27:46.125772\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.443441329523921 0.991 411.614951133728 0.266 439.3969521522522 0.2875 312.37363386154175\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10500, 100])\n",
      "val minibatch gradients shape  torch.Size([3500, 100])\n",
      "kernel compute time:  5.589942693710327\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 3500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[10013, 10014, 10015, 10016, 10017]\n",
      "size of private set:  4000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  493\n",
      "Total unique points:  3687\n",
      "overlap % of sel with prev idx:  0.01\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 17:33:41.279213\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.585641930811107 0.9904444444444445 376.54112792015076 0.293 400.23504853248596 0.3259 409.8507513999939\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10000, 100])\n",
      "val minibatch gradients shape  torch.Size([4000, 100])\n",
      "kernel compute time:  5.2378318309783936\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer fl1cg -numQueries 4000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5\n",
      "[9513, 9514, 9515, 9516, 9517]\n",
      "size of private set:  4500\n",
      "Num unique points within this selection:  477\n",
      "New unique points:  471\n",
      "Total unique points:  4158\n",
      "overlap % of sel with prev idx:  0.01\n",
      "overlap % of sel with all prev idx:  0.012\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 17:41:16.461487\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.401136875152588 0.9916 380.597043633461 0.323 399.50522804260254 0.3373 465.3440327644348\n"
     ]
    }
   ],
   "source": [
    "fl1cg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl1cg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDETCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdetcg\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/logdetcg/500/2\n",
      "cifar100_duplicate_SIM_logdetcg_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.632797718048096\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 0 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[10353, 10487, 10634, 10644, 1322]\n",
      "size of private set:  500\n",
      "Num unique points within this selection:  424\n",
      "New unique points:  424\n",
      "Total unique points:  424\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 17:49:43.888921\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.6389831667765975 0.99 601.2419514656067 0.118 654.3074841499329 0.1172 68.57554721832275\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13500, 100])\n",
      "val minibatch gradients shape  torch.Size([500, 100])\n",
      "kernel compute time:  3.8308825492858887\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[13013, 13014, 13015, 13016, 13017]\n",
      "size of private set:  1000\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  496\n",
      "Total unique points:  920\n",
      "overlap % of sel with prev idx:  0.004\n",
      "overlap % of sel with all prev idx:  0.004\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 17:51:34.689982\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.300329091958702 0.9926666666666667 560.3874595165253 0.162 599.1265749931335 0.1551 107.91328382492065\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([13000, 100])\n",
      "val minibatch gradients shape  torch.Size([1000, 100])\n",
      "kernel compute time:  3.6859030723571777\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 1000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[12513, 12514, 12515, 12516, 12517]\n",
      "size of private set:  1500\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  494\n",
      "Total unique points:  1414\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.012\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 17:54:31.088949\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.984923557145521 0.9955 510.2653241157532 0.19 545.5540723800659 0.1894 166.72341537475586\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12500, 100])\n",
      "val minibatch gradients shape  torch.Size([1500, 100])\n",
      "kernel compute time:  3.6023595333099365\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 1500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[12013, 12014, 12015, 12016, 12017]\n",
      "size of private set:  2000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  495\n",
      "Total unique points:  1909\n",
      "overlap % of sel with prev idx:  0.008\n",
      "overlap % of sel with all prev idx:  0.01\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 17:59:00.943251\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.652026887983084 0.9912 474.2452586889267 0.22 500.40602827072144 0.2348 200.90976762771606\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([12000, 100])\n",
      "val minibatch gradients shape  torch.Size([2000, 100])\n",
      "kernel compute time:  3.5719153881073\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 2000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[11513, 11514, 11515, 11516, 11517]\n",
      "size of private set:  2500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  494\n",
      "Total unique points:  2403\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.012\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 18:05:04.183134\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.9626546604558825 0.9926666666666667 440.4941631555557 0.25 470.47499227523804 0.2651 243.56848907470703\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11500, 100])\n",
      "val minibatch gradients shape  torch.Size([2500, 100])\n",
      "kernel compute time:  3.5233936309814453\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 2500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[11013, 11014, 11015, 11016, 11017]\n",
      "size of private set:  3000\n",
      "Num unique points within this selection:  500\n",
      "New unique points:  493\n",
      "Total unique points:  2896\n",
      "overlap % of sel with prev idx:  0.012\n",
      "overlap % of sel with all prev idx:  0.014\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 18:12:39.532056\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.145631028804928 0.9934285714285714 412.5573480129242 0.271 447.68159079551697 0.2856 296.00746965408325\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([11000, 100])\n",
      "val minibatch gradients shape  torch.Size([3000, 100])\n",
      "kernel compute time:  3.4358253479003906\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 3000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10513, 10514, 10515, 10516, 10517]\n",
      "size of private set:  3500\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  494\n",
      "Total unique points:  3390\n",
      "overlap % of sel with prev idx:  0.01\n",
      "overlap % of sel with all prev idx:  0.012\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 18:22:14.238478\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.321089299628511 0.9925 406.4350736141205 0.284 420.6192800998688 0.296 361.4678056240082\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10500, 100])\n",
      "val minibatch gradients shape  torch.Size([3500, 100])\n",
      "kernel compute time:  3.2830867767333984\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 3500 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[10013, 10014, 10015, 10016, 10017]\n",
      "size of private set:  4000\n",
      "Num unique points within this selection:  499\n",
      "New unique points:  494\n",
      "Total unique points:  3884\n",
      "overlap % of sel with prev idx:  0.006\n",
      "overlap % of sel with all prev idx:  0.012\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 18:34:13.490379\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.646058871876448 0.9924444444444445 365.9378824830055 0.335 386.1862919330597 0.3368 436.7801139354706\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([10000, 100])\n",
      "val minibatch gradients shape  torch.Size([4000, 100])\n",
      "kernel compute time:  3.266127347946167\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode private -naiveOrRandom naive -magnificationLambda 10 -numSummaries 1 -budget 500 -queryPrivacyOptimizer logdetcg -numQueries 4000 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5 -privateKernelFile /home/snk170001/bioml/dss/notebooks/lake_target_kernel.hdf5 -privateprivateKernelFile /home/snk170001/bioml/dss/notebooks/target_kernel.hdf5\n",
      "[9513, 9514, 9515, 9516, 9517]\n",
      "size of private set:  4500\n",
      "Num unique points within this selection:  461\n",
      "New unique points:  453\n",
      "Total unique points:  4337\n",
      "overlap % of sel with prev idx:  0.01\n",
      "overlap % of sel with all prev idx:  0.016\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 18:48:47.808448\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 12.213962826877832 0.9918 342.23155426979065 0.356 357.7537977695465 0.368 473.2140145301819\n"
     ]
    }
   ],
   "source": [
    "logdetcg_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'logdetcg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/fl/500/2\n",
      "cifar100_duplicate_SIM_fl_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.674696445465088\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3898, 7578, 154, 5561, 1153]\n",
      "Num unique points within this selection:  495\n",
      "New unique points:  495\n",
      "Total unique points:  495\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 18:58:40.224322\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.0964059429243207 0.991 570.2246913909912 0.161 616.7154955863953 0.1311 76.3777928352356\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.137340784072876\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[5730, 12308, 8960, 7453, 3513]\n",
      "Num unique points within this selection:  492\n",
      "New unique points:  295\n",
      "Total unique points:  790\n",
      "overlap % of sel with prev idx:  0.408\n",
      "overlap % of sel with all prev idx:  0.408\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 19:01:43.007360\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.260079935193062 0.99 575.6322748661041 0.134 606.20649766922 0.1493 122.97774004936218\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.699686050415039\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[8614, 4973, 8972, 2613, 5103]\n",
      "Num unique points within this selection:  489\n",
      "New unique points:  188\n",
      "Total unique points:  978\n",
      "overlap % of sel with prev idx:  0.4\n",
      "overlap % of sel with all prev idx:  0.614\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 19:05:25.789656\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.895539444871247 0.994 544.1186120510101 0.167 589.3381094932556 0.1594 134.99830770492554\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.406339406967163\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2701, 5461, 4040, 932, 11397]\n",
      "Num unique points within this selection:  486\n",
      "New unique points:  108\n",
      "Total unique points:  1086\n",
      "overlap % of sel with prev idx:  0.396\n",
      "overlap % of sel with all prev idx:  0.778\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 19:09:14.201635\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.168020327575505 0.9904 542.249368429184 0.178 581.8522410392761 0.1674 139.35489988327026\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.760768890380859\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[5398, 5991, 7736, 7830, 4379]\n",
      "Num unique points within this selection:  490\n",
      "New unique points:  70\n",
      "Total unique points:  1156\n",
      "overlap % of sel with prev idx:  0.406\n",
      "overlap % of sel with all prev idx:  0.858\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 19:13:00.219878\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.4766459641978145 0.9933333333333333 557.3881258964539 0.171 583.8563346862793 0.1739 171.22548460960388\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.417094945907593\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[7445, 4310, 6649, 4476, 2562]\n",
      "Num unique points within this selection:  487\n",
      "New unique points:  54\n",
      "Total unique points:  1210\n",
      "overlap % of sel with prev idx:  0.412\n",
      "overlap % of sel with all prev idx:  0.892\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 19:17:11.441164\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.950823306106031 0.9905714285714285 534.0286936759949 0.174 577.2754082679749 0.178 160.49696588516235\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.943092346191406\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[5098, 5298, 4047, 883, 58]\n",
      "Num unique points within this selection:  486\n",
      "New unique points:  39\n",
      "Total unique points:  1249\n",
      "overlap % of sel with prev idx:  0.4\n",
      "overlap % of sel with all prev idx:  0.92\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 19:21:05.697270\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.908254033420235 0.992 538.100982427597 0.162 570.3845462799072 0.1689 180.01265668869019\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.654448509216309\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2376, 6099, 1114, 6265, 1697]\n",
      "Num unique points within this selection:  490\n",
      "New unique points:  44\n",
      "Total unique points:  1293\n",
      "overlap % of sel with prev idx:  0.378\n",
      "overlap % of sel with all prev idx:  0.912\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 19:25:16.326214\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.164572822162881 0.9902222222222222 532.4717454910278 0.189 565.5730218887329 0.1755 188.75535082817078\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.094037771224976\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3049, 4553, 416, 3018, 4534]\n",
      "Num unique points within this selection:  486\n",
      "New unique points:  47\n",
      "Total unique points:  1340\n",
      "overlap % of sel with prev idx:  0.408\n",
      "overlap % of sel with all prev idx:  0.906\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 19:29:28.066704\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.030077196424827 0.9962 500.0066237449646 0.197 536.0019249916077 0.1867 228.52724075317383\n"
     ]
    }
   ],
   "source": [
    "fl_test_acc, selected_idx = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM gc\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/gc/500/1\n",
      "cifar100_duplicate_SIM_gc_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.658437490463257\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[1575, 687, 3368, 6525, 54]\n",
      "Num unique points within this selection:  245\n",
      "New unique points:  245\n",
      "Total unique points:  245\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 19:33:57.349792\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.4798969347029924 0.992 638.6064236164093 0.11 698.7356581687927 0.1083 70.91008019447327\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.15013861656189\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2109, 10538, 3492, 7945, 12681]\n",
      "Num unique points within this selection:  236\n",
      "New unique points:  231\n",
      "Total unique points:  476\n",
      "overlap % of sel with prev idx:  0.028\n",
      "overlap % of sel with all prev idx:  0.028\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 19:35:41.492198\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.4301726976409554 0.992 610.0815560817719 0.123 672.66059923172 0.121 89.98208165168762\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.753023862838745\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[10778, 8864, 12452, 9536, 818]\n",
      "Num unique points within this selection:  262\n",
      "New unique points:  242\n",
      "Total unique points:  718\n",
      "overlap % of sel with prev idx:  0.044\n",
      "overlap % of sel with all prev idx:  0.078\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 19:37:43.067040\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.3981912466697395 0.9955 559.2424819469452 0.153 622.9401841163635 0.1428 111.76619005203247\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.444525241851807\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[5425, 10712, 9216, 2106, 6945]\n",
      "Num unique points within this selection:  275\n",
      "New unique points:  235\n",
      "Total unique points:  953\n",
      "overlap % of sel with prev idx:  0.044\n",
      "overlap % of sel with all prev idx:  0.204\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 19:40:06.383306\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.545878611505032 0.9912 534.5510365962982 0.144 587.2554869651794 0.1493 129.8500156402588\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.994110107421875\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[4252, 4168, 5607, 4100, 3683]\n",
      "Num unique points within this selection:  274\n",
      "New unique points:  217\n",
      "Total unique points:  1170\n",
      "overlap % of sel with prev idx:  0.078\n",
      "overlap % of sel with all prev idx:  0.262\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 19:42:45.200931\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.71935252379626 0.9913333333333333 533.9424679279327 0.168 572.0351753234863 0.166 183.36907052993774\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.240545749664307\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[1135, 6688, 6535, 5824, 5559]\n",
      "Num unique points within this selection:  275\n",
      "New unique points:  228\n",
      "Total unique points:  1398\n",
      "overlap % of sel with prev idx:  0.082\n",
      "overlap % of sel with all prev idx:  0.264\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 19:46:15.180174\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.086085196817294 0.9922857142857143 506.45006275177 0.182 545.3562870025635 0.1863 200.96248197555542\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.86660623550415\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[1413, 8180, 7218, 6377, 6197]\n",
      "Num unique points within this selection:  318\n",
      "New unique points:  240\n",
      "Total unique points:  1638\n",
      "overlap % of sel with prev idx:  0.052\n",
      "overlap % of sel with all prev idx:  0.282\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 19:50:01.549848\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.820656567346305 0.99125 491.48640871047974 0.2 527.5676927566528 0.1885 240.34490823745728\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.368710517883301\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2919, 2953, 6245, 9060, 10209]\n",
      "Num unique points within this selection:  292\n",
      "New unique points:  200\n",
      "Total unique points:  1838\n",
      "overlap % of sel with prev idx:  0.07\n",
      "overlap % of sel with all prev idx:  0.424\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 19:54:25.635529\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.149655094894115 0.9962222222222222 481.3032989501953 0.21 510.91263914108276 0.2065 287.454256772995\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.064566373825073\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 500 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3651, 3119, 2378, 4172, 7307]\n",
      "Num unique points within this selection:  301\n",
      "New unique points:  188\n",
      "Total unique points:  2026\n",
      "overlap % of sel with prev idx:  0.086\n",
      "overlap % of sel with all prev idx:  0.466\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 19:59:37.600941\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.1513982026372105 0.9956 451.85815715789795 0.228 492.6343004703522 0.229 306.79491782188416\n"
     ]
    }
   ],
   "source": [
    "gc_test_acc, selected_idx = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdet\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/logdet/500/1\n",
      "cifar100_duplicate_SIM_logdet_budget:500_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.642914533615112\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[11457, 2395, 2510, 11568, 11203]\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  498\n",
      "Total unique points:  498\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 20:05:34.983558\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.369558642618358 0.993 579.9189698696136 0.134 635.4109206199646 0.1217 88.82849168777466\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  7.207124471664429\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[11170, 12513, 8694, 8911, 6187]\n",
      "Num unique points within this selection:  496\n",
      "New unique points:  452\n",
      "Total unique points:  950\n",
      "overlap % of sel with prev idx:  0.088\n",
      "overlap % of sel with all prev idx:  0.088\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 20:07:45.384054\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.24062667042017 0.9906666666666667 543.2369549274445 0.148 561.3203182220459 0.1637 123.56008672714233\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.869152784347534\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[11837, 12539, 10698, 8932, 10983]\n",
      "Num unique points within this selection:  495\n",
      "New unique points:  431\n",
      "Total unique points:  1381\n",
      "overlap % of sel with prev idx:  0.076\n",
      "overlap % of sel with all prev idx:  0.134\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 20:10:27.289226\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.400377202779055 0.9925 481.30502939224243 0.213 517.321207523346 0.2062 155.17855620384216\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  6.305836200714111\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[4117, 11231, 10042, 9615, 2485]\n",
      "Num unique points within this selection:  496\n",
      "New unique points:  383\n",
      "Total unique points:  1764\n",
      "overlap % of sel with prev idx:  0.098\n",
      "overlap % of sel with all prev idx:  0.23\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 20:13:38.063004\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 7.010244223754853 0.9912 491.50249767303467 0.214 519.3035807609558 0.2093 205.40392589569092\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.904811859130859\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3691, 10936, 5522, 4857, 11961]\n",
      "Num unique points within this selection:  497\n",
      "New unique points:  366\n",
      "Total unique points:  2130\n",
      "overlap % of sel with prev idx:  0.1\n",
      "overlap % of sel with all prev idx:  0.262\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 20:17:36.894285\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.433314726687968 0.9906666666666667 443.17249512672424 0.247 475.71262216567993 0.2443 223.1085979938507\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  5.368788719177246\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[11366, 9874, 10258, 11450, 9319]\n",
      "Num unique points within this selection:  498\n",
      "New unique points:  327\n",
      "Total unique points:  2457\n",
      "overlap % of sel with prev idx:  0.104\n",
      "overlap % of sel with all prev idx:  0.344\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 20:21:51.976023\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.643833872163668 0.9934285714285714 429.72372245788574 0.252 457.8021500110626 0.269 303.7759790420532\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.954448699951172\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[8879, 10221, 10548, 8882, 10007]\n",
      "Num unique points within this selection:  494\n",
      "New unique points:  316\n",
      "Total unique points:  2773\n",
      "overlap % of sel with prev idx:  0.112\n",
      "overlap % of sel with all prev idx:  0.364\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 20:27:25.523859\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.860343836247921 0.99075 419.86527919769287 0.263 450.8119201660156 0.2696 331.55470037460327\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.591123819351196\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[10183, 4414, 9811, 8745, 9856]\n",
      "Num unique points within this selection:  496\n",
      "New unique points:  295\n",
      "Total unique points:  3068\n",
      "overlap % of sel with prev idx:  0.114\n",
      "overlap % of sel with all prev idx:  0.41\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 20:33:25.163985\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.162072711624205 0.9908888888888889 403.3681662082672 0.285 432.18535256385803 0.2879 363.6869077682495\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  4.143878221511841\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[9549, 8318, 6865, 8532, 5924]\n",
      "Num unique points within this selection:  495\n",
      "New unique points:  261\n",
      "Total unique points:  3329\n",
      "overlap % of sel with prev idx:  0.134\n",
      "overlap % of sel with all prev idx:  0.478\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 20:39:55.389216\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.576964249368757 0.9902 396.38047552108765 0.3 414.8404903411865 0.3028 429.64368629455566\n"
     ]
    }
   ],
   "source": [
    "logdet_test_acc, selected_idx = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'logdet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-100 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  14000\n",
      "Budget:  500\n",
      "CG_active_learning_results/cifar100/duplicate/random/500/2\n",
      "cifar100_duplicate_random_random_budget:500_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Num unique points within this selection:  444\n",
      "New unique points:  444\n",
      "Total unique points:  444\n",
      "overlap % of sel with prev idx:  0.0\n",
      "overlap % of sel with all prev idx:  0.0\n",
      "selEpoch: 1, Selection Ended at: 2021-04-04 17:20:00.943072\n",
      "After augmentation, size of train_set:  1000  lake set:  13500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.340421751141548 0.99 612.1731848716736 0.108 663.2503147125244 0.115 77.20470333099365\n",
      "AL epoch:  2\n",
      "Num unique points within this selection:  448\n",
      "New unique points:  348\n",
      "Total unique points:  792\n",
      "overlap % of sel with prev idx:  0.224\n",
      "overlap % of sel with all prev idx:  0.224\n",
      "selEpoch: 2, Selection Ended at: 2021-04-04 17:21:22.990592\n",
      "After augmentation, size of train_set:  1500  lake set:  13000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.567542855627835 0.9913333333333333 573.0062410831451 0.14 606.5828218460083 0.1525 121.19595575332642\n",
      "AL epoch:  3\n",
      "Num unique points within this selection:  450\n",
      "New unique points:  316\n",
      "Total unique points:  1108\n",
      "overlap % of sel with prev idx:  0.168\n",
      "overlap % of sel with all prev idx:  0.304\n",
      "selEpoch: 3, Selection Ended at: 2021-04-04 17:23:28.625353\n",
      "After augmentation, size of train_set:  2000  lake set:  12500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.18127512652427 0.99 531.4661483764648 0.163 582.5213580131531 0.1709 159.6516420841217\n",
      "AL epoch:  4\n",
      "Num unique points within this selection:  444\n",
      "New unique points:  261\n",
      "Total unique points:  1369\n",
      "overlap % of sel with prev idx:  0.194\n",
      "overlap % of sel with all prev idx:  0.43\n",
      "selEpoch: 4, Selection Ended at: 2021-04-04 17:26:13.172866\n",
      "After augmentation, size of train_set:  2500  lake set:  12000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.098089906387031 0.9924 503.37067890167236 0.186 543.7405362129211 0.1862 184.18340301513672\n",
      "AL epoch:  5\n",
      "Num unique points within this selection:  450\n",
      "New unique points:  214\n",
      "Total unique points:  1583\n",
      "overlap % of sel with prev idx:  0.218\n",
      "overlap % of sel with all prev idx:  0.538\n",
      "selEpoch: 5, Selection Ended at: 2021-04-04 17:29:22.296476\n",
      "After augmentation, size of train_set:  3000  lake set:  11500\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.41431223321706 0.99 516.4566371440887 0.196 542.2680134773254 0.2005 205.23872470855713\n",
      "AL epoch:  6\n",
      "Num unique points within this selection:  441\n",
      "New unique points:  186\n",
      "Total unique points:  1769\n",
      "overlap % of sel with prev idx:  0.18\n",
      "overlap % of sel with all prev idx:  0.61\n",
      "selEpoch: 6, Selection Ended at: 2021-04-04 17:32:52.460604\n",
      "After augmentation, size of train_set:  3500  lake set:  11000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.7826107130385935 0.9925714285714285 470.1091525554657 0.229 516.8687229156494 0.208 253.89775609970093\n",
      "AL epoch:  7\n",
      "Num unique points within this selection:  455\n",
      "New unique points:  174\n",
      "Total unique points:  1943\n",
      "overlap % of sel with prev idx:  0.21\n",
      "overlap % of sel with all prev idx:  0.642\n",
      "selEpoch: 7, Selection Ended at: 2021-04-04 17:37:11.402869\n",
      "After augmentation, size of train_set:  4000  lake set:  10500\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.836441901046783 0.991 481.90336513519287 0.227 503.09625339508057 0.2289 275.29738450050354\n",
      "AL epoch:  8\n",
      "Num unique points within this selection:  439\n",
      "New unique points:  159\n",
      "Total unique points:  2102\n",
      "overlap % of sel with prev idx:  0.214\n",
      "overlap % of sel with all prev idx:  0.662\n",
      "selEpoch: 8, Selection Ended at: 2021-04-04 17:41:51.509550\n",
      "After augmentation, size of train_set:  4500  lake set:  10000\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.037876130547374 0.9913333333333333 467.43076622486115 0.217 491.35480785369873 0.2253 266.96796464920044\n",
      "AL epoch:  9\n",
      "Num unique points within this selection:  451\n",
      "New unique points:  179\n",
      "Total unique points:  2281\n",
      "overlap % of sel with prev idx:  0.192\n",
      "overlap % of sel with all prev idx:  0.64\n",
      "selEpoch: 9, Selection Ended at: 2021-04-04 17:46:23.371787\n",
      "After augmentation, size of train_set:  5000  lake set:  9500\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 12.042022319510579 0.99 453.7250244617462 0.235 483.81643891334534 0.2394 299.5362665653229\n"
     ]
    }
   ],
   "source": [
    "random_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 2, device, False, \"random\",'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  1000 Lake size:  5000\n",
      "Budget:  500 selected every:  2\n",
      "CG_active_learning_results/fl/cifar10/500/2\n",
      "cifar10_budget:500_epochs:5_selEvery:2_variant_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Epoch: 1 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0976276411674917 0.99 300.9132677912712 0.422 338.40636587142944 0.4212 78.468909740448\n",
      "saving initial model\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  2.3506908416748047\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[2380, 3494, 1470, 997, 2472]\n",
      "selEpoch: 1, Selection Ended at: 2021-03-29 22:38:57.727570\n",
      "After augmentation, size of train_set:  1000  lake set:  4500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.527952621690929 0.994 258.5853514075279 0.508 281.0728050470352 0.511 156.07492542266846\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.9650421142578125\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[4179, 4268, 4033, 1890, 696]\n",
      "Num unique points within this selection:  326\n",
      "New unique points:  135\n",
      "Total unique points:  456\n",
      "overlap % of sel with prev idx:  0.592\n",
      "overlap % of sel with all prev idx:  0.592\n",
      "selEpoch: 2, Selection Ended at: 2021-03-29 22:41:46.896948\n",
      "After augmentation, size of train_set:  1500  lake set:  4000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.741335757309571 0.9906666666666667 200.10388906300068 0.584 228.39167082309723 0.5871 255.1613154411316\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.740058183670044\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[3392, 3202, 518, 2713, 2292]\n",
      "Num unique points within this selection:  321\n",
      "New unique points:  28\n",
      "Total unique points:  484\n",
      "overlap % of sel with prev idx:  0.664\n",
      "overlap % of sel with all prev idx:  0.896\n",
      "selEpoch: 3, Selection Ended at: 2021-03-29 22:46:14.044528\n",
      "After augmentation, size of train_set:  2000  lake set:  3500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.0933011858724058 0.9925 151.94851377606392 0.655 168.8060017824173 0.6468 330.9352750778198\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  1.4325330257415771\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 500 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/lake_kernel.hdf5\n",
      "[636, 3142, 2007, 1573, 3263]\n",
      "Num unique points within this selection:  317\n",
      "New unique points:  14\n",
      "Total unique points:  498\n",
      "overlap % of sel with prev idx:  0.608\n",
      "overlap % of sel with all prev idx:  0.952\n",
      "selEpoch: 4, Selection Ended at: 2021-03-29 22:51:56.191210\n",
      "After augmentation, size of train_set:  2500  lake set:  3000\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.007301649078727 0.992 136.26833433657885 0.68 141.59341114759445 0.6916 374.1799874305725\n"
     ]
    }
   ],
   "source": [
    "fl_vanilla_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, \"vanilla\", model_name, budget, split_cfg, learning_rate, 2, device, False, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badge_vanilla_test_acc = train_model_al(datkbuildPath, exePath, 8, data_name, datadir, \"vanilla\", model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",'badge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading accuracies\n",
    "badge_test_acc_p = [round(float(x)*100, 2) for x in badge_test_acc]\n",
    "gccg_test_acc_p = [round(float(x)*100, 2) for x in gccg_test_acc]\n",
    "us_test_acc_p = [round(float(x)*100, 2) for x in us_test_acc]\n",
    "fl1cg_test_acc_p = [round(float(x)*100, 2) for x in fl1cg_test_acc]\n",
    "logdetcg_test_acc_p = [round(float(x)*100, 2) for x in logdetcg_test_acc]\n",
    "random_test_acc_p = [round(float(x)*100, 2) for x in random_test_acc]\n",
    "fl_test_acc_p = [round(float(x)*100, 2) for x in fl_test_acc]\n",
    "fl_vanilla_test_acc_p = [round(float(x)*100, 2) for x in fl_vanilla_test_acc]\n",
    "badge_vanilla_test_acc_p = [round(float(x)*100, 2) for x in badge_vanilla_test_acc]\n",
    "\n",
    "badge_test_acc_p[0] = 31\n",
    "gccg_test_acc_p[0] = 31\n",
    "us_test_acc_p[0] = 31\n",
    "fl1cg_test_acc_p[0] = 31\n",
    "logdetcg_test_acc_p[0] = 31\n",
    "random_test_acc_p[0] = 31\n",
    "fl_test_acc_p[0] = 31\n",
    "fl_vanilla_test_acc_p[0] = 31 \n",
    "badge_vanilla_test_acc_p[0] = 31\n",
    "\n",
    "# badge_test_acc_p = [31.26, 36.07, 38.31, 35.87, 39.29, 38.2, 40.72, 40.13, 41.36, 43.35]\n",
    "# gccg_test_acc_p = [32.56, 36.25, 35.38, 34.29, 36.53, 35.66, 36.88, 36.25, 36.35, 37.17]\n",
    "# us_test_acc_p = [33.07, 35.47, 34.69, 36.69, 32.45, 33.67, 34.49, 33.58, 34.15, 33.97]\n",
    "# fl1cg_test_acc_p = [32.02, 37.94, 37.11, 37.42, 37.01, 36.4, 36.64, 37.06, 36.0, 36.82]\n",
    "# logdetcg_test_acc_p = [33.67, 37.01, 38.88, 37.46, 37.04, 36.1, 37.15, 36.33, 39.8, 36.04]\n",
    "# random_test_acc_p = [33.2, 37.72, 38.84, 39.86, 41.05, 38.98, 40.87, 41.46, 42.72, 42.92]\n",
    "# fl_test_acc_p = [33.02, 34.81, 37.76, 39.46, 40.07, 39.7, 43.65, 42.35, 41.76, 42.04]\n",
    "# fl_vanilla_test_acc_p = [32.75, 37.96, 36.96, 39.31, 41.59, 42.5, 39.93, 42.21, 43.33, 45.26]\n",
    "# badge_vanilla_test_acc_p = [32.86, 36.08, 37.65, 38.73, 41.16, 40.53, 41.31, 41.28, 44.42, 43.74]\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "n_rounds = 9\n",
    "x_axis = np.array([500+budget*i for i in range(n_rounds-1)])\n",
    "plt.figure()\n",
    "plt.plot(x_axis, gccg_test_acc_p[:8], 'b-', label='GCCG',marker='o')\n",
    "plt.plot(x_axis, fl1cg_test_acc_p[:8], 'm-', label='FL1CG',marker='o')\n",
    "plt.plot(x_axis, logdetcg_test_acc_p[:8], 'y-', label='LOGDETCG',marker='o')\n",
    "plt.plot(x_axis, fl_test_acc_p[:8], 'p-', label='FL',marker='o')\n",
    "plt.plot(x_axis, us_test_acc_p[:8], 'g-', label='UNCERTAINITY',marker='o')\n",
    "plt.plot(x_axis, badge_test_acc_p[:8], 'c', label='BADGE',marker='o')\n",
    "plt.plot(x_axis, random_test_acc_p[:8], 'r', label='RANDOM',marker='o')\n",
    "plt.plot(x_axis, fl_vanilla_test_acc_p[:8], 'v-', label='FL_v',marker='o')\n",
    "plt.plot(x_axis, badge_vanilla_test_acc_p[:8], 'h-', label='BADGE_v',marker='o')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('No of Images')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title(\"Comparison with CG functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'badge_test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8cf990e17d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadge_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(badge_vanilla_test_acc_p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_test_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl_vanilla_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl1cg_test_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'badge_test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "print(badge_test_acc[0])\n",
    "# print(badge_vanilla_test_acc_p)\n",
    "print(fl_test_acc)\n",
    "print(fl_vanilla_test_acc[0])\n",
    "print(fl1cg_test_acc[0])\n",
    "# print(gccg_test_acc[0])\n",
    "print(us_test_acc[0])\n",
    "print(logdetcg_test_acc[0])\n",
    "# print(random_test_acc_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badge_vanilla_test_acc_p = [round(float(x)*100, 2) for x in badge_vanilla_test_acc]\n",
    "print(badge_vanilla_test_acc_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trn_batch_size = 20\n",
    "val_batch_size = 10\n",
    "tst_batch_size = 100\n",
    "\n",
    "split_cfg_dup = {\"train_size\":500, \"val_size\":1000, \"lake_size\":1000, \"num_rep\":4}\n",
    "\n",
    "\n",
    "_, _, test_set, lake_set_dup, num_cls = load_dataset_custom(datadir, \"cifar10\", \"duplicate\", split_cfg_dup)\n",
    "\n",
    "print(len(lake_set_dup))\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_dup, batch_size=trn_batch_size,\n",
    "                                          shuffle=True, pin_memory=True)\n",
    "\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cfg_va = {\"train_size\":500, \"val_size\":1000, \"lake_size\":4000, \"num_rep\":4}\n",
    "_, _, test_set, lake_set_va, num_cls = load_dataset_custom(datadir, \"cifar10\", \"vanilla\", split_cfg_va)\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_va, batch_size=trn_batch_size,\n",
    "                                          shuffle=False, pin_memory=True)\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cfg_va = {\"train_size\":500, \"val_size\":1000, \"lake_size\":3623, \"num_rep\":4}\n",
    "_, _, test_set, lake_set_va, num_cls = load_dataset_custom(datadir, \"cifar10\", \"vanilla\", split_cfg_va)\n",
    "trainloader = torch.utils.data.DataLoader(lake_set_va, batch_size=trn_batch_size,\n",
    "                                          shuffle=False, pin_memory=True)\n",
    "tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "model = create_model(model_name, num_cls, device)\n",
    "criterion,_ = loss_function()\n",
    "optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "full_trn_acc = 0\n",
    "while(full_trn_acc<0.99):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "        # Variables in Pytorch are differentiable.\n",
    "        inputs, target = Variable(inputs), Variable(inputs)\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "    full_trn_loss = 0\n",
    "    full_trn_correct = 0\n",
    "    full_trn_total = 0\n",
    "    tst_loss = 0\n",
    "    tst_correct = 0\n",
    "    tst_total = 0\n",
    "    final_val_predictions = []\n",
    "    final_val_classifications = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            full_trn_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            full_trn_total += targets.size(0)\n",
    "            full_trn_correct += predicted.eq(targets).sum().item()\n",
    "        full_trn_acc = full_trn_correct / full_trn_total\n",
    "        for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            tst_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            tst_total += targets.size(0)\n",
    "            tst_correct += predicted.eq(targets).sum().item()\n",
    "            tst_acc = tst_correct / tst_total\n",
    "        print(\"acc so far (train, test): \", full_trn_acc, tst_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = [1,4,7,8,10]\n",
    "set2 = [2,6,3,1,9]\n",
    "print(len(set(set1)-set(set2)))\n",
    "print(len(set(set1+set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp([-1,-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
