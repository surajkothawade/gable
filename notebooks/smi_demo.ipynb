{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from cords.cords.selectionstrategies.supervisedlearning import DataSelectionStrategy\n",
    "from cords.cords.utils.models import ResNet18\n",
    "from gable.gable.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "# for cuda\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class custom_subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "        labels(sequence) : targets as required for the indices. will be the same length as indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, labels):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.targets = labels.type(torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target = self.targets[idx]\n",
    "        return (image, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device):\n",
    "    if name == 'ResNet18':\n",
    "        model = ResNet18(num_cls)\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def kernel(x, y, measure=\"cosine\", exp=2):\n",
    "    if(measure==\"eu_sim\"):\n",
    "        dist = pairwise_distances(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = max(dist.ravel()) - dist\n",
    "#         n = x.size(0)\n",
    "#         m = y.size(0)\n",
    "#         d = x.size(1)\n",
    "#         x = x.unsqueeze(1).expand(n, m, d)\n",
    "#         y = y.unsqueeze(0).expand(n, m, d)\n",
    "#         dist = torch.pow(x - y, exp).sum(2)\n",
    "#         const = torch.max(dist).item()\n",
    "#         sim = (const - dist)\n",
    "    \n",
    "        #dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))\n",
    "    if(measure==\"cosine\"):\n",
    "        sim = cosine_similarity(x.cpu().numpy(), y.cpu().numpy())\n",
    "    return sim\n",
    "\n",
    "\n",
    "def save_kernel_hdf5(lake_kernel, lake_target_kernel, target_kernel=[], numpy=True):\n",
    "    if(not(numpy)):\n",
    "        lake_kernel = lake_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_target_kernel = lake_target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_target_kernel)\n",
    "    if(not(numpy)):\n",
    "        target_kernel = target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=target_kernel)\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "#     saveDir = os.path.join(saveDir, prefix)\n",
    "#     if(not(os.path.exists(saveDir))):\n",
    "#         os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    class_err_log = []\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        print(\"val, test error% for class \", i, \" : \", round((len(val_err_class_idx)/len(val_class_idxs))*100,2), round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        class_err_log.append(\"val, test error% for class \"+ str(i) + \" : \"+ str(round((len(val_err_class_idx)/len(val_class_idxs))*100,2)) + \", \" + str(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)))\n",
    "        tst_err_log.append(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    return tst_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = custom_subset(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    return aug_train_set, remain_lake_set\n",
    "                        \n",
    "def getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getMisclsSetNumpy(X_val, y_val, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return X_val[miscls_idx], y_val[miscls_idx]\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def getSMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, sf):\n",
    "    if(sf==\"fl1mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel.hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf == \"logdetmi\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \"  -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel.hdf5\") + \" -queryqueryKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl2mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gcmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gccg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl1cg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"logdetcg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\") + \" -privateprivateKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl\" or sf==\"logdet\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\")\n",
    "    elif(sf ==\"gc\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\")\n",
    "    print(\"Executing SIM command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "def remove_ood_points(lake_set, subset, idc_idx):\n",
    "    idx_subset = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in idc_idx:\n",
    "        idc_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        idx_subset += list(np.array(subset)[idc_subset_idx])\n",
    "    print(len(idx_subset),\"/\",len(subset), \" idc points.\")\n",
    "    return idx_subset\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "#check overlap with prev selections\n",
    "def check_overlap(prev_idx, prev_idx_hist, idx):\n",
    "    prev_idx = [int(x/num_rep) for x in prev_idx]\n",
    "    prev_idx_hist = [int(x/num_rep) for x in prev_idx_hist]\n",
    "    idx = [int(x/num_rep) for x in idx]\n",
    "    # overlap = set(prev_idx).intersection(set(idx))\n",
    "    overlap = [value for value in idx if value in prev_idx] \n",
    "    # overlap_hist = set(prev_idx_hist).intersection(set(idx))\n",
    "    overlap_hist = [value for value in idx if value in prev_idx_hist]\n",
    "    new_points = set(idx) - set(prev_idx_hist)\n",
    "    total_unique_points = set(idx+prev_idx_hist)\n",
    "    print(\"New unique points: \", len(new_points))\n",
    "    print(\"Total unique points: \", len(total_unique_points))\n",
    "    print(\"overlap % of sel with prev idx: \", len(overlap)/len(idx))\n",
    "    print(\"overlap % of sel with all prev idx: \", len(overlap_hist)/len(idx))\n",
    "    return len(overlap)/len(idx), len(overlap_hist)/len(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "datadir = 'data/'\n",
    "data_name = 'cifar10'\n",
    "num_cls=10\n",
    "fraction = float(0.1)\n",
    "budget=30\n",
    "num_epochs = int(10)\n",
    "num_rep = 10\n",
    "model_name = 'ResNet18'\n",
    "learning_rate = 0.01\n",
    "# feature='vanilla'\n",
    "# split_cfg = {\"train_size\":500, \"val_size\":1000, \"lake_size\":5000, \"num_rep\":num_rep, \"lake_subset_repeat_size\":1000}\n",
    "# feature = 'duplicate'\n",
    "feature = 'classimb'\n",
    "split_cfg = {\"num_cls_imbalance\":2, \"per_imbclass_train\":10, \"per_imbclass_val\":5, \"per_imbclass_lake\":150, \"per_class_train\":200, \"per_class_val\":5, \"per_class_lake\":3000} #cifar10\n",
    "# split_cfg = {\"num_cls_imbalance\":2, \"per_imbclass_train\":10, \"per_imbclass_val\":5, \"per_imbclass_lake\":75, \"per_class_train\":200, \"per_class_val\":5, \"per_class_lake\":295} #cifar100\n",
    "initModelPath = data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_imbclass_train\"]) + \"_\" + str(split_cfg[\"per_class_train\"]) + \"_\" + str(split_cfg[\"num_cls_imbalance\"])\n",
    "# feature = 'ood'\n",
    "# split_cfg = {'num_cls_idc':5, 'per_idc_train':100, 'per_idc_val':10, 'per_idc_lake':500, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':5000}#cifar10\n",
    "# split_cfg = {'num_cls_idc':50, 'per_idc_train':100, 'per_idc_val':2, 'per_idc_lake':100, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':500}#cifar100\n",
    "# initModelPath = \"weights/\"+data_name + \"_\" + feature + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_idc_train\"]) + \"_\" + str(split_cfg[\"per_idc_val\"]) + \"_\" + str(split_cfg[\"num_cls_idc\"])\n",
    "num_runs = 1  # number of random runs\n",
    "computeClassErrorLog = True\n",
    "\n",
    "magnification = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "datkbuildPath = \"/home/snk170001/bioml/dss/notebooks/datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "print(\"Using Device:\", device)\n",
    "doublePrecision = False\n",
    "linearLayer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distil.distil.active_learning_strategies import BADGE, EntropySampling, GLISTER\n",
    "from distil.distil.utils.DataHandler import DataHandler_CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    if(feature==\"classimb\" or feature==\"ood\"):\n",
    "        if(strategy == \"SIM\" or strategy == \"SF\" or strategy==\"random\"):\n",
    "            if(strategy == \"SF\" or strategy==\"random\"):\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, True)\n",
    "            else:\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "        elif(strategy==\"AL\"):\n",
    "            if(sf==\"badge\" or sf==\"us\"):\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, True)\n",
    "            else: #dont augment train with valid\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "        print(\"selected classes are: \", sel_cls_idx)\n",
    "    if(feature==\"duplicate\" or feature==\"vanilla\"):\n",
    "        sel_cls_idx = None\n",
    "        if(strategy == \"SIM\" or strategy==\"random\"):\n",
    "            train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        elif(strategy==\"AL\"):\n",
    "            X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True)\n",
    "        \n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "    all_logs_dir = 'SMI_active_learning_results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \"feature\":feature, \"sel_func\":sf, \"sel_budget\":budget, \"num_selections\":num_epochs, \"model\":model_name, \"learning_rate\":learning_rate, \"setting\":split_cfg, \"all_class_acc\":None, \"test_acc\":[],\"sel_per_cls\":[], \"sel_cls_idx\":sel_cls_idx.tolist()}\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device)\n",
    "    model1 = create_model(model_name, num_cls, device)\n",
    "    if(strategy == \"AL\"):\n",
    "        strategy_args = {'batch_size' : budget, 'lr':float(0.001)}\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(X_tr.astype(np.float64), y_tr, X_unlabeled.astype(np.float64), model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"glister\" or sf==\"glister-tss\"):\n",
    "            strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args, valid=True, X_val=X_val, Y_val=y_val, typeOf='rand', lam=0.1)\n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "#     optimizer, scheduler = optimizer_with_scheduler(model, num_epochs, learning_rate)\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "    #overlap vars\n",
    "    prev_idx = None\n",
    "    prev_idx_hist = []\n",
    "    sel_hist = []\n",
    "    per_ep_overlap = []\n",
    "    overall_overlap = []\n",
    "    idx_tracker = np.array(list(range(len(lake_set))))\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        final_tst_predictions = []\n",
    "                        final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        if((val_correct/val_total) > best_val_acc):\n",
    "                            final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                            final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "                continue\n",
    "        else:\n",
    "#             if(full_trn_acc[i-1] >= 0.99): #The model has already trained on the seed dataset\n",
    "            #use misclassifications on validation set as queries\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append(tst_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set = getMisclsSet(val_set, val_class_err_idxs, sel_cls_idx)\n",
    "                        misclsloader = torch.utils.data.DataLoader(miscls_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, misclsloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                elif(sf.endswith(\"cg\")): #atleast one selection must be done for private set in cond gain functions\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, privateSetloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        #compute subset with private set a NULL\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                else:\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                start_time = time.time()\n",
    "                cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "                clone_dict = copy.deepcopy(model.state_dict())\n",
    "                #update the selection strategy model with new params for gradient computation\n",
    "                setf_model.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mi\")): #SMI functions need the target set gradients\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "#                     print(setf_model.grads_per_elem)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "#                     print(setf_model.val_grads_per_elem)\n",
    "                    if(doublePrecision):\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    else:\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                    numQueryPrivate = train_val_kernel.shape[1]\n",
    "                elif(sf.endswith(\"cg\")):\n",
    "                    if(len(private_set)!=0):\n",
    "                        setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                        print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                        print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                        if(doublePrecision):\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                        else:\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                        numQueryPrivate = train_val_kernel.shape[1]\n",
    "                    else:\n",
    "#                         assert(((i + 1)/select_every)==1)\n",
    "                        setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        train_val_kernel = []\n",
    "                        numQueryPrivate = 0\n",
    "                else: # For other submodular functions needing only image kernel\n",
    "                    setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                    train_val_kernel = []\n",
    "                    numQueryPrivate = 0\n",
    "\n",
    "                kernel_time = time.time()\n",
    "                if(doublePrecision):\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.grads_per_elem.double()) #img_img_kernel\n",
    "                else:\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem, setf_model.grads_per_elem) #img_img_kernel\n",
    "                if(sf==\"logdetmi\" or sf==\"logdetcg\"):\n",
    "                    if(sf==\"logdetcg\"):\n",
    "                        if(len(private_set)!=0):\n",
    "                            val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                        else:\n",
    "                            val_kernel = []\n",
    "                    if(sf==\"logdetmi\"):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel, val_kernel)\n",
    "                else:\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel)\n",
    "                print(\"kernel compute time: \", time.time()-kernel_time)\n",
    "                #call the c++ exec to read kernel and compute subset of selected minibatches\n",
    "                subset = getSMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQueryPrivate), sf)\n",
    "                print(subset[:5])\n",
    "                model.load_state_dict(cached_state_dict)\n",
    "                if(sf.endswith(\"cg\")): #for first selection\n",
    "                    if(len(private_set)==0):\n",
    "                        private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "                    else:\n",
    "                        private_set = getPrivateSet(lake_set, subset, private_set)\n",
    "                    print(\"size of private set: \", len(private_set))\n",
    "\n",
    "    #           temp = np.array(list(trainloader.batch_sampler))[subset] #if per batch\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\"):\n",
    "                    miscls_X_val, miscls_y_val = getMisclsSetNumpy(X_val, y_val, val_class_err_idxs, sel_cls_idx)\n",
    "                    strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args, valid=True, X_val=miscls_X_val, Y_val=miscls_y_val, typeOf='rand', lam=0.1)\n",
    "                    print(\"reinit Glister with targeted miscls samples\")\n",
    "                strategy_sel.update_model(model)\n",
    "                if(sf==\"badge\" or sf==\"glister\" or sf==\"glister-tss\"):\n",
    "                    subset = strategy_sel.select(budget)\n",
    "                if(sf==\"us\"):\n",
    "                    subset = list(strategy_sel.select(budget).cpu().numpy())\n",
    "                print(len(subset), \" samples selected\")\n",
    "                X_tr = np.concatenate((X_tr, X_unlabeled[subset]), axis=0)\n",
    "                X_unlabeled = np.delete(X_unlabeled, subset, axis = 0)\n",
    "                y_tr = np.concatenate((y_tr, y_unlabeled[subset]), axis = 0)\n",
    "                y_unlabeled = np.delete(y_unlabeled, subset, axis = 0)\n",
    "                strategy_sel.update_data(X_tr, y_tr, X_unlabeled)\n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            if(i>1 and sf.endswith(\"cg\")):\n",
    "                per_ep, overall = check_overlap(prev_idx, prev_idx_hist, list(idx_tracker[subset]))\n",
    "                per_ep_overlap.append(per_ep)\n",
    "                overall_overlap.append(overall)\n",
    "            if(feature==\"ood\"): #remove ood points from the subset\n",
    "                subset = remove_ood_points(lake_set, subset, sel_cls_idx)\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "            perClsSel = getPerClassSel(lake_set, subset, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            prev_idx = list(idx_tracker[subset])\n",
    "            prev_idx_hist += list(idx_tracker[subset])\n",
    "            sel_hist.append(list(idx_tracker[subset]))\n",
    "            idx_tracker = np.delete(idx_tracker, subset, axis=0)\n",
    "            \n",
    "            #augment the train_set with selected indices from the lake\n",
    "            if(feature==\"classimb\"):\n",
    "                train_set, lake_set = aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "            else:\n",
    "                train_set, lake_set = aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set))\n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            assert(len(idx_tracker)==len(lake_set))\n",
    "#             model =  model.apply(weight_reset).cuda()\n",
    "            model = create_model(model_name, num_cls, device)\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<150):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                # print(batch_idx)\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "                # sys.exit()\n",
    "\n",
    "            if((val_correct/val_total) > best_val_acc):\n",
    "                final_tst_predictions = []\n",
    "                final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "                if((val_correct/val_total) > best_val_acc):\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                    final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                    final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "            if((val_correct/val_total) > best_val_acc):\n",
    "                best_val_acc = (val_correct/val_total)\n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append(tst_err_log)\n",
    "        print(csvlog)\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "#         with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    return tst_acc, csvlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL2MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl2mi\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/fl2mi/250/1\n",
      "cifar10_ood_SIM_5_fl2mi_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  25.809986352920532\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[22022, 352, 482, 22068, 630]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 01:24:02.578995\n",
      "After augmentation, size of train_set:  750  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.7701138406991959 0.9906666666666667 8.359041303396225 0.66 65.2625623345375 0.7224 100.97826361656189\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27250, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  26.32744574546814\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[332, 531, 650, 21500, 720]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 01:27:48.508925\n",
      "After augmentation, size of train_set:  1000  lake set:  27000\n",
      "[21479, 21661, 15897, 15972, 21699][ 55 ]  Training Acc:  0.898\n",
      "250 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 01:31:46.384018\n",
      "After augmentation, size of train_set:  1250  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.710095959249884 0.992 5.706669747829437 0.72 42.91602864861488 0.8112 193.62923049926758\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26750, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  24.871567726135254\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[742, 10, 21562, 21482, 15880]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 01:36:36.174268\n",
      "After augmentation, size of train_set:  1500  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.9770222285296768 0.9913333333333333 3.030808288604021 0.84 34.097163438797 0.8372 205.02052974700928\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  24.6893892288208\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[344, 422, 410, 21225, 672]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 01:41:36.718895\n",
      "After augmentation, size of train_set:  1750  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.189609886845574 0.9908571428571429 4.256861284375191 0.72 31.63667207956314 0.8466 208.34815382957458\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26250, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  24.406989097595215\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[21077, 21127, 235, 269, 21193]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 01:46:52.651626\n",
      "After augmentation, size of train_set:  2000  lake set:  26000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.5766488425433636 0.9915 3.1529368894989602 0.76 33.86895860731602 0.8444 268.1076762676239\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26000, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  23.814831972122192\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[15471, 15513, 176, 296, 20947]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 01:53:09.130033\n",
      "After augmentation, size of train_set:  2250  lake set:  25750\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.920837601996027 0.9951111111111111 3.552974972873926 0.8 27.27564473450184 0.868 284.59620475769043\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([25750, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  23.396615743637085\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[177, 55, 20601, 65, 60]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 01:59:24.568102\n",
      "After augmentation, size of train_set:  2500  lake set:  25500\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.680730330408551 0.9908 2.818355493247509 0.82 28.85740813612938 0.8734 279.0692756175995\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([25500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  22.644582271575928\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15287, 148, 189, 163, 171]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 02:05:35.562265\n",
      "After augmentation, size of train_set:  2750  lake set:  25250\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.786815393017605 0.9905454545454545 1.731472097337246 0.88 24.892776526510715 0.878 296.4772324562073\n"
     ]
    }
   ],
   "source": [
    "fl2mi_tst, fl2mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL1MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl1mi\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/fl1mi/250/1\n",
      "cifar10_ood_SIM_5_fl1mi_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  27.026771545410156\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[19981, 15511, 2731, 8974, 6335]\n",
      "172 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 02:15:10.512114\n",
      "After augmentation, size of train_set:  672  lake set:  27328\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.7613541446626186 0.9940476190476191 9.10689628124237 0.58 67.06663745641708 0.7226 96.05106329917908\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27328, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  26.34294295310974\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[17548, 20910, 929, 6385, 18826]\n",
      "172 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 02:20:41.814936\n",
      "After augmentation, size of train_set:  844  lake set:  27156\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.5243939054198563 0.9917061611374408 6.6202298402786255 0.66 57.53711265325546 0.7394 145.2319459915161\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27156, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  25.567604303359985\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[135, 2546, 16638, 5296, 16608]\n",
      "166 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 02:26:16.245844\n",
      "After augmentation, size of train_set:  1010  lake set:  26990\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.0843677716329694 0.9900990099009901 6.546001821756363 0.7 54.914184391498566 0.753 120.09031438827515\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26990, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  26.071480989456177\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[10279, 2908, 22430, 20710, 26091]\n",
      "169 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 02:31:19.178366\n",
      "After augmentation, size of train_set:  1179  lake set:  26821\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0893140978878364 0.996607294317218 7.440249681472778 0.7 44.3736063092947 0.8046 178.74755191802979\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26821, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  25.363916158676147\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[22969, 6747, 19972, 21499, 12314]\n",
      "160 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 02:37:32.435857\n",
      "After augmentation, size of train_set:  1339  lake set:  26661\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.845112178241834 0.9940253920836445 5.637115061283112 0.76 45.32774770259857 0.8044 176.58194971084595\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26661, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  24.563108682632446\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[14603, 8659, 26199, 10210, 19213]\n",
      "153 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 02:43:56.994940\n",
      "After augmentation, size of train_set:  1492  lake set:  26508\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.6233045461121947 0.9953083109919572 5.50430902838707 0.7 38.88425016403198 0.824 191.85843467712402\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26508, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  24.9305157661438\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[16659, 12216, 10867, 25242, 7195]\n",
      "148 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 02:50:08.814799\n",
      "After augmentation, size of train_set:  1640  lake set:  26360\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.7889464204199612 0.9926829268292683 5.283602714538574 0.74 36.57945302128792 0.8374 203.74439883232117\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26360, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  24.437942266464233\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6210, 19744, 24760, 23874, 18560]\n",
      "154 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 02:57:17.678046\n",
      "After augmentation, size of train_set:  1794  lake set:  26206\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.6401402545161545 0.9910813823857302 3.879861496388912 0.82 34.010177329182625 0.8436 213.57048225402832\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26206, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  23.863126039505005\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl1mi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[1347, 5988, 14153, 17124, 25996]\n",
      "152 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 03:03:50.563045\n",
      "After augmentation, size of train_set:  1946  lake set:  26054\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.875067262153607 0.9917780061664954 2.175138220191002 0.82 33.800503976643085 0.848 261.29249715805054\n"
     ]
    }
   ],
   "source": [
    "fl1mi_tst, fl1mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n",
    "badge_tst, badge_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL glister-tss\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  1620 Val size:  50 Lake size:  24300\n",
      "selected classes are:  [8 1]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/classimb/glister-tss/30/1\n",
      "cifar10_classimb_AL_2_glister-tss_budget:30_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  cifar10_ResNet18_0.01_10_200_2\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  60.0 25.4\n",
      "val, test error% for class  1  :  100.0 81.6\n",
      "val, test error% for class  2  :  40.0 57.2\n",
      "val, test error% for class  3  :  80.0 53.1\n",
      "val, test error% for class  4  :  60.0 59.3\n",
      "val, test error% for class  5  :  80.0 50.4\n",
      "val, test error% for class  6  :  40.0 30.1\n",
      "val, test error% for class  7  :  40.0 34.8\n",
      "val, test error% for class  8  :  100.0 90.2\n",
      "val, test error% for class  9  :  80.0 28.0\n",
      "total misclassified ex from imb classes:  10\n",
      "reinit Glister with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snk170001/bioml/dss/notebooks/distil/distil/active_learning_strategies/strategy.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l0_grads = data - outputs\n",
      "/home/snk170001/bioml/dss/notebooks/distil/distil/active_learning_strategies/glister.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores = F.softmax(self.out, dim=1).half()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 20:48:30.390338\n",
      "After augmentation, size of train_set:  1650  lake set:  24270\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.0150016711559147 0.9915151515151515 9.649336516857147 0.52 309.43938660621643 0.5195 305.5812261104584\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  0.0 19.0\n",
      "val, test error% for class  1  :  80.0 91.5\n",
      "val, test error% for class  2  :  40.0 51.9\n",
      "val, test error% for class  3  :  40.0 51.2\n",
      "val, test error% for class  4  :  80.0 55.4\n",
      "val, test error% for class  5  :  80.0 48.3\n",
      "val, test error% for class  6  :  0.0 30.2\n",
      "val, test error% for class  7  :  40.0 24.2\n",
      "val, test error% for class  8  :  100.0 94.7\n",
      "val, test error% for class  9  :  20.0 14.1\n",
      "total misclassified ex from imb classes:  9\n",
      "reinit Glister with targeted miscls samples\n",
      "30  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 20:53:54.033690\n",
      "After augmentation, size of train_set:  1680  lake set:  24240\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.150019661290571 0.9910714285714286 11.509056270122528 0.58 333.13064765930176 0.5031 273.82739663124084\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  20.0 21.9\n",
      "val, test error% for class  1  :  80.0 87.7\n",
      "val, test error% for class  2  :  0.0 46.9\n",
      "val, test error% for class  3  :  60.0 56.6\n",
      "val, test error% for class  4  :  60.0 45.9\n",
      "val, test error% for class  5  :  60.0 53.3\n",
      "val, test error% for class  6  :  0.0 42.8\n",
      "val, test error% for class  7  :  40.0 27.4\n",
      "val, test error% for class  8  :  80.0 95.8\n",
      "val, test error% for class  9  :  20.0 18.6\n",
      "total misclassified ex from imb classes:  8\n",
      "reinit Glister with targeted miscls samples\n",
      "30  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 20:58:44.949331\n",
      "After augmentation, size of train_set:  1710  lake set:  24210\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.133161946432665 0.9900584795321637 11.493747472763062 0.48 272.7525005340576 0.5167 298.32945251464844\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  60.0 21.9\n",
      "val, test error% for class  1  :  60.0 87.7\n",
      "val, test error% for class  2  :  40.0 46.9\n",
      "val, test error% for class  3  :  40.0 56.6\n",
      "val, test error% for class  4  :  80.0 45.9\n",
      "val, test error% for class  5  :  80.0 53.3\n",
      "val, test error% for class  6  :  0.0 42.8\n",
      "val, test error% for class  7  :  60.0 27.4\n",
      "val, test error% for class  8  :  60.0 95.8\n",
      "val, test error% for class  9  :  40.0 18.6\n",
      "total misclassified ex from imb classes:  6\n",
      "reinit Glister with targeted miscls samples\n",
      "30  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 21:04:00.603333\n",
      "After augmentation, size of train_set:  1740  lake set:  24180\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.528168432880193 0.9919540229885058 11.06721830368042 0.6 266.83821201324463 0.5297 291.29015278816223\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  0.0 32.5\n",
      "val, test error% for class  1  :  80.0 85.5\n",
      "val, test error% for class  2  :  0.0 43.4\n",
      "val, test error% for class  3  :  40.0 59.1\n",
      "val, test error% for class  4  :  60.0 52.9\n",
      "val, test error% for class  5  :  40.0 39.8\n",
      "val, test error% for class  6  :  0.0 29.5\n",
      "val, test error% for class  7  :  40.0 32.7\n",
      "val, test error% for class  8  :  100.0 76.2\n",
      "val, test error% for class  9  :  40.0 18.7\n",
      "total misclassified ex from imb classes:  9\n",
      "reinit Glister with targeted miscls samples\n",
      "30  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 21:09:10.442820\n",
      "After augmentation, size of train_set:  1770  lake set:  24150\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.73225536942482 0.992090395480226 11.298354864120483 0.58 301.83359575271606 0.5195 305.22835755348206\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  0.0 32.5\n",
      "val, test error% for class  1  :  80.0 85.5\n",
      "val, test error% for class  2  :  20.0 43.4\n",
      "val, test error% for class  3  :  40.0 59.1\n",
      "val, test error% for class  4  :  60.0 52.9\n",
      "val, test error% for class  5  :  40.0 39.8\n",
      "val, test error% for class  6  :  0.0 29.5\n",
      "val, test error% for class  7  :  40.0 32.7\n",
      "val, test error% for class  8  :  100.0 76.2\n",
      "val, test error% for class  9  :  40.0 18.7\n",
      "total misclassified ex from imb classes:  9\n",
      "reinit Glister with targeted miscls samples\n",
      "30  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 21:14:34.230288\n",
      "After augmentation, size of train_set:  1800  lake set:  24120\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.971510534407571 0.9905555555555555 8.87359681725502 0.6 272.2290127277374 0.5338 294.4555995464325\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  20.0 32.5\n",
      "val, test error% for class  1  :  60.0 85.5\n",
      "val, test error% for class  2  :  20.0 43.4\n",
      "val, test error% for class  3  :  80.0 59.1\n",
      "val, test error% for class  4  :  60.0 52.9\n",
      "val, test error% for class  5  :  40.0 39.8\n",
      "val, test error% for class  6  :  0.0 29.5\n",
      "val, test error% for class  7  :  0.0 32.7\n",
      "val, test error% for class  8  :  100.0 76.2\n",
      "val, test error% for class  9  :  20.0 18.7\n",
      "total misclassified ex from imb classes:  8\n",
      "reinit Glister with targeted miscls samples\n",
      "30  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 21:19:45.817098\n",
      "After augmentation, size of train_set:  1830  lake set:  24090\n",
      "Selection Epoch  7  Training epoch [ 80 ]  Training Acc:  0.9715846994535519\r"
     ]
    }
   ],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"AL\",\"glister-tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradMatch-Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM gcmi\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/gcmi/250/1\n",
      "cifar10_ood_SIM_5_gcmi_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  51.23167824745178\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[702, 831, 869, 756, 923]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 11:52:14.790957\n",
      "After augmentation, size of train_set:  750  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.7589459931477904 0.996 10.655465121380985 0.58 69.17162293195724 0.6916 84.5190634727478\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27250, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  51.824878454208374\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[22185, 21840, 21276, 21923, 21762]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 11:55:53.567291\n",
      "After augmentation, size of train_set:  1000  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.8288990466389805 0.99 8.167296231724322 0.54 68.17526516318321 0.7266 110.6651599407196\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27000, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  9.031732559204102\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[16005, 16128, 16018, 16231, 15907]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 11:59:12.770363\n",
      "After augmentation, size of train_set:  1250  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.5689736705535324 0.9936 8.090642608702183 0.68 58.612861186265945 0.7512 157.86666464805603\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26750, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  48.87214493751526\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[331, 406, 173, 339, 303]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 12:04:01.708456\n",
      "After augmentation, size of train_set:  1500  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.327689355937764 0.992 3.288975454866886 0.8 44.31669324636459 0.7918 168.26548099517822\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  48.47782039642334\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[21127, 21493, 21365, 21273, 21142]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 12:08:49.749654\n",
      "After augmentation, size of train_set:  1750  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.066399241099134 0.9902857142857143 6.627282992005348 0.7 48.40368766710162 0.7974 201.88084959983826\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26250, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  47.17808437347412\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[21006, 21136, 21206, 21118, 20961]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 12:14:15.970311\n",
      "After augmentation, size of train_set:  2000  lake set:  26000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.405863747699186 0.991 5.161108493804932 0.7 35.2831626534462 0.8232 192.22814989089966\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26000, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  46.94124126434326\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[24649, 21422, 25365, 24934, 22686]\n",
      "162 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 12:19:25.947405\n",
      "After augmentation, size of train_set:  2162  lake set:  25838\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.6565128377988003 0.9912118408880666 3.91598679125309 0.8 29.728891357779503 0.8556 322.05520963668823\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([25838, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  45.378212451934814\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[2471, 4497, 3791, 4072, 2325]\n",
      "233 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 12:26:44.074535\n",
      "After augmentation, size of train_set:  2395  lake set:  25605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.5986103724862915 0.9903966597077244 3.1986907701939344 0.82 30.33627639710903 0.8662 295.12819623947144\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([25605, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  44.598461866378784\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer gcmi -numQueries 50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[12187, 14263, 13518, 14956, 133]\n",
      "243 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 12:33:36.358851\n",
      "After augmentation, size of train_set:  2638  lake set:  25362\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.134460815577768 0.9901440485216073 1.4753268184140325 0.86 24.130608417093754 0.8684 296.0029377937317\n"
     ]
    }
   ],
   "source": [
    "gcmi_tst, gcmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDETMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdetmi\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/logdetmi/250/1\n",
      "cifar10_ood_SIM_5_logdetmi_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  51.133453130722046\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[16, 21875, 22059, 260, 16392]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 12:41:15.094003\n",
      "After augmentation, size of train_set:  750  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.7969616636401042 0.9946666666666667 8.512683272361755 0.58 64.78667414188385 0.7308 100.3179099559784\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27250, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  50.166661500930786\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[22204, 21873, 16144, 240, 21594]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 12:45:35.976918\n",
      "After augmentation, size of train_set:  1000  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.580436619813554 0.991 6.52990576159209 0.8 52.682604908943176 0.7728 127.03787922859192\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27000, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  49.423760652542114\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[21788, 177, 15974, 21948, 21347]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 12:50:15.215905\n",
      "After augmentation, size of train_set:  1250  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.9590293123037554 0.9912 8.669859886169434 0.64 50.35640487074852 0.7992 162.14443802833557\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26750, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  49.19693660736084\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[16052, 21382, 188, 21577, 21200]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 12:55:50.647749\n",
      "After augmentation, size of train_set:  1500  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.4408393966732547 0.9926666666666667 4.945162415504456 0.72 40.39829456806183 0.8228 155.1564702987671\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  48.093997955322266\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[21079, 560, 15693, 349, 340]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 13:00:59.003206\n",
      "After augmentation, size of train_set:  1750  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.5135258852969855 0.992 4.033963099122047 0.8 35.14315730333328 0.8376 196.2552092075348\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26250, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  46.51787877082825\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[21161, 15707, 15614, 20992, 21058]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 13:06:46.273346\n",
      "After augmentation, size of train_set:  2000  lake set:  26000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.0863787038251758 0.9935 2.039472073316574 0.86 32.97922432422638 0.8434 197.44605898857117\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26000, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  46.925413846969604\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[20790, 15498, 20948, 20620, 233]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 13:12:25.504971\n",
      "After augmentation, size of train_set:  2250  lake set:  25750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.8768864553421736 0.9902222222222222 3.24617400765419 0.8 26.366208404302597 0.8658 286.4319324493408\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([25750, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  44.60443425178528\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[63, 20630, 15468, 179, 15450]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 13:19:34.596081\n",
      "After augmentation, size of train_set:  2500  lake set:  25500\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.228919034358114 0.99 3.051715986803174 0.82 27.315482951700687 0.8658 284.41445803642273\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([25500, 10])\n",
      "val minibatch gradients shape  torch.Size([50, 10])\n",
      "kernel compute time:  44.228517055511475\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer logdetmi -numQueries  50  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel.hdf5\n",
      "[20433, 20325, 20365, 159, 20425]\n",
      "250 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 13:26:34.187972\n",
      "After augmentation, size of train_set:  2750  lake set:  25250\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.9122148170135915 0.9912727272727273 2.4077522307634354 0.86 27.395369589328766 0.867 289.5191147327423\n"
     ]
    }
   ],
   "source": [
    "logdetmi_tst, logdetmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/fl/250/1\n",
      "cifar10_ood_SIM_5_fl_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  52.7969446182251\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[17156, 2032, 5961, 24137, 8327]\n",
      "51 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 13:36:48.327954\n",
      "After augmentation, size of train_set:  551  lake set:  27449\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8693804489448667 0.9909255898366606 12.127799987792969 0.56 81.16496202349663 0.6906 79.85430026054382\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  52.37261176109314\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[10726, 20125, 4681, 13082, 156]\n",
      "53 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 13:43:22.986126\n",
      "After augmentation, size of train_set:  604  lake set:  27396\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.2174557545222342 0.9900662251655629 10.187591433525085 0.58 64.33019626140594 0.7114 89.8519549369812\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.136428117752075\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[12723, 21602, 7412, 19002, 21991]\n",
      "51 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 13:50:08.681282\n",
      "After augmentation, size of train_set:  655  lake set:  27345\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.371655789669603 0.9938931297709923 10.428329706192017 0.56 70.67224168777466 0.7116 79.20817732810974\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  52.134687662124634\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[24511, 2597, 11187, 7974, 18151]\n",
      "49 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 13:56:33.981166\n",
      "After augmentation, size of train_set:  704  lake set:  27296\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.1875109623651952 0.9957386363636364 10.82448623329401 0.66 62.28696137666702 0.7392 110.17626404762268\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  50.741355657577515\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[26892, 23351, 15325, 25575, 12041]\n",
      "49 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 14:03:28.884588\n",
      "After augmentation, size of train_set:  753  lake set:  27247\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0421636009123176 0.9920318725099602 10.239803075790405 0.6 68.33631372451782 0.7208 93.74382257461548\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  50.915549755096436\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[15, 268, 25615, 21426, 26973]\n",
      "52 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 14:10:01.029530\n",
      "After augmentation, size of train_set:  805  lake set:  27195\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.6631085907574743 0.9925465838509316 8.242164760828018 0.56 57.748964965343475 0.7284 98.86479830741882\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.72822976112366\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[9510, 20419, 18456, 1517, 6525]\n",
      "47 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 14:16:43.775270\n",
      "After augmentation, size of train_set:  852  lake set:  27148\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0909677335293964 0.9917840375586855 8.601506307721138 0.6 65.50611194968224 0.7358 103.5907609462738\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.621509075164795\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[26987, 4156, 9440, 5237, 16633]\n",
      "56 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 14:23:36.173166\n",
      "After augmentation, size of train_set:  908  lake set:  27092\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.7731942445971072 0.9933920704845814 5.165130376815796 0.72 58.094892889261246 0.7284 103.79926991462708\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.57057785987854\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer fl -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[20777, 15958, 22286, 5560, 15848]\n",
      "48 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 14:30:31.264310\n",
      "After augmentation, size of train_set:  956  lake set:  27044\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.2078033917932771 0.99581589958159 6.246937096118927 0.64 55.57441186904907 0.7612 114.41028022766113\n"
     ]
    }
   ],
   "source": [
    "fl_tst, fl_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM gc\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/gc/250/1\n",
      "cifar10_ood_SIM_5_gc_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.60829973220825\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[4959, 25257, 13169, 18620, 15923]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 14:34:48.508722\n",
      "After augmentation, size of train_set:  500  lake set:  27500\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.3551803244045004 1.0 8.289013361558318 0.64 70.24067521095276 0.7084 70.76533722877502\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.67305779457092\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[3052, 27106, 14586, 4504, 13021]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 14:38:31.470487\n",
      "After augmentation, size of train_set:  500  lake set:  27500\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.9202991845086217 0.994 9.770078480243683 0.62 74.3319441974163 0.6804 44.11907243728638\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  52.6955988407135\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[17448, 25960, 14350, 20639, 6231]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 14:41:33.618696\n",
      "After augmentation, size of train_set:  500  lake set:  27500\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.7600834821350873 0.992 12.027020543813705 0.56 78.11529928445816 0.6908 56.05388116836548\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  52.01300287246704\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[6034, 5438, 11321, 17811, 26770]\n",
      "1 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 14:44:47.987812\n",
      "After augmentation, size of train_set:  501  lake set:  27499\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.7547603095840714 0.9940119760479041 10.458679765462875 0.54 73.02838191390038 0.7006 98.4992241859436\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  53.09925818443298\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[10749, 17598, 12996, 16617, 23929]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 14:48:48.374219\n",
      "After augmentation, size of train_set:  501  lake set:  27499\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8650218508264516 0.9940119760479041 10.723197102546692 0.58 72.9016769528389 0.69 106.6109733581543\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.43876671791077\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[3472, 15942, 7386, 26146, 2029]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 14:52:51.932666\n",
      "After augmentation, size of train_set:  501  lake set:  27499\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.6477601444239554 0.9940119760479041 9.645003199577332 0.6 82.37512457370758 0.6868 107.26265358924866\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.7373583316803\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[11854, 25368, 17345, 4576, 19488]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 14:57:02.744185\n",
      "After augmentation, size of train_set:  501  lake set:  27499\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.3287439523264766 0.9920159680638723 11.13961935043335 0.58 69.89867433905602 0.7094 90.68148684501648\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  52.71295404434204\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[14564, 8056, 26280, 2844, 25354]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 15:00:58.750979\n",
      "After augmentation, size of train_set:  501  lake set:  27499\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.9535529967397451 0.9900199600798403 10.685061872005463 0.54 68.08997958898544 0.6942 94.4917266368866\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.510210037231445\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget 250 -genericOptimizer gc -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[18163, 24727, 4522, 13930, 19959]\n",
      "0 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 15:04:51.442313\n",
      "After augmentation, size of train_set:  501  lake set:  27499\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8555007055401802 0.9920159680638723 13.253008723258972 0.56 84.96017050743103 0.6774 105.70470094680786\n"
     ]
    }
   ],
   "source": [
    "gc_tst, gc_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'gc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdet\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/logdet/250/1\n",
      "cifar10_ood_SIM_5_logdet_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.262340784072876\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[19007, 10185, 13124, 23201, 3339]\n",
      "80 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 15:09:03.790006\n",
      "After augmentation, size of train_set:  580  lake set:  27420\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8790817863773555 0.9913793103448276 8.734388172626495 0.62 68.51666140556335 0.7114 64.38613486289978\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  52.55565619468689\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[6137, 4756, 22744, 18148, 8876]\n",
      "83 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 15:12:28.900825\n",
      "After augmentation, size of train_set:  663  lake set:  27337\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0861181733198464 0.9909502262443439 8.157585680484772 0.62 58.71815147995949 0.7262 105.76351499557495\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  50.9478862285614\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[17419, 16073, 26005, 12314, 8805]\n",
      "87 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 15:16:32.868965\n",
      "After augmentation, size of train_set:  750  lake set:  27250\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.717011316679418 0.992 7.043551586568356 0.68 63.470406115055084 0.7218 87.89004111289978\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  50.467869997024536\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[12460, 3100, 787, 18119, 26959]\n",
      "87 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 15:20:22.663681\n",
      "After augmentation, size of train_set:  837  lake set:  27163\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8426825794158503 0.994026284348865 8.433999303728342 0.7 63.11402004957199 0.7406 110.165780544281\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  51.66331362724304\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[6291, 21692, 11180, 24658, 16226]\n",
      "82 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 15:24:30.208135\n",
      "After augmentation, size of train_set:  919  lake set:  27081\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.3330913104582578 0.9902067464635473 8.772791504859924 0.64 58.40586391091347 0.7538 107.41384816169739\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  50.38710260391235\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[59, 646, 24654, 21266, 3741]\n",
      "80 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 15:28:37.618143\n",
      "After augmentation, size of train_set:  999  lake set:  27001\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.317392819095403 0.991991991991992 8.3787180185318 0.68 52.64830645918846 0.7674 120.89065527915955\n",
      "AL epoch:  7\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  49.69261622428894\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[7127, 3079, 12974, 5846, 271]\n",
      "82 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 15:32:52.178557\n",
      "After augmentation, size of train_set:  1081  lake set:  26919\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.038546039490029 0.9916743755781684 5.87228969624266 0.68 48.93748012185097 0.7676 147.14046216011047\n",
      "AL epoch:  8\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  50.466506242752075\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[143, 1113, 16060, 21320, 26663]\n",
      "88 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 15:37:39.237770\n",
      "After augmentation, size of train_set:  1169  lake set:  26831\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.753238994628191 0.9905902480752781 4.92039567232132 0.66 48.11401650309563 0.7692 154.32035183906555\n",
      "AL epoch:  9\n",
      "Per Element Training Gradient Computation is Completed\n",
      "kernel compute time:  49.40327000617981\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget 250 -genericOptimizer logdet -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5\n",
      "[12947, 25633, 1365, 233, 16365]\n",
      "82 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 15:42:33.125539\n",
      "After augmentation, size of train_set:  1251  lake set:  26749\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.4025158690637909 0.9952038369304557 7.406422019004822 0.64 47.56115773320198 0.7926 159.41509580612183\n"
     ]
    }
   ],
   "source": [
    "logdet_tst, logdet_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'logdet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  550 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/random/250/1\n",
      "cifar10_ood_random_5_random_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ood_ResNet18_0.01_100_10_5\n",
      "AL epoch:  1\n",
      "24 / 250  idc points.\n",
      "selEpoch: 1, Selection Ended at: 2021-04-06 15:45:20.009927\n",
      "After augmentation, size of train_set:  574  lake set:  27476\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.5512348141055554 0.9965156794425087 0.28813036251813173 0.98 68.79186511039734 0.7088 70.29656529426575\n",
      "AL epoch:  2\n",
      "32 / 250  idc points.\n",
      "selEpoch: 2, Selection Ended at: 2021-04-06 15:46:32.165355\n",
      "After augmentation, size of train_set:  606  lake set:  27444\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.822573650861159 0.995049504950495 0.09718385001178831 1.0 64.05200499296188 0.7268 79.47257041931152\n",
      "AL epoch:  3\n",
      "18 / 250  idc points.\n",
      "selEpoch: 3, Selection Ended at: 2021-04-06 15:47:53.512012\n",
      "After augmentation, size of train_set:  624  lake set:  27426\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.9479326109867543 0.9935897435897436 0.1176660880446434 1.0 64.69625556468964 0.7136 96.3580379486084\n",
      "AL epoch:  4\n",
      "28 / 250  idc points.\n",
      "selEpoch: 4, Selection Ended at: 2021-04-06 15:49:31.778436\n",
      "After augmentation, size of train_set:  652  lake set:  27398\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.4051631583133712 0.99079754601227 0.4085740400478244 0.98 64.84347331523895 0.7092 83.93890905380249\n",
      "AL epoch:  5\n",
      "19 / 250  idc points.\n",
      "selEpoch: 5, Selection Ended at: 2021-04-06 15:50:57.694614\n",
      "After augmentation, size of train_set:  671  lake set:  27379\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.4832020308822393 0.9910581222056631 0.2723536124685779 0.98 56.07823556661606 0.7246 61.225330114364624\n",
      "AL epoch:  6\n",
      "27 / 250  idc points.\n",
      "selEpoch: 6, Selection Ended at: 2021-04-06 15:52:00.842474\n",
      "After augmentation, size of train_set:  698  lake set:  27352\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.98258317791624 0.9914040114613181 0.23260363098233938 1.0 62.89919638633728 0.7198 91.9548909664154\n",
      "AL epoch:  7\n",
      "21 / 250  idc points.\n",
      "selEpoch: 7, Selection Ended at: 2021-04-06 15:53:34.653604\n",
      "After augmentation, size of train_set:  719  lake set:  27331\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.224646186688915 0.9930458970792768 0.13830250222235918 1.0 59.1867510676384 0.7346 77.04600381851196\n",
      "AL epoch:  8\n",
      "20 / 250  idc points.\n",
      "selEpoch: 8, Selection Ended at: 2021-04-06 15:54:53.499929\n",
      "After augmentation, size of train_set:  739  lake set:  27311\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0834422351326793 0.9918809201623816 0.06900793628301471 1.0 61.247901529073715 0.7312 94.02570962905884\n",
      "AL epoch:  9\n",
      "23 / 250  idc points.\n",
      "selEpoch: 9, Selection Ended at: 2021-04-06 15:56:29.529652\n",
      "After augmentation, size of train_set:  762  lake set:  27288\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8273461908102036 0.9960629921259843 0.09010032881633379 1.0 55.796661257743835 0.7474 137.95979380607605\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1,6):\n",
    "random_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"random\",'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] [1, 3]\n"
     ]
    }
   ],
   "source": [
    "x=[1,2,3,4]\n",
    "val_idx =  list(np.random.choice(np.array(x), size=2, replace=False))\n",
    "print(x, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
