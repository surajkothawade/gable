{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from cords.cords.selectionstrategies.supervisedlearning import DataSelectionStrategy\n",
    "from cords.cords.utils.models import ResNet18\n",
    "from gable.gable.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "# for cuda\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class custom_subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "        labels(sequence) : targets as required for the indices. will be the same length as indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, labels):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.targets = labels.type(torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target = self.targets[idx]\n",
    "        return (image, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device):\n",
    "    if name == 'ResNet18':\n",
    "        model = ResNet18(num_cls)\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def kernel(x, y, measure=\"cosine\", exp=2):\n",
    "    if(measure==\"eu_sim\"):\n",
    "        dist = pairwise_distances(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = max(dist.ravel()) - dist\n",
    "#         n = x.size(0)\n",
    "#         m = y.size(0)\n",
    "#         d = x.size(1)\n",
    "#         x = x.unsqueeze(1).expand(n, m, d)\n",
    "#         y = y.unsqueeze(0).expand(n, m, d)\n",
    "#         dist = torch.pow(x - y, exp).sum(2)\n",
    "#         const = torch.max(dist).item()\n",
    "#         sim = (const - dist)\n",
    "    \n",
    "        #dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))\n",
    "    if(measure==\"cosine\"):\n",
    "        sim = cosine_similarity(x.cpu().numpy(), y.cpu().numpy())\n",
    "    return sim\n",
    "\n",
    "\n",
    "def save_kernel_hdf5(lake_kernel, lake_target_kernel, target_kernel=[], numpy=True):\n",
    "    if(not(numpy)):\n",
    "        lake_kernel = lake_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_target_kernel = lake_target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_target_kernel)\n",
    "    if(not(numpy)):\n",
    "        target_kernel = target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_target_kernel.hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=target_kernel)\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "    saveDir = os.path.join(saveDir, prefix)\n",
    "    if(not(os.path.exists(saveDir))):\n",
    "        os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    class_err_log = []\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        print(\"val, test error% for class \", i, \" : \", round((len(val_err_class_idx)/len(val_class_idxs))*100,2), round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        class_err_log.append(\"val, test error% for class \"+ str(i) + \" : \"+ str(round((len(val_err_class_idx)/len(val_class_idxs))*100,2)) + \", \" + str(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)))\n",
    "        tst_err_log.append(round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2))\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    return tst_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = custom_subset(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    return aug_train_set, remain_lake_set\n",
    "                        \n",
    "def getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def getSMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, sf):\n",
    "    if(sf==\"fl1mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel.hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf == \"logdetmi\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \"  -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel.hdf5\") + \" -queryqueryKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl2mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gcmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"gccg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl1cg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\")\n",
    "    elif(sf==\"logdetcg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel.hdf5\") + \" -privateprivateKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel.hdf5\")\n",
    "    elif(sf==\"fl\" or sf==\"logdet\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\")\n",
    "    elif(sf ==\"gc\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel.hdf5\")\n",
    "    print(\"Executing SIM command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "def remove_ood_points(lake_set, subset, idc_idx):\n",
    "    idx_subset = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in idc_idx:\n",
    "        idc_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        idx_subset += list(np.array(subset)[idc_subset_idx])\n",
    "    print(len(idx_subset),\"/\",len(subset), \" idc points.\")\n",
    "    return idx_subset\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "#check overlap with prev selections\n",
    "def check_overlap(prev_idx, prev_idx_hist, idx):\n",
    "    prev_idx = [int(x/num_rep) for x in prev_idx]\n",
    "    prev_idx_hist = [int(x/num_rep) for x in prev_idx_hist]\n",
    "    idx = [int(x/num_rep) for x in idx]\n",
    "    # overlap = set(prev_idx).intersection(set(idx))\n",
    "    overlap = [value for value in idx if value in prev_idx] \n",
    "    # overlap_hist = set(prev_idx_hist).intersection(set(idx))\n",
    "    overlap_hist = [value for value in idx if value in prev_idx_hist]\n",
    "    new_points = set(idx) - set(prev_idx_hist)\n",
    "    total_unique_points = set(idx+prev_idx_hist)\n",
    "    print(\"New unique points: \", len(new_points))\n",
    "    print(\"Total unique points: \", len(total_unique_points))\n",
    "    print(\"overlap % of sel with prev idx: \", len(overlap)/len(idx))\n",
    "    print(\"overlap % of sel with all prev idx: \", len(overlap_hist)/len(idx))\n",
    "    return len(overlap)/len(idx), len(overlap_hist)/len(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "datadir = 'data/'\n",
    "data_name = 'cifar10'\n",
    "num_cls=10\n",
    "fraction = float(0.1)\n",
    "budget=250\n",
    "num_epochs = int(10)\n",
    "select_every = int(2)\n",
    "num_rep = 10\n",
    "model_name = 'ResNet18'\n",
    "learning_rate = 0.01\n",
    "# feature='vanilla'\n",
    "# split_cfg = {\"train_size\":500, \"val_size\":1000, \"lake_size\":5000, \"num_rep\":num_rep, \"lake_subset_repeat_size\":1000}\n",
    "# feature = 'duplicate'\n",
    "# feature = 'classimb'\n",
    "# split_cfg = {\"num_cls_imbalance\":2, \"per_imbclass_train\":10, \"per_imbclass_val\":5, \"per_imbclass_lake\":150, \"per_class_train\":200, \"per_class_val\":5, \"per_class_lake\":3000}\n",
    "# initModelPath = data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_imbclass_train\"]) + \"_\" + str(split_cfg[\"per_class_train\"]) + \"_\" + str(split_cfg[\"num_cls_imbalance\"])\n",
    "feature = 'ood'\n",
    "split_cfg = {'num_cls_idc':5, 'per_idc_train':100, 'per_idc_val':10, 'per_idc_lake':500, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':5000}\n",
    "initModelPath = \"weights/\"+data_name + \"_\" + feature + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_idc_train\"]) + \"_\" + str(split_cfg[\"per_idc_val\"]) + \"_\" + str(split_cfg[\"num_cls_idc\"])\n",
    "num_runs = 1  # number of random runs\n",
    "\n",
    "\n",
    "magnification = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "datkbuildPath = \"/home/snk170001/bioml/dss/notebooks/datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "print(\"Using Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distil.distil.active_learning_strategies import BADGE, EntropySampling, GLISTER\n",
    "from distil.distil.utils.DataHandler import DataHandler_CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    if(feature==\"classimb\" or feature==\"ood\"):\n",
    "        if(strategy == \"SIM\" or strategy == \"SF\" or strategy==\"random\"):\n",
    "            if(strategy == \"SF\" or strategy==\"random\"):\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, True)\n",
    "            else:\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "        elif(strategy==\"AL\"):\n",
    "            if(sf==\"badge\" or sf==\"us\"):\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, True)\n",
    "            else: #dont augment train with valid\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "        print(\"selected classes are: \", sel_cls_idx)\n",
    "    if(feature==\"duplicate\" or feature==\"vanilla\"):\n",
    "        sel_cls_idx = None\n",
    "        if(strategy == \"SIM\" or strategy==\"random\"):\n",
    "            train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        elif(strategy==\"AL\"):\n",
    "            X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True)\n",
    "        \n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "    all_logs_dir = 'SMI_active_learning_results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \"feature\":feature, \"sel_budget\":budget, \"num_selections\":num_epochs, \"model\":model_name, \"learning_rate\":learning_rate, \"setting\":split_cfg, \"test_acc\":[],\"sel_per_cls\":[], \"sel_cls_idx\":sel_cls_idx.tolist()}\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device)\n",
    "    model1 = create_model(model_name, num_cls, device)\n",
    "    if(strategy == \"AL\"):\n",
    "        strategy_args = {'batch_size' : budget, 'lr':float(0.001)}\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(X_tr.astype(np.float64), y_tr, X_unlabeled.astype(np.float64), model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args)\n",
    "        elif(sf==\"glister\"):\n",
    "            strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, DataHandler_CIFAR10, num_cls, device, strategy_args, valid=True, X_val=X_val, Y_val=y_val, typeOf='rand', lam=0.1)\n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "#     optimizer, scheduler = optimizer_with_scheduler(model, num_epochs, learning_rate)\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "    #overlap vars\n",
    "    prev_idx = None\n",
    "    prev_idx_hist = []\n",
    "    sel_hist = []\n",
    "    per_ep_overlap = []\n",
    "    overall_overlap = []\n",
    "    idx_tracker = np.array(list(range(len(lake_set))))\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training\")\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        final_tst_predictions = []\n",
    "                        final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        if((val_correct/val_total) > best_val_acc):\n",
    "                            final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                            final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                continue\n",
    "        else:\n",
    "#             if(full_trn_acc[i-1] >= 0.99): #The model has already trained on the seed dataset\n",
    "            #use misclassifications on validation set as queries\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append([\"epoch \"+str(i)]+tst_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set = getMisclsSet(val_set, val_class_err_idxs, sel_cls_idx)\n",
    "                        misclsloader = torch.utils.data.DataLoader(miscls_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, misclsloader, model1, num_cls, True, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, True, device)\n",
    "                elif(sf.endswith(\"cg\")): #atleast one selection must be done for private set in cond gain functions\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, privateSetloader, model1, num_cls, True, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        #compute subset with private set a NULL\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, True, device)\n",
    "                else:\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, True, device)\n",
    "                start_time = time.time()\n",
    "                cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "                clone_dict = copy.deepcopy(model.state_dict())\n",
    "                #update the selection strategy model with new params for gradient computation\n",
    "                setf_model.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mi\")): #SMI functions need the target set gradients\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "#                     print(setf_model.grads_per_elem)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "#                     print(setf_model.val_grads_per_elem)\n",
    "                    train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    numQueryPrivate = train_val_kernel.shape[1]\n",
    "                elif(sf.endswith(\"cg\")):\n",
    "                    if(len(private_set)!=0):\n",
    "                        setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                        print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                        print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_private_kernel\n",
    "                        numQueryPrivate = train_val_kernel.shape[1]\n",
    "                    else:\n",
    "#                         assert(((i + 1)/select_every)==1)\n",
    "                        setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        train_val_kernel = []\n",
    "                        numQueryPrivate = 0\n",
    "                else: # For other submodular functions needing only image kernel\n",
    "                    setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                    train_val_kernel = []\n",
    "                    numQueryPrivate = 0\n",
    "\n",
    "                kernel_time = time.time()\n",
    "                train_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.grads_per_elem.double()) #img_img_kernel\n",
    "                if(sf==\"logdetmi\" or sf==\"logdetcg\"):\n",
    "                    if(sf==\"logdetcg\"):\n",
    "                        if(len(private_set)!=0):\n",
    "                            val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                        else:\n",
    "                            val_kernel = []\n",
    "                    if(sf==\"logdetmi\"):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel, val_kernel)\n",
    "                else:\n",
    "                    save_kernel_hdf5(train_kernel, train_val_kernel)\n",
    "                print(\"kernel compute time: \", time.time()-kernel_time)\n",
    "                #call the c++ exec to read kernel and compute subset of selected minibatches\n",
    "                subset = getSMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQueryPrivate), sf)\n",
    "                print(subset[:5])\n",
    "                model.load_state_dict(cached_state_dict)\n",
    "                if(sf.endswith(\"cg\")): #for first selection\n",
    "                    if(len(private_set)==0):\n",
    "                        private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "                    else:\n",
    "                        private_set = getPrivateSet(lake_set, subset, private_set)\n",
    "                    print(\"size of private set: \", len(private_set))\n",
    "\n",
    "    #           temp = np.array(list(trainloader.batch_sampler))[subset] #if per batch\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                strategy_sel.update_model(model)\n",
    "                if(sf==\"badge\" or sf==\"glister\"):\n",
    "                    subset = strategy_sel.select(budget)\n",
    "                if(sf==\"us\"):\n",
    "                    subset = list(strategy_sel.select(budget).cpu().numpy())\n",
    "                print(len(subset), \" samples selected\")\n",
    "                X_tr = np.concatenate((X_tr, X_unlabeled[subset]), axis=0)\n",
    "                X_unlabeled = np.delete(X_unlabeled, subset, axis = 0)\n",
    "                y_tr = np.concatenate((y_tr, y_unlabeled[subset]), axis = 0)\n",
    "                y_unlabeled = np.delete(y_unlabeled, subset, axis = 0)\n",
    "                strategy_sel.update_data(X_tr, y_tr, X_unlabeled)\n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            if(i>1 and sf.endswith(\"cg\")):\n",
    "                per_ep, overall = check_overlap(prev_idx, prev_idx_hist, list(idx_tracker[subset]))\n",
    "                per_ep_overlap.append(per_ep)\n",
    "                overall_overlap.append(overall)\n",
    "            prev_idx = list(idx_tracker[subset])\n",
    "            prev_idx_hist += list(idx_tracker[subset])\n",
    "            sel_hist.append(list(idx_tracker[subset]))\n",
    "            idx_tracker = np.delete(idx_tracker, subset, axis=0)\n",
    "\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "            perClsSel = getPerClassSel(lake_set, subset, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            \n",
    "            if(feature==\"ood\"): #remove ood points from the subset\n",
    "                subset = remove_ood_points(lake_set, subset, sel_cls_idx)\n",
    "            #augment the train_set with selected indices from the lake\n",
    "            if(feature==\"classimb\"):\n",
    "                train_set, lake_set = aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "            else:\n",
    "                train_set, lake_set = aug_train_subset(train_set, lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set))\n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            assert(len(idx_tracker)==len(lake_set))\n",
    "#             model =  model.apply(weight_reset).cuda()\n",
    "            model = create_model(model_name, num_cls, device)\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<150):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                # print(batch_idx)\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "                # sys.exit()\n",
    "\n",
    "            if((val_correct/val_total) > best_val_acc):\n",
    "                final_tst_predictions = []\n",
    "                final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "                if((val_correct/val_total) > best_val_acc):\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                    final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                    final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "            if((val_correct/val_total) > best_val_acc):\n",
    "                best_val_acc = (val_correct/val_total)\n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append([\"final\"]+tst_err_log)\n",
    "        print(csvlog)\n",
    "        with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    return tst_acc, csvlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL2MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl2mi\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  500 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/fl2mi/250/1\n",
      "cifar10_ood_SIM_fl2mi_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27500, 5130])\n",
      "val minibatch gradients shape  torch.Size([50, 5130])\n",
      "kernel compute time:  84.08813142776489\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[22342, 16103, 21787, 22394, 927]\n",
      "selEpoch: 1, Selection Ended at: 2021-04-02 22:01:15.608771\n",
      "250 / 250  idc points.\n",
      "After augmentation, size of train_set:  750  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.7726742010563612 0.9973333333333333 7.850208616815507 0.74 61.61202374100685 0.7474 119.00056171417236\n",
      "AL epoch:  2\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27250, 5130])\n",
      "val minibatch gradients shape  torch.Size([50, 5130])\n",
      "kernel compute time:  81.60552048683167\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[15996, 489, 21576, 623, 21371]\n",
      "selEpoch: 2, Selection Ended at: 2021-04-02 22:06:04.397272\n",
      "250 / 250  idc points.\n",
      "After augmentation, size of train_set:  1000  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.2186823600204661 0.996 7.229867013171315 0.66 65.6394093632698 0.728 142.15225744247437\n",
      "AL epoch:  3\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([27000, 5130])\n",
      "val minibatch gradients shape  torch.Size([50, 5130])\n",
      "kernel compute time:  81.06294178962708\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[21231, 21393, 15930, 21204, 21838]\n",
      "selEpoch: 3, Selection Ended at: 2021-04-02 22:11:11.978202\n",
      "250 / 250  idc points.\n",
      "After augmentation, size of train_set:  1250  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.4197131001274101 0.9952 6.028936713933945 0.74 45.85590800642967 0.8008 146.7058732509613\n",
      "AL epoch:  4\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26750, 5130])\n",
      "val minibatch gradients shape  torch.Size([50, 5130])\n",
      "kernel compute time:  78.47408986091614\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[15886, 536, 563, 499, 22]\n",
      "selEpoch: 4, Selection Ended at: 2021-04-02 22:16:21.932856\n",
      "250 / 250  idc points.\n",
      "After augmentation, size of train_set:  1500  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.0536805904703215 0.994 6.991904020309448 0.72 42.69732287526131 0.8066 208.70397019386292\n",
      "AL epoch:  5\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26500, 5130])\n",
      "val minibatch gradients shape  torch.Size([50, 5130])\n",
      "kernel compute time:  79.03652906417847\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[118, 21468, 107, 484, 21266]\n",
      "selEpoch: 5, Selection Ended at: 2021-04-02 22:22:32.295894\n",
      "250 / 250  idc points.\n",
      "After augmentation, size of train_set:  1750  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.6647935618530028 0.9902857142857143 4.212590798735619 0.8 42.502557426691055 0.8178 199.61923265457153\n",
      "AL epoch:  6\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([26250, 5130])\n",
      "val minibatch gradients shape  torch.Size([50, 5130])\n",
      "kernel compute time:  78.01002502441406\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 250 -queryPrivacyOptimizer fl2mi -numQueries  50 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel.hdf5\n",
      "[15649, 283, 21087, 20801, 382]\n",
      "selEpoch: 6, Selection Ended at: 2021-04-02 22:28:35.487564\n",
      "250 / 250  idc points.\n",
      "After augmentation, size of train_set:  2000  lake set:  26000\n",
      "Selection Epoch  6  Training epoch [ 36 ]  Training Acc:  0.9565\r"
     ]
    }
   ],
   "source": [
    "fl2mi_tst, fl2mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL1MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl1mi_tst, fl1mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n",
    "badge_tst, badge_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, True, \"AL\",\"glister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcmi_tst, gcmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, True, \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDETMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdetmi_tst, logdetmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl_tst, fl_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_tst, gc_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdet_tst, logdet_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'logdet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  550 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/random/250/1\n",
      "cifar10_ood_random_random_budget:250_epochs:10_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "selEpoch: 1, Selection Ended at: 2021-04-02 09:21:41.232425\n",
      "After augmentation, size of train_set:  800  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.9093278856016695 0.99375 0.09576878967345692 1.0 93.28716039657593 0.6404 132.26292181015015\n",
      "AL epoch:  2\n",
      "selEpoch: 2, Selection Ended at: 2021-04-02 09:23:55.389971\n",
      "After augmentation, size of train_set:  1050  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.6831520383711904 0.9923809523809524 0.12025957001606002 0.98 100.70056062936783 0.6138 161.45492601394653\n",
      "AL epoch:  3\n",
      "selEpoch: 3, Selection Ended at: 2021-04-02 09:26:38.750802\n",
      "After augmentation, size of train_set:  1300  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.788465037709102 0.99 0.17156196397263557 0.98 112.62990039587021 0.5942 201.27535486221313\n",
      "AL epoch:  4\n",
      "selEpoch: 4, Selection Ended at: 2021-04-02 09:30:01.947549\n",
      "After augmentation, size of train_set:  1550  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.0670807859278284 0.9922580645161291 0.1776101258583367 0.98 104.0275090932846 0.6192 261.3683466911316\n",
      "AL epoch:  5\n",
      "selEpoch: 5, Selection Ended at: 2021-04-02 09:34:25.221562\n",
      "After augmentation, size of train_set:  1800  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.096244236221537 0.9911111111111112 0.06572085904190317 1.0 94.41577678918839 0.6444 270.93921971321106\n",
      "AL epoch:  6\n",
      "selEpoch: 6, Selection Ended at: 2021-04-02 09:38:58.071686\n",
      "After augmentation, size of train_set:  2050  lake set:  26000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.4177981672110036 0.9902439024390244 0.10242040501907468 1.0 103.43621924519539 0.6178 319.70918369293213\n",
      "AL epoch:  7\n",
      "selEpoch: 7, Selection Ended at: 2021-04-02 09:44:19.691830\n",
      "After augmentation, size of train_set:  2300  lake set:  25750\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.440098370308988 0.9904347826086957 0.7065832676598802 0.96 86.93276101350784 0.6514 320.7349257469177\n",
      "AL epoch:  8\n",
      "selEpoch: 8, Selection Ended at: 2021-04-02 09:49:42.342225\n",
      "After augmentation, size of train_set:  2550  lake set:  25500\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.1211487017571926 0.9933333333333333 0.25704322662204504 0.98 87.49789571762085 0.6536 376.8043670654297\n",
      "AL epoch:  9\n",
      "selEpoch: 9, Selection Ended at: 2021-04-02 09:56:01.073897\n",
      "After augmentation, size of train_set:  2800  lake set:  25250\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.942159432452172 0.9903571428571428 0.12010641931556165 1.0 80.76021859049797 0.6716 455.83142352104187\n",
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  550 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/random/250/2\n",
      "cifar10_ood_random_random_budget:250_epochs:10_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "selEpoch: 1, Selection Ended at: 2021-04-02 10:03:43.996641\n",
      "After augmentation, size of train_set:  800  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.5601718174293637 0.99375 0.13664106036594603 1.0 95.5088162124157 0.656 141.3902530670166\n",
      "AL epoch:  2\n",
      "selEpoch: 2, Selection Ended at: 2021-04-02 10:06:07.582142\n",
      "After augmentation, size of train_set:  1050  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.0311954435892403 0.9914285714285714 0.02733409299980849 1.0 83.90139654278755 0.6688 154.4466588497162\n",
      "AL epoch:  3\n",
      "selEpoch: 3, Selection Ended at: 2021-04-02 10:08:43.948978\n",
      "After augmentation, size of train_set:  1300  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.4835046904627234 0.9915384615384616 0.034341965219937265 1.0 98.96816396713257 0.6346 212.15668725967407\n",
      "AL epoch:  4\n",
      "selEpoch: 4, Selection Ended at: 2021-04-02 10:12:18.000274\n",
      "After augmentation, size of train_set:  1550  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.1801175900618546 0.9903225806451613 0.06742593832314014 1.0 89.49491965770721 0.6496 247.66782975196838\n",
      "AL epoch:  5\n",
      "selEpoch: 5, Selection Ended at: 2021-04-02 10:16:27.856364\n",
      "After augmentation, size of train_set:  1800  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.173454174073413 0.9933333333333333 0.3377773789688945 0.96 103.16264188289642 0.6194 238.84373807907104\n",
      "AL epoch:  6\n",
      "selEpoch: 6, Selection Ended at: 2021-04-02 10:20:28.648741\n",
      "After augmentation, size of train_set:  2050  lake set:  26000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.8760117053752765 0.9926829268292683 0.2745531899854541 0.98 106.98863810300827 0.6024 314.7899453639984\n",
      "AL epoch:  7\n",
      "selEpoch: 7, Selection Ended at: 2021-04-02 10:25:45.352518\n",
      "After augmentation, size of train_set:  2300  lake set:  25750\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.627366547705606 0.99 0.08105523698031902 1.0 102.79326543211937 0.6144 334.25580883026123\n",
      "AL epoch:  8\n",
      "selEpoch: 8, Selection Ended at: 2021-04-02 10:31:21.537634\n",
      "After augmentation, size of train_set:  2550  lake set:  25500\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.570067242486402 0.9909803921568627 0.1281619444489479 1.0 84.41679912805557 0.6476 366.23520278930664\n",
      "AL epoch:  9\n",
      "selEpoch: 9, Selection Ended at: 2021-04-02 10:37:29.697111\n",
      "After augmentation, size of train_set:  2800  lake set:  25250\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 4.47096904515638 0.9925 0.24456020630896091 0.98 94.16735988855362 0.6368 422.8662893772125\n",
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  550 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/random/250/3\n",
      "cifar10_ood_random_random_budget:250_epochs:10_runs3\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "selEpoch: 1, Selection Ended at: 2021-04-02 10:44:38.812249\n",
      "After augmentation, size of train_set:  800  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 0.8482529560569674 0.99625 0.21309760597068816 0.98 83.2020680308342 0.6706 113.91220450401306\n",
      "AL epoch:  2\n",
      "selEpoch: 2, Selection Ended at: 2021-04-02 10:46:34.621939\n",
      "After augmentation, size of train_set:  1050  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.1742166911717504 0.9904761904761905 0.07708534598350525 1.0 102.03880798816681 0.6214 162.30139565467834\n",
      "AL epoch:  3\n",
      "selEpoch: 3, Selection Ended at: 2021-04-02 10:49:18.823892\n",
      "After augmentation, size of train_set:  1300  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.8374478017503861 0.9915384615384616 0.21498131926637143 0.98 95.1519103050232 0.6494 219.93868255615234\n",
      "AL epoch:  4\n",
      "selEpoch: 4, Selection Ended at: 2021-04-02 10:53:00.657892\n",
      "After augmentation, size of train_set:  1550  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.4140971570741385 0.9903225806451613 0.08016548678278923 1.0 89.7610182762146 0.658 242.35191416740417\n",
      "AL epoch:  5\n",
      "selEpoch: 5, Selection Ended at: 2021-04-02 10:57:04.919991\n",
      "After augmentation, size of train_set:  1800  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.9385595894418657 0.9905555555555555 0.5987983196973801 0.96 104.47840923070908 0.6024 250.70315217971802\n",
      "AL epoch:  6\n",
      "selEpoch: 6, Selection Ended at: 2021-04-02 11:01:17.540422\n",
      "After augmentation, size of train_set:  2050  lake set:  26000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.1722179781645536 0.995609756097561 0.09976453706622124 1.0 84.77124893665314 0.6516 323.31091594696045\n",
      "AL epoch:  7\n",
      "selEpoch: 7, Selection Ended at: 2021-04-02 11:06:42.761937\n",
      "After augmentation, size of train_set:  2300  lake set:  25750\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.673627009615302 0.9904347826086957 0.3608453217893839 0.96 107.43248200416565 0.6098 355.7695505619049\n",
      "AL epoch:  8\n",
      "selEpoch: 8, Selection Ended at: 2021-04-02 11:12:40.438602\n",
      "After augmentation, size of train_set:  2550  lake set:  25500\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.5001222389983013 0.9933333333333333 0.045151056605391204 1.0 85.55981421470642 0.6592 433.8608453273773\n",
      "AL epoch:  9\n",
      "selEpoch: 9, Selection Ended at: 2021-04-02 11:19:56.215392\n",
      "After augmentation, size of train_set:  2800  lake set:  25250\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.6571842457633466 0.99 0.3475836478173733 0.96 93.13130760192871 0.6434 409.06700682640076\n",
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  550 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/random/250/4\n",
      "cifar10_ood_random_random_budget:250_epochs:10_runs4\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "selEpoch: 1, Selection Ended at: 2021-04-02 11:26:51.964053\n",
      "After augmentation, size of train_set:  800  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.127566350856796 0.99125 0.2658982954453677 0.98 104.73275810480118 0.6314 111.0058434009552\n",
      "AL epoch:  2\n",
      "selEpoch: 2, Selection Ended at: 2021-04-02 11:28:44.995778\n",
      "After augmentation, size of train_set:  1050  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.0587334356969222 0.9961904761904762 0.16110328800277784 1.0 99.2733924984932 0.6414 177.91693925857544\n",
      "AL epoch:  3\n",
      "selEpoch: 3, Selection Ended at: 2021-04-02 11:31:45.047207\n",
      "After augmentation, size of train_set:  1300  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.6742135644890368 0.9961538461538462 0.026416753185912967 1.0 80.34137934446335 0.671 201.56660795211792\n",
      "AL epoch:  4\n",
      "selEpoch: 4, Selection Ended at: 2021-04-02 11:35:08.511727\n",
      "After augmentation, size of train_set:  1550  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.494142726296559 0.9929032258064516 0.06099729158449918 1.0 101.21543145179749 0.6218 223.19811010360718\n",
      "AL epoch:  5\n",
      "selEpoch: 5, Selection Ended at: 2021-04-02 11:38:53.608985\n",
      "After augmentation, size of train_set:  1800  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.054296956048347 0.995 0.13822172989603132 1.0 105.19504129886627 0.603 272.86603140830994\n",
      "AL epoch:  6\n",
      "selEpoch: 6, Selection Ended at: 2021-04-02 11:43:28.380418\n",
      "After augmentation, size of train_set:  2050  lake set:  26000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.638739675283432 0.9917073170731707 0.40622144169174135 1.0 110.0075831413269 0.576 262.69329380989075\n",
      "AL epoch:  7\n",
      "selEpoch: 7, Selection Ended at: 2021-04-02 11:47:53.011315\n",
      "After augmentation, size of train_set:  2300  lake set:  25750\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.1133693656884134 0.9926086956521739 0.21941900183446705 1.0 87.10232377052307 0.65 348.2750427722931\n",
      "AL epoch:  8\n",
      "selEpoch: 8, Selection Ended at: 2021-04-02 11:53:43.186542\n",
      "After augmentation, size of train_set:  2550  lake set:  25500\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.588943686860148 0.9929411764705882 0.10141533613204956 1.0 93.62231069803238 0.6408 427.542546749115\n",
      "AL epoch:  9\n",
      "selEpoch: 9, Selection Ended at: 2021-04-02 12:00:52.649562\n",
      "After augmentation, size of train_set:  2800  lake set:  25250\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 5.155834064411465 0.99 0.08362955879420042 1.0 106.69824993610382 0.6032 382.7394211292267\n",
      "random random\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  550 Val size:  50 Lake size:  27500 Test set:  5000\n",
      "selected classes are:  [8 1 5 0 7]\n",
      "Saving results to:  SMI_active_learning_results/cifar10/ood/random/250/5\n",
      "cifar10_ood_random_random_budget:250_epochs:10_runs5\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training\n",
      "AL epoch:  1\n",
      "selEpoch: 1, Selection Ended at: 2021-04-02 12:07:21.872950\n",
      "After augmentation, size of train_set:  800  lake set:  27250\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.7352589012589306 0.99125 0.567921853158623 0.98 93.33616715669632 0.6358 96.47536969184875\n",
      "AL epoch:  2\n",
      "selEpoch: 2, Selection Ended at: 2021-04-02 12:09:00.239400\n",
      "After augmentation, size of train_set:  1050  lake set:  27000\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.303734221844934 0.9923809523809524 0.11560836294665933 1.0 100.82479912042618 0.6222 175.03675961494446\n",
      "AL epoch:  3\n",
      "selEpoch: 3, Selection Ended at: 2021-04-02 12:11:57.183550\n",
      "After augmentation, size of train_set:  1300  lake set:  26750\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 1.1999587528407574 0.9976923076923077 0.08766862703487277 1.0 113.07772088050842 0.6002 209.64637160301208\n",
      "AL epoch:  4\n",
      "selEpoch: 4, Selection Ended at: 2021-04-02 12:15:28.737406\n",
      "After augmentation, size of train_set:  1550  lake set:  26500\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 2.025252896710299 0.9929032258064516 0.09762176391086541 1.0 83.24653273820877 0.681 242.81268572807312\n",
      "AL epoch:  5\n",
      "selEpoch: 5, Selection Ended at: 2021-04-02 12:19:33.456744\n",
      "After augmentation, size of train_set:  1800  lake set:  26250\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.477835336816497 0.9905555555555555 0.19008614122867584 1.0 113.90878236293793 0.6012 316.20295310020447\n",
      "AL epoch:  6\n",
      "selEpoch: 6, Selection Ended at: 2021-04-02 12:24:51.552384\n",
      "After augmentation, size of train_set:  2050  lake set:  26000\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.5693273993674666 0.9912195121951219 0.09721184568479657 1.0 90.36046922206879 0.645 273.49690556526184\n",
      "AL epoch:  7\n",
      "selEpoch: 7, Selection Ended at: 2021-04-02 12:29:26.946530\n",
      "After augmentation, size of train_set:  2300  lake set:  25750\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.754189036670141 0.9917391304347826 0.08979011222254485 1.0 92.0713741183281 0.6426 347.76988220214844\n",
      "AL epoch:  8\n",
      "selEpoch: 8, Selection Ended at: 2021-04-02 12:35:16.631405\n",
      "After augmentation, size of train_set:  2550  lake set:  25500\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.317333521670662 0.9929411764705882 0.07650625659152865 1.0 90.66566663980484 0.6576 409.64942741394043\n",
      "AL epoch:  9\n",
      "selEpoch: 9, Selection Ended at: 2021-04-02 12:42:08.199506\n",
      "After augmentation, size of train_set:  2800  lake set:  25250\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 3.98571583093144 0.9935714285714285 0.07855167309753597 1.0 98.52441263198853 0.6264 464.33018922805786\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    random_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, i, device, False, \"random\",'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] [1, 3]\n"
     ]
    }
   ],
   "source": [
    "x=[1,2,3,4]\n",
    "val_idx =  list(np.random.choice(np.array(x), size=2, replace=False))\n",
    "print(x, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
