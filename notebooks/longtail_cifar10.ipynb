{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a426ba5d",
   "metadata": {},
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817d913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd())\n",
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from cords.cords.selectionstrategies.supervisedlearning import DataSelectionStrategy\n",
    "# from cords.cords.utils.models import ResNet18\n",
    "from distil.distil.utils.models.resnet import ResNet18\n",
    "# from ..gable.utils.models import MobileNetV2\n",
    "from gable.gable.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_recall_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from distil.distil.active_learning_strategies import BADGE, EntropySampling, GLISTER, GradMatchActive, CoreSet, LeastConfidence, MarginSampling \n",
    "from distil.distil.utils.data_handler import DataHandler_CIFAR10, DataHandler_MNIST\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "# for cuda\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac79daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38676ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class custom_subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "        labels(sequence) : targets as required for the indices. will be the same length as indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, labels):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.targets = labels.type(torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target = self.targets[idx]\n",
    "        return (image, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31810eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device):\n",
    "    if name == 'ResNet18':\n",
    "        model = ResNet18(num_cls)\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    elif name == 'MobileNetV2':\n",
    "        model = MobileNetV2(num_cls)\n",
    "#         model = models.mobilenet_v2(pretrained=True)\n",
    "#         model.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(model.last_channel, num_cls),\n",
    "#         ) \n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def kernel(x, y, measure=\"cosine\", exp=2):\n",
    "    if(measure==\"eu_sim\"):\n",
    "        dist = pairwise_distances(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = max(dist.ravel()) - dist\n",
    "#         n = x.size(0)\n",
    "#         m = y.size(0)\n",
    "#         d = x.size(1)\n",
    "#         x = x.unsqueeze(1).expand(n, m, d)\n",
    "#         y = y.unsqueeze(0).expand(n, m, d)\n",
    "#         dist = torch.pow(x - y, exp).sum(2)\n",
    "#         const = torch.max(dist).item()\n",
    "#         sim = (const - dist)\n",
    "    \n",
    "        #dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))\n",
    "    if(measure==\"cosine\"):\n",
    "        sim = cosine_similarity(x.cpu().numpy(), y.cpu().numpy())\n",
    "    return sim\n",
    "\n",
    "\n",
    "def save_kernel_hdf5(lake_kernel, lake_target_kernel, target_kernel=[], lake_private_kernel=[], pp_kernel=[], numpy=True):\n",
    "    if(not(numpy)):\n",
    "        lake_kernel = lake_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_kernel_\" + device.split(\":\")[1] +\".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_target_kernel = lake_target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_target_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_target_kernel)\n",
    "    if(not(numpy)):\n",
    "        target_kernel = target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_target_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=target_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_private_kernel = lake_private_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_private_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_private_kernel)\n",
    "    if(not(numpy)):\n",
    "        pp_kernel = pp_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_pp_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=pp_kernel)\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "#     saveDir = os.path.join(saveDir, prefix)\n",
    "#     if(not(os.path.exists(saveDir))):\n",
    "#         os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "        print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = custom_subset(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    if(feature==\"ood\"): \n",
    "        ood_lake_idx = list(set(lake_subset_idxs)-set(subset))\n",
    "        private_set =  custom_subset(true_lake_set, ood_lake_idx, torch.Tensor(np.array([split_cfg['num_cls_idc']]*len(ood_lake_idx))).float())\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = custom_subset(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    remain_true_lake_set = custom_subset(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "    print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    if(feature!=\"ood\"): assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    if(feature==\"ood\"): \n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set, private_set, lake_ss\n",
    "    else:\n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set\n",
    "                        \n",
    "def getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getMisclsSetNumpy(X_val, y_val, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return X_val[miscls_idx], y_val[miscls_idx]\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def getSMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, sf):\n",
    "    if(sf==\"fl1mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf == \"logdetmi\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \"  -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryqueryKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"fl2mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"gcmi\" or sf==\"div-gcmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"gccg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numPrivates \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"fl1cg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numPrivates \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"logdetcg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numPrivates \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateprivateKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"fl\" or sf==\"logdet\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf ==\"gc\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf ==\"dsum\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    print(\"Executing SIM command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "def getCMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, numPrivates, sf):\n",
    "    if(sf==\"flmic\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode joint -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -numPrivates \" + numPrivates + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_private_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    if(sf == \"logdetmic\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode joint -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -numPrivates \" + numPrivates + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_private_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    \n",
    "    print(\"Executing CMI command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "def remove_ood_points(lake_set, subset, idc_idx):\n",
    "    idx_subset = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in idc_idx:\n",
    "        idc_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        idx_subset += list(np.array(subset)[idc_subset_idx])\n",
    "    print(len(idx_subset),\"/\",len(subset), \" idc points.\")\n",
    "    return idx_subset\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "#check overlap with prev selections\n",
    "def check_overlap(prev_idx, prev_idx_hist, idx):\n",
    "    prev_idx = [int(x/num_rep) for x in prev_idx]\n",
    "    prev_idx_hist = [int(x/num_rep) for x in prev_idx_hist]\n",
    "    idx = [int(x/num_rep) for x in idx]\n",
    "    # overlap = set(prev_idx).intersection(set(idx))\n",
    "    overlap = [value for value in idx if value in prev_idx] \n",
    "    # overlap_hist = set(prev_idx_hist).intersection(set(idx))\n",
    "    overlap_hist = [value for value in idx if value in prev_idx_hist]\n",
    "    new_points = set(idx) - set(prev_idx_hist)\n",
    "    total_unique_points = set(idx+prev_idx_hist)\n",
    "    print(\"New unique points: \", len(new_points))\n",
    "    print(\"Total unique points: \", len(total_unique_points))\n",
    "    print(\"overlap % of sel with prev idx: \", len(overlap)/len(idx))\n",
    "    print(\"overlap % of sel with all prev idx: \", len(overlap_hist)/len(idx))\n",
    "    return len(overlap)/len(idx), len(overlap_hist)/len(idx)\n",
    "\n",
    "def getFeatures(model, dataloader):\n",
    "    pass\n",
    "\n",
    "def get_roc_auc(target, output, n_classes):\n",
    "    target = label_binarize(target, classes=list(range(n_classes)))\n",
    "    output = np.array(output)\n",
    "#     print('Targets: ',target,' Outputs: ',output)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(target[:,i],output[:,i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        fpr[i] = fpr[i].tolist()\n",
    "        tpr[i] = tpr[i].tolist()\n",
    "        roc_auc[i] = roc_auc[i].tolist()\n",
    "#     fig, ax = plt.subplots(figsize=(4,4))\n",
    "#     ax.plot([0, 1], [0, 1], 'k--')\n",
    "#     ax.set_xlim([0.0, 1.0])\n",
    "#     ax.set_ylim([0.0, 1.05])\n",
    "#     ax.set_xlabel('False Positive Rate')\n",
    "#     ax.set_ylabel('True Positive Rate')\n",
    "#     ax.set_title('Receiver operating characteristic example')\n",
    "#     for i in range(n_classes):\n",
    "#         ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))\n",
    "#     ax.legend(loc=\"best\")\n",
    "#     ax.grid(alpha=.4)\n",
    "#     plt.show()\n",
    "    return fpr,tpr,roc_auc\n",
    "    \n",
    "def get_pr_auc(target, output, n_classes):\n",
    "    target = label_binarize(target, classes=list(range(n_classes)))\n",
    "    output = np.array(output)\n",
    "#     print('Targets: ',target,' Outputs: ',output)\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    aupr = dict()\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(target[:,i],output[:,i])\n",
    "        aupr[i] = auc(recall[i], precision[i])\n",
    "        precision[i] = precision[i].tolist()\n",
    "        recall[i] = recall[i].tolist()\n",
    "        aupr[i] = aupr[i].tolist()\n",
    "#     fig, ax = plt.subplots(figsize=(4,4))\n",
    "#     ax.plot([0, 1], [0, 1], 'k--')\n",
    "#     ax.set_xlim([0.0, 1.0])\n",
    "#     ax.set_ylim([0.0, 1.05])\n",
    "#     ax.set_xlabel('Recall')\n",
    "#     ax.set_ylabel('Precision')\n",
    "#     ax.set_title('Receiver operating characteristic example')\n",
    "#     for i in range(n_classes):\n",
    "#         ax.plot(recall[i], precision[i], label='Precision Recall curve (area = %0.2f) for label %i' % (aupr[i], i))\n",
    "#     ax.legend(loc=\"best\")\n",
    "#     ax.grid(alpha=.4)\n",
    "#     plt.show()\n",
    "    return precision, recall, aupr\n",
    "\n",
    "def get_macro_roc_auc(target, output):\n",
    "    target = label_binarize(target,classes=list(range(num_cls)))\n",
    "    output = np.array(output)\n",
    "    return roc_auc_score(target,output,average=\"macro\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e5d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "datadir = '../data'\n",
    "data_name = 'cifar10'\n",
    "num_cls=10\n",
    "fraction = float(0.1)\n",
    "budget=250\n",
    "num_epochs = int(10)\n",
    "num_rep = 10\n",
    "model_name = 'ResNet18'\n",
    "learning_rate = 0.01\n",
    "feature = 'longtail'\n",
    "split_cfg = {\"num_cls_imbalance\":10,\n",
    "             \"sel_cls_idx\":[0,1,2,3,4,5,6,7,8,9],\n",
    "             \"per_imbclass_train\":{0:3,1:6,2:12,3:24,4:48,5:96,6:192,7:384,8:768,9:1536}, \n",
    "             \"per_imbclass_val\":{0:10,1:10,2:10,3:10,4:10,5:10,6:10,7:10,8:10,9:10},\n",
    "             \"per_imbclass_lake\":{0:60,1:120,2:240,3:480,4:960,5:960,6:1920,7:3840,8:4222,9:3454}, #0-4:20x, 5-7:10x, 8-9:Remaining \n",
    "            } #cifar10\n",
    "initModelPath = \"weights/\"+data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_imbclass_train\"])\n",
    "num_runs = 1  # number of random runs\n",
    "computeClassErrorLog = True\n",
    "run=1\n",
    "magnification = 1\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "datkbuildPath = \"/scratch4/snk170001/code/datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "print(\"Using Device:\", device)\n",
    "doublePrecision = True\n",
    "linearLayer = True\n",
    "handler = DataHandler_CIFAR10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd88b65",
   "metadata": {},
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd258a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    if(feature==\"classimb\" or feature==\"longtail\" or feature==\"ood\"):\n",
    "        if(strategy == \"SIM\" or strategy == \"SF\" or strategy==\"random\"):\n",
    "            if(strategy == \"SF\" or strategy==\"random\"):\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, True)\n",
    "            else:\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "        elif(strategy==\"AL\"):\n",
    "            if(sf==\"badge\" or sf==\"us\"):\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "            else: #dont augment train with valid\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "        print(\"selected classes are: \", sel_cls_idx)\n",
    "    if(feature==\"duplicate\" or feature==\"vanilla\"):\n",
    "        sel_cls_idx = None\n",
    "        if(strategy == \"SIM\" or strategy==\"random\"):\n",
    "            train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        elif(strategy==\"AL\"):\n",
    "            X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True)\n",
    "    if(feature==\"ood\"): num_cls+=1 #Add one class for OOD class\n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 10\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 10\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    val_csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "    all_logs_dir = 'active_learning_results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_linear:'  + str(linearLayer) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \"feature\":feature, \"sel_func\":sf, \"sel_budget\":budget, \"num_selections\":num_epochs, \"model\":model_name, \"learning_rate\":learning_rate, \"setting\":split_cfg, \"all_class_acc\":None, \"test_acc\":[],\"sel_per_cls\":[], \"sel_cls_idx\":sel_cls_idx, \"trn_fpr\":[], \"trn_tpr\":[], \"trn_roc_auc\":[], \"val_fpr\":[], \"val_tpr\":[], \"val_roc_auc\":[], \"tst_fpr\":[], \"tst_tpr\":[], \"tst_roc_auc\":[], \"trn_precision\":[], \"trn_recall\":[], \"trn_aupr\":[], \"val_precision\":[], \"val_recall\":[], \"val_aupr\":[], \"tst_precision\":[], \"tst_recall\":[], \"tst_aupr\":[], \"macro_avg_roc\":[]}\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device)\n",
    "    model1 = create_model(model_name, num_cls, device)\n",
    "    if(strategy == \"AL\"):\n",
    "        strategy_args = {'batch_size' : budget, 'lr':float(0.001), 'device':device}\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"glister\" or sf==\"glister-tss\"):\n",
    "            strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args, valid=True, X_val=X_val, Y_val=y_val, typeOf='rand', lam=0.1)\n",
    "        elif(sf==\"gradmatch-tss\"):\n",
    "            strategy_args = {'batch_size' : 1, 'lr':float(0.01)}\n",
    "            strategy_sel = GradMatchActive(X_tr, y_tr, X_unlabeled, model, F.cross_entropy, handler, num_cls, strategy_args[\"lr\"], \"PerBatch\", False, strategy_args, valid=True, X_val=X_val, Y_val=y_val)\n",
    "        elif(sf==\"coreset\"):\n",
    "            strategy_sel = CoreSet(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"leastconf\"):\n",
    "            strategy_sel = LeastConfidence(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"margin\"):\n",
    "            strategy_sel = MarginSampling(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "    optimizer, scheduler = optimizer_with_scheduler(model, 150, learning_rate)\n",
    "#     optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "    #overlap vars\n",
    "    prev_idx = None\n",
    "    prev_idx_hist = []\n",
    "    sel_hist = []\n",
    "    per_ep_overlap = []\n",
    "    overall_overlap = []\n",
    "    idx_tracker = np.array(list(range(len(lake_set))))\n",
    "    #kernels\n",
    "    train_private_kernel = []\n",
    "    train_val_kernel = []\n",
    "    pp_kernel = []\n",
    "    val_kernel = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    final_val_targets = []\n",
    "                    final_val_outputs = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "                        final_val_targets += list(targets.cpu().numpy())\n",
    "                        final_val_outputs += list(outputs.cpu().numpy().tolist())\n",
    "                    val_fpr, val_tpr, val_roc_auc = get_roc_auc(final_val_targets,final_val_outputs,num_cls)\n",
    "                    val_precision, val_recall, val_aupr = get_pr_auc(final_val_targets,final_val_outputs,num_cls)\n",
    "                    res_dict[\"val_fpr\"].append(val_fpr)\n",
    "                    res_dict[\"val_tpr\"].append(val_tpr)\n",
    "                    res_dict[\"val_roc_auc\"].append(val_roc_auc)\n",
    "                    res_dict[\"val_precision\"].append(val_precision)\n",
    "                    res_dict[\"val_recall\"].append(val_recall)\n",
    "                    res_dict[\"val_aupr\"].append(val_aupr)\n",
    "  \n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        final_tst_predictions = []\n",
    "                        final_tst_classifications = []\n",
    "                    final_tst_targets = []\n",
    "                    final_tst_outputs = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        final_tst_targets += list(targets.cpu().numpy())\n",
    "                        final_tst_outputs += list(outputs.cpu().numpy().tolist())\n",
    "                        if((val_correct/val_total) > best_val_acc):\n",
    "                            final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                            final_tst_classifications += list(predicted.eq(targets).cpu().numpy())  \n",
    "                    tst_fpr, tst_tpr, tst_roc_auc = get_roc_auc(final_tst_targets,final_tst_outputs,num_cls)\n",
    "                    tst_precision, tst_recall, tst_aupr = get_pr_auc(final_tst_targets,final_tst_outputs,num_cls)\n",
    "                    macro_avg_roc = get_macro_roc_auc(final_tst_targets,final_tst_outputs)\n",
    "                    res_dict[\"tst_fpr\"].append(tst_fpr)\n",
    "                    res_dict[\"tst_tpr\"].append(tst_tpr)\n",
    "                    res_dict[\"tst_roc_auc\"].append(tst_roc_auc)\n",
    "                    res_dict[\"tst_precision\"].append(tst_precision)\n",
    "                    res_dict[\"tst_recall\"].append(tst_recall)\n",
    "                    res_dict[\"tst_aupr\"].append(tst_aupr)\n",
    "                    res_dict[\"macro_avg_roc\"].append(macro_avg_roc)\n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "                continue\n",
    "        else:\n",
    "#             if(full_trn_acc[i-1] >= 0.99): #The model has already trained on the seed dataset\n",
    "            #use misclassifications on validation set as queries\n",
    "            #compute hypothesized labels\n",
    "            hyp_lake_labels = []\n",
    "            for batch_idx, (inputs, _) in enumerate(lakeloader):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                hyp_lake_labels += list(predicted)\n",
    "            lake_set = custom_subset(lake_set, list(range(len(hyp_lake_labels))), torch.Tensor(hyp_lake_labels))\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "#             sys.exit()\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append(tst_err_log)\n",
    "                val_csvlog.append(val_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\" or strategy==\"SF\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\" or feature==\"longtail\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set = getMisclsSet(val_set, val_class_err_idxs, list(range(num_cls)))\n",
    "                        misclsloader = torch.utils.data.DataLoader(miscls_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, misclsloader, model1, num_cls, linearLayer, F.cross_entropy, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, F.cross_entropy, device)\n",
    "                elif(sf.endswith(\"cg\")): #atleast one selection must be done for private set in cond gain functions\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, privateSetloader, model1, num_cls, linearLayer, F.cross_entropy, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        #compute subset with private set a NULL\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, F.cross_entropy, device)\n",
    "                elif(sf.endswith(\"mic\")): #configured for the OOD setting\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, F.cross_entropy, device) #In-dist samples are in Val\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model_private = DataSelectionStrategy(privateSetloader, privateSetloader, model1, num_cls, linearLayer, F.cross_entropy, device)\n",
    "                else:\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, F.cross_entropy, device)\n",
    "                start_time = time.time()\n",
    "                cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "                clone_dict = copy.deepcopy(model.state_dict())\n",
    "                #update the selection strategy model with new params for gradient computation\n",
    "                setf_model.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mic\") and len(private_set)!=0): setf_model_private.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mi\")): #SMI functions need the target set gradients\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "#                     print(setf_model.grads_per_elem)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "#                     print(setf_model.val_grads_per_elem)\n",
    "                    if(doublePrecision):\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    else:\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                    numQueryPrivate = train_val_kernel.shape[1]\n",
    "                elif(sf.endswith(\"cg\")):\n",
    "                    if(len(private_set)!=0):\n",
    "                        setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                        print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                        print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                        if(doublePrecision):\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                        else:\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                        numQueryPrivate = train_val_kernel.shape[1]\n",
    "                    else:\n",
    "#                         assert(((i + 1)/select_every)==1)\n",
    "                        setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        train_val_kernel = []\n",
    "                        numQueryPrivate = 0\n",
    "                elif(sf.endswith(\"mic\")):\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                    if(doublePrecision):\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    else:\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                    numQuery = train_val_kernel.shape[1]\n",
    "                    if(len(private_set)!=0): \n",
    "                        setf_model_private.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        print(\"private gradients shape: \", setf_model_private.grads_per_elem.shape)\n",
    "                        if(doublePrecision):\n",
    "                            train_private_kernel = kernel(setf_model.grads_per_elem.double(), setf_model_private.grads_per_elem.double()) #img_private_kernel\n",
    "                        else:\n",
    "                            train_private_kernel = kernel(setf_model.grads_per_elem, setf_model_private.grads_per_elem) #img_private_kernel\n",
    "                        numPrivate = train_private_kernel.shape[1]\n",
    "                    else:\n",
    "                        train_private_kernel = []\n",
    "                        numPrivate = 0\n",
    "                else: # For other submodular functions needing only image kernel\n",
    "                    setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                    numQueryPrivate = 0\n",
    "\n",
    "                kernel_time = time.time()\n",
    "                if(doublePrecision):\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.grads_per_elem.double()) #img_img_kernel\n",
    "                else:\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem, setf_model.grads_per_elem) #img_img_kernel\n",
    "                if(sf==\"logdetmi\" or sf==\"logdetcg\" or sf==\"logdetmic\"):\n",
    "                    if(sf==\"logdetcg\"):\n",
    "                        if(len(private_set)!=0):\n",
    "                            val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#private_private_kernel\n",
    "                        else:\n",
    "                            val_kernel = []\n",
    "                    if(sf==\"logdetmi\"):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                    if(sf==\"logdetmic\"):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                        pp_kernel = kernel(setf_model_private.grads_per_elem, setf_model_private.grads_per_elem)#query_query_kernel\n",
    "                save_kernel_hdf5(train_kernel, train_val_kernel, val_kernel, train_private_kernel, pp_kernel)\n",
    "#                 else:\n",
    "#                     save_kernel_hdf5(train_kernel, train_val_kernel)\n",
    "                print(\"kernel compute time: \", time.time()-kernel_time)\n",
    "                #call the c++ exec to read kernel and compute subset of selected minibatches\n",
    "                if(sf.endswith(\"mic\")):\n",
    "                    subset = getCMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQuery), str(numPrivate), sf)\n",
    "                else:\n",
    "                    subset = getSMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQueryPrivate), sf)\n",
    "                print(\"True targets of subset: \", torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "                print(\"Hypothesized targets of subset: \", torch.Tensor(lake_set.targets.float())[subset])\n",
    "                model.load_state_dict(cached_state_dict)\n",
    "                if(sf.endswith(\"cg\")): #for first selection\n",
    "                    if(len(private_set)==0):\n",
    "                        private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "                    else:\n",
    "                        private_set = getPrivateSet(lake_set, subset, private_set)\n",
    "                    print(\"size of private set: \", len(private_set))\n",
    "\n",
    "    #           temp = np.array(list(trainloader.batch_sampler))[subset] #if per batch\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\" or sf==\"gradmatch-tss\" and (feature==\"classimb\" or feature==\"longtail\")):\n",
    "                    miscls_X_val, miscls_y_val = getMisclsSetNumpy(X_val, y_val, val_class_err_idxs, sel_cls_idx)\n",
    "                    if(sf==\"glister-tss\"): strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args, valid=True, X_val=miscls_X_val, Y_val=miscls_y_val, typeOf='rand', lam=0.1)\n",
    "                    if(sf==\"gradmatch-tss\"): strategy_sel = GradMatchActive(X_tr, y_tr, X_unlabeled, model, F.cross_entropy, handler, num_cls, strategy_args[\"lr\"], \"PerBatch\", False, strategy_args, valid=True, X_val=miscls_X_val, Y_val=miscls_y_val)\n",
    "                    print(\"reinit AL with targeted miscls samples\")\n",
    "                strategy_sel.update_model(model)\n",
    "                if(sf==\"badge\" or sf==\"glister\" or sf==\"glister-tss\" or sf==\"coreset\" or sf==\"margin\"):\n",
    "                    subset = strategy_sel.select(budget)\n",
    "                if(sf==\"us\" or sf==\"leastconf\"):\n",
    "                    subset = list(strategy_sel.select(budget).cpu().numpy())\n",
    "                if(sf==\"gradmatch-tss\"):\n",
    "                    subset = strategy_sel.select(budget, False) #Fixed weight gradmatch\n",
    "                print(len(subset), \" samples selected\")\n",
    "                \n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            if(i>1 and sf.endswith(\"cg\")):\n",
    "                per_ep, overall = check_overlap(prev_idx, prev_idx_hist, list(idx_tracker[subset]))\n",
    "                per_ep_overlap.append(per_ep)\n",
    "                overall_overlap.append(overall)\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            if(feature==\"ood\"): #remove ood points from the subset\n",
    "                subset = remove_ood_points(true_lake_set, subset, sel_cls_idx)\n",
    "            \n",
    "            if(strategy==\"AL\"):#augment train and remove from lake for AL startegies\n",
    "                X_tr = np.concatenate((X_tr, X_unlabeled[subset]), axis=0)\n",
    "                X_unlabeled = np.delete(X_unlabeled, lake_subset_idxs, axis = 0)\n",
    "                y_tr = np.concatenate((y_tr, y_unlabeled[subset]), axis = 0)\n",
    "                y_unlabeled = np.delete(y_unlabeled, lake_subset_idxs, axis = 0)\n",
    "                strategy_sel.update_data(X_tr, y_tr, X_unlabeled)\n",
    "                print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "            perClsSel = getPerClassSel(true_lake_set, lake_subset_idxs, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            prev_idx = list(idx_tracker[lake_subset_idxs])\n",
    "            prev_idx_hist += list(idx_tracker[lake_subset_idxs])\n",
    "            sel_hist.append(list(idx_tracker[lake_subset_idxs]))\n",
    "            idx_tracker = np.delete(idx_tracker, lake_subset_idxs, axis=0)\n",
    "            #augment the train_set with selected indices from the lake\n",
    "            if(feature==\"classimb\" or feature==\"longtail\"):\n",
    "                train_set, lake_set, true_lake_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "            elif(feature==\"ood\"):\n",
    "                train_set, lake_set, true_lake_set, new_private_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "                train_set = torch.utils.data.ConcatDataset([train_set, new_private_set]) #Add the OOD samples with a common OOD class\n",
    "                val_set = torch.utils.data.ConcatDataset([val_set, add_val_set])\n",
    "                if(len(private_set)!=0):\n",
    "                    private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "                else:\n",
    "                    private_set = new_private_set\n",
    "            else:\n",
    "                train_set, lake_set, true_lake_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set))\n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, shuffle=False, pin_memory=True)\n",
    "            assert(len(idx_tracker)==len(lake_set))\n",
    "#             model =  model.apply(weight_reset).cuda()\n",
    "            model = create_model(model_name, num_cls, device)\n",
    "#             optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "            optimizer, scheduler = optimizer_with_scheduler(model, 150, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<150):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            full_trn_targets = []\n",
    "            full_trn_outputs = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "#                     get_roc_auc(targets,outputs,num_cls)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_targets += list(targets.cpu().numpy())\n",
    "                    full_trn_outputs += list(outputs.cpu().numpy().tolist())\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        print(\"ROC Curve for train set: \\n\")\n",
    "        trn_fpr,trn_tpr,trn_roc_auc = get_roc_auc(full_trn_targets,full_trn_outputs,num_cls)\n",
    "        trn_precision, trn_recall, trn_aupr = get_pr_auc(full_trn_targets,full_trn_outputs,num_cls)\n",
    "        res_dict[\"trn_fpr\"].append(trn_fpr)\n",
    "        res_dict[\"trn_tpr\"].append(trn_tpr)\n",
    "        res_dict[\"trn_roc_auc\"].append(trn_roc_auc)\n",
    "        res_dict[\"trn_precision\"].append(trn_precision)\n",
    "        res_dict[\"trn_recall\"].append(trn_recall)\n",
    "        res_dict[\"trn_aupr\"].append(trn_aupr)\n",
    "        with torch.no_grad():\n",
    "            final_val_targets = []\n",
    "            final_val_outputs = []\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                # print(batch_idx)\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "#                 get_roc_auc(targets,outputs,num_cls)\n",
    "#                 print('Target: ',targets,'\\n Outputs:', outputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                final_val_targets += list(targets.cpu().numpy())\n",
    "                final_val_outputs += list(outputs.cpu().numpy().tolist())\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "                # sys.exit()\n",
    "\n",
    "#             if((val_correct/val_total) > best_val_acc):\n",
    "            final_tst_targets = []\n",
    "            final_tst_outputs = []\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "#                 get_roc_auc(targets,outputs,num_cls)\n",
    "#                 print('Target: ',targets,'\\n Outputs:', outputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                if(feature==\"ood\"): \n",
    "                    _, predicted = outputs[...,:-1].max(1)\n",
    "                else:\n",
    "                    _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "#                 if((val_correct/val_total) > best_val_acc):\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                final_tst_targets += list(targets.cpu().numpy())\n",
    "                final_tst_outputs += list(outputs.cpu().numpy().tolist())\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "#             if((val_correct/val_total) > best_val_acc):\n",
    "#                 best_val_acc = (val_correct/val_total)\n",
    "            print(\"ROC Curve for validation set: \\n\")\n",
    "            val_fpr, val_tpr, val_roc_auc = get_roc_auc(final_val_targets,final_val_outputs,num_cls)\n",
    "            val_precision, val_recall, val_aupr = get_pr_auc(final_val_targets,final_val_outputs,num_cls)\n",
    "            res_dict[\"val_fpr\"].append(val_fpr)\n",
    "            res_dict[\"val_tpr\"].append(val_tpr)\n",
    "            res_dict[\"val_roc_auc\"].append(val_roc_auc)\n",
    "            res_dict[\"val_precision\"].append(val_precision)\n",
    "            res_dict[\"val_recall\"].append(val_recall)\n",
    "            res_dict[\"val_aupr\"].append(val_aupr)\n",
    "            print(\"ROC Curve for test set: \\n\")\n",
    "            tst_fpr, tst_tpr, tst_roc_auc = get_roc_auc(final_tst_targets,final_tst_outputs,num_cls)\n",
    "            tst_precision, tst_recall, tst_aupr = get_pr_auc(final_tst_targets,final_tst_outputs,num_cls)\n",
    "            macro_avg_roc = get_macro_roc_auc(final_tst_targets,final_tst_outputs)\n",
    "            res_dict[\"tst_fpr\"].append(tst_fpr)\n",
    "            res_dict[\"tst_tpr\"].append(tst_tpr)\n",
    "            res_dict[\"tst_roc_auc\"].append(tst_roc_auc)\n",
    "            res_dict[\"tst_precision\"].append(tst_precision)\n",
    "            res_dict[\"tst_recall\"].append(tst_recall)\n",
    "            res_dict[\"tst_aupr\"].append(tst_aupr)\n",
    "            res_dict[\"macro_avg_roc\"].append(macro_avg_roc)\n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "            \n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append(tst_err_log)\n",
    "        val_csvlog.append(val_err_log)\n",
    "        print(csvlog)\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "        res_dict[\"all_val_class_acc\"] = val_csvlog\n",
    "#         with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    return tst_acc, csvlog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f52bcf",
   "metadata": {},
   "source": [
    "# FL2MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6421356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl2mi\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  3069 Val size:  100 Lake size:  16256\n",
      "selected classes are:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Saving results to:  active_learning_results/cifar10/longtail/fl2mi/100/1\n",
      "cifar10_longtail_SIM_10_fl2mi_budget:100_epochs:10_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "ROC Curve for train set: ing epoch [ 67 ]  Training Acc:  0.9902248289345064\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 1 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.662004909245297 0.9902248289345064 33.63485687226057 0.43 3374.031535565853 0.4516 478.19543957710266\n",
      "saving initial model\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 97.8\n",
      "val, test error% for class  1  :  100.0 98.7\n",
      "val, test error% for class  2  :  100.0 87.4\n",
      "val, test error% for class  3  :  100.0 86.5\n",
      "val, test error% for class  4  :  70.0 67.6\n",
      "val, test error% for class  5  :  80.0 60.0\n",
      "val, test error% for class  6  :  10.0 26.7\n",
      "val, test error% for class  7  :  0.0 12.2\n",
      "val, test error% for class  8  :  10.0 6.9\n",
      "val, test error% for class  9  :  0.0 4.6\n",
      "total misclassified ex from imb classes:  57\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 6 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-20af760089b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfl2mi_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl2mi_csvlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_al\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatkbuildPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputeClassErrorLog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SIM\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fl2mi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-129a389b7f8c>\u001b[0m in \u001b[0;36mtrain_model_al\u001b[0;34m(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeErrorLog, strategy, sf)\u001b[0m\n\u001b[1;32m    206\u001b[0m                         \u001b[0mmiscls_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetMisclsSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_class_err_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                         \u001b[0mmisclsloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiscls_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                         \u001b[0msetf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSelectionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlakeloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmisclsloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinearLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set last arg to true for linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                         \u001b[0msetf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSelectionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlakeloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinearLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 6 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "fl2mi_tst, fl2mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692f1fc",
   "metadata": {},
   "source": [
    "# FL1MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae581717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl1mi_tst, fl1mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab759e",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "453e1a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL badge\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  3069 Val size:  100 Lake size:  16256\n",
      "selected classes are:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Saving results to:  active_learning_results/cifar10/longtail/badge/250/1\n",
      "cifar10_longtail_AL_10_badge_budget:250_epochs:10_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ResNet18_0.01_{0: 3, 1: 6, 2: 12, 3: 24, 4: 48, 5: 96, 6: 192, 7: 384, 8: 768, 9: 1536}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 98.8\n",
      "val, test error% for class  1  :  100.0 99.7\n",
      "val, test error% for class  2  :  100.0 95.1\n",
      "val, test error% for class  3  :  100.0 94.3\n",
      "val, test error% for class  4  :  90.0 77.1\n",
      "val, test error% for class  5  :  80.0 72.0\n",
      "val, test error% for class  6  :  90.0 52.7\n",
      "val, test error% for class  7  :  90.0 21.0\n",
      "val, test error% for class  8  :  30.0 9.6\n",
      "val, test error% for class  9  :  10.0 3.5\n",
      "250  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-06-19 12:40:17.939561\n",
      "250 16006 16256\n",
      "After augmentation, size of train_set:  3319  lake set:  16006\n",
      "ROC Curve for train set: ing epoch [ 67 ]  Training Acc:  0.9912624284423021\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.989431295514805 0.991262428442302 26.955835580825806 0.53 3195.253446817398 0.4957 517.8180816173553\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  70.0 89.6\n",
      "val, test error% for class  1  :  100.0 96.2\n",
      "val, test error% for class  2  :  90.0 82.8\n",
      "val, test error% for class  3  :  90.0 88.4\n",
      "val, test error% for class  4  :  50.0 67.4\n",
      "val, test error% for class  5  :  30.0 43.3\n",
      "val, test error% for class  6  :  0.0 12.7\n",
      "val, test error% for class  7  :  20.0 12.7\n",
      "val, test error% for class  8  :  10.0 7.9\n",
      "val, test error% for class  9  :  10.0 3.3\n",
      "250  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-06-19 12:49:39.134925\n",
      "250 15756 16006\n",
      "After augmentation, size of train_set:  3569  lake set:  15756\n",
      "ROC Curve for train set: ing epoch [ 70 ]  Training Acc:  0.9932754272905576\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.736851271562045 0.9932754272905576 21.186913723126054 0.62 2562.8530635647476 0.5523 572.3441927433014\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  80.0 81.6\n",
      "val, test error% for class  1  :  100.0 92.3\n",
      "val, test error% for class  2  :  60.0 72.6\n",
      "val, test error% for class  3  :  60.0 73.2\n",
      "val, test error% for class  4  :  40.0 57.2\n",
      "val, test error% for class  5  :  10.0 35.8\n",
      "val, test error% for class  6  :  10.0 11.8\n",
      "val, test error% for class  7  :  10.0 11.1\n",
      "val, test error% for class  8  :  10.0 7.8\n",
      "val, test error% for class  9  :  0.0 4.3\n",
      "250  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-06-19 12:59:53.947213\n",
      "250 15506 15756\n",
      "After augmentation, size of train_set:  3819  lake set:  15506\n",
      "ROC Curve for train set: ing epoch [ 71 ]  Training Acc:  0.9921445404556166\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.552692614583066 0.9921445404556166 23.258226525620557 0.61 2557.812991976738 0.5682 626.1754291057587\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  70.0 85.0\n",
      "val, test error% for class  1  :  100.0 96.5\n",
      "val, test error% for class  2  :  70.0 63.6\n",
      "val, test error% for class  3  :  50.0 70.1\n",
      "val, test error% for class  4  :  50.0 43.1\n",
      "val, test error% for class  5  :  30.0 38.1\n",
      "val, test error% for class  6  :  0.0 12.5\n",
      "val, test error% for class  7  :  10.0 14.7\n",
      "val, test error% for class  8  :  10.0 4.7\n",
      "val, test error% for class  9  :  0.0 3.5\n",
      "250  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-06-19 13:11:02.050028\n",
      "250 15256 15506\n",
      "After augmentation, size of train_set:  4069  lake set:  15256\n",
      "ROC Curve for train set: ing epoch [ 74 ]  Training Acc:  0.9909068567215532\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.314221424574498 0.9909068567215532 17.97372364345938 0.61 1899.7485768478364 0.6193 699.0720782279968\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  60.0 61.3\n",
      "val, test error% for class  1  :  70.0 70.2\n",
      "val, test error% for class  2  :  60.0 71.9\n",
      "val, test error% for class  3  :  70.0 66.0\n",
      "val, test error% for class  4  :  40.0 43.3\n",
      "val, test error% for class  5  :  40.0 40.4\n",
      "val, test error% for class  6  :  0.0 6.9\n",
      "val, test error% for class  7  :  30.0 12.5\n",
      "val, test error% for class  8  :  0.0 4.8\n",
      "val, test error% for class  9  :  20.0 3.4\n",
      "250  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-06-19 13:23:22.751583\n",
      "250 15006 15256\n",
      "After augmentation, size of train_set:  4319  lake set:  15006\n",
      "ROC Curve for train set: ing epoch [ 77 ]  Training Acc:  0.9928224125955082\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.938794468558626 0.9928224125955082 15.8066046545282 0.67 1929.4155813455582 0.6304 771.3444581031799\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  60.0 70.3\n",
      "val, test error% for class  1  :  70.0 72.5\n",
      "val, test error% for class  2  :  60.0 70.2\n",
      "val, test error% for class  3  :  70.0 65.1\n",
      "val, test error% for class  4  :  20.0 28.3\n",
      "val, test error% for class  5  :  20.0 27.0\n",
      "val, test error% for class  6  :  0.0 10.8\n",
      "val, test error% for class  7  :  30.0 16.7\n",
      "val, test error% for class  8  :  0.0 3.6\n",
      "val, test error% for class  9  :  0.0 5.1\n",
      "250  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-06-19 13:36:55.840956\n",
      "250 14756 15006\n",
      "After augmentation, size of train_set:  4569  lake set:  14756\n",
      "ROC Curve for train set: ing epoch [ 78 ]  Training Acc:  0.9929962792733642\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 12.0610722307174 0.992996279273364 19.269936867291108 0.6 1996.5061589628458 0.6223 830.9082624912262\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  70.0 66.4\n",
      "val, test error% for class  1  :  80.0 73.1\n",
      "val, test error% for class  2  :  70.0 70.8\n",
      "val, test error% for class  3  :  70.0 68.1\n",
      "val, test error% for class  4  :  30.0 28.1\n",
      "val, test error% for class  5  :  30.0 37.9\n",
      "val, test error% for class  6  :  10.0 15.6\n",
      "val, test error% for class  7  :  30.0 8.7\n",
      "val, test error% for class  8  :  0.0 7.0\n",
      "val, test error% for class  9  :  10.0 2.0\n",
      "250  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-06-19 13:51:26.965467\n",
      "250 14506 14756\n",
      "After augmentation, size of train_set:  4819  lake set:  14506\n",
      "ROC Curve for train set: ing epoch [ 79 ]  Training Acc:  0.9923220585183649\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.017065700230887 0.9923220585183649 17.38985561206937 0.64 1899.683723595459 0.6522 892.3813021183014\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  60.0 66.2\n",
      "val, test error% for class  1  :  90.0 66.2\n",
      "val, test error% for class  2  :  60.0 67.2\n",
      "val, test error% for class  3  :  70.0 73.7\n",
      "val, test error% for class  4  :  30.0 20.9\n",
      "val, test error% for class  5  :  20.0 22.6\n",
      "val, test error% for class  6  :  0.0 15.8\n",
      "val, test error% for class  7  :  20.0 6.4\n",
      "val, test error% for class  8  :  0.0 6.6\n",
      "val, test error% for class  9  :  10.0 2.2\n",
      "250  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-06-19 14:06:59.768506\n",
      "250 14256 14506\n",
      "After augmentation, size of train_set:  5069  lake set:  14256\n",
      "ROC Curve for train set: ing epoch [ 78 ]  Training Acc:  0.9932925626356284\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.313297640328528 0.9932925626356284 17.42513754637912 0.66 1826.3867606961867 0.654 926.0463705062866\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  80.0 70.9\n",
      "val, test error% for class  1  :  90.0 74.3\n",
      "val, test error% for class  2  :  50.0 59.6\n",
      "val, test error% for class  3  :  50.0 43.8\n",
      "val, test error% for class  4  :  10.0 33.9\n",
      "val, test error% for class  5  :  30.0 37.3\n",
      "val, test error% for class  6  :  0.0 11.2\n",
      "val, test error% for class  7  :  30.0 7.8\n",
      "val, test error% for class  8  :  0.0 5.7\n",
      "val, test error% for class  9  :  0.0 1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-06-19 14:23:05.710023\n",
      "250 14006 14256\n",
      "After augmentation, size of train_set:  5319  lake set:  14006\n",
      "ROC Curve for train set: ing epoch [ 78 ]  Training Acc:  0.9952998683963151\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.880913501539908 0.9952998683963151 13.889364654547535 0.7 1575.1706732516177 0.686 970.0724008083344\n",
      "val, test error% for class  0  :  70.0 59.4\n",
      "val, test error% for class  1  :  80.0 63.8\n",
      "val, test error% for class  2  :  50.0 56.1\n",
      "val, test error% for class  3  :  40.0 58.0\n",
      "val, test error% for class  4  :  10.0 27.7\n",
      "val, test error% for class  5  :  10.0 27.8\n",
      "val, test error% for class  6  :  10.0 8.3\n",
      "val, test error% for class  7  :  20.0 6.4\n",
      "val, test error% for class  8  :  0.0 3.3\n",
      "val, test error% for class  9  :  10.0 3.2\n",
      "[[98.8, 99.7, 95.1, 94.3, 77.1, 72.0, 52.7, 21.0, 9.6, 3.5, 62.38000000000001], [89.6, 96.2, 82.8, 88.4, 67.4, 43.3, 12.7, 12.7, 7.9, 3.3, 50.42999999999999], [81.6, 92.3, 72.6, 73.2, 57.2, 35.8, 11.8, 11.1, 7.8, 4.3, 44.77], [85.0, 96.5, 63.6, 70.1, 43.1, 38.1, 12.5, 14.7, 4.7, 3.5, 43.18], [61.3, 70.2, 71.9, 66.0, 43.3, 40.4, 6.9, 12.5, 4.8, 3.4, 38.06999999999999], [70.3, 72.5, 70.2, 65.1, 28.3, 27.0, 10.8, 16.7, 3.6, 5.1, 36.96000000000001], [66.4, 73.1, 70.8, 68.1, 28.1, 37.9, 15.6, 8.7, 7.0, 2.0, 37.769999999999996], [66.2, 66.2, 67.2, 73.7, 20.9, 22.6, 15.8, 6.4, 6.6, 2.2, 34.78], [70.9, 74.3, 59.6, 43.8, 33.9, 37.3, 11.2, 7.8, 5.7, 1.5, 34.599999999999994], [59.4, 63.8, 56.1, 58.0, 27.7, 27.8, 8.3, 6.4, 3.3, 3.2, 31.4]]\n"
     ]
    }
   ],
   "source": [
    "# train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n",
    "badge_tst, badge_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e3f91",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e79875a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL us\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  3069 Val size:  100 Lake size:  16256\n",
      "selected classes are:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Saving results to:  active_learning_results/cifar10/longtail/us/250/1\n",
      "cifar10_longtail_AL_10_us_budget:250_epochs:10_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ResNet18_0.01_{0: 3, 1: 6, 2: 12, 3: 24, 4: 48, 5: 96, 6: 192, 7: 384, 8: 768, 9: 1536}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 98.8\n",
      "val, test error% for class  1  :  100.0 99.7\n",
      "val, test error% for class  2  :  100.0 95.1\n",
      "val, test error% for class  3  :  100.0 94.3\n",
      "val, test error% for class  4  :  100.0 77.1\n",
      "val, test error% for class  5  :  80.0 72.0\n",
      "val, test error% for class  6  :  80.0 52.7\n",
      "val, test error% for class  7  :  80.0 21.0\n",
      "val, test error% for class  8  :  30.0 9.6\n",
      "val, test error% for class  9  :  10.0 3.5\n",
      "250  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-06-19 14:39:51.474484\n",
      "250 16006 16256\n",
      "After augmentation, size of train_set:  3319  lake set:  16006\n",
      "ROC Curve for train set: ing epoch [ 70 ]  Training Acc:  0.9942753841518531\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 8.706533238590055 0.994275384151853 27.086940117180347 0.48 3029.7804719805717 0.5094 534.8257908821106\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  80.0 92.3\n",
      "val, test error% for class  1  :  100.0 97.6\n",
      "val, test error% for class  2  :  90.0 91.2\n",
      "val, test error% for class  3  :  90.0 82.3\n",
      "val, test error% for class  4  :  70.0 48.6\n",
      "val, test error% for class  5  :  50.0 38.4\n",
      "val, test error% for class  6  :  10.0 17.7\n",
      "val, test error% for class  7  :  10.0 13.0\n",
      "val, test error% for class  8  :  10.0 6.3\n",
      "val, test error% for class  9  :  10.0 3.2\n",
      "250  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-06-19 14:49:08.907823\n",
      "250 15756 16006\n",
      "After augmentation, size of train_set:  3569  lake set:  15756\n",
      "ROC Curve for train set: ing epoch [ 75 ]  Training Acc:  0.9927150462314374\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 9.665642433683388 0.9927150462314374 22.94189398828894 0.59 2583.8442988209426 0.5614 620.5947983264923\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  90.0 86.2\n",
      "val, test error% for class  1  :  100.0 94.9\n",
      "val, test error% for class  2  :  70.0 61.8\n",
      "val, test error% for class  3  :  50.0 65.0\n",
      "val, test error% for class  4  :  30.0 42.7\n",
      "val, test error% for class  5  :  60.0 50.0\n",
      "val, test error% for class  6  :  0.0 18.1\n",
      "val, test error% for class  7  :  0.0 7.5\n",
      "val, test error% for class  8  :  10.0 9.4\n",
      "val, test error% for class  9  :  0.0 3.0\n",
      "250  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-06-19 14:59:52.802934\n",
      "250 15506 15756\n",
      "After augmentation, size of train_set:  3819  lake set:  15506\n",
      "ROC Curve for train set: ing epoch [ 74 ]  Training Acc:  0.9910971458496989\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.564101812095032 0.9910971458496989 20.83815416507423 0.57 2474.022961234674 0.5683 637.3998243808746\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  90.0 79.9\n",
      "val, test error% for class  1  :  100.0 91.2\n",
      "val, test error% for class  2  :  80.0 72.1\n",
      "val, test error% for class  3  :  60.0 72.0\n",
      "val, test error% for class  4  :  50.0 57.8\n",
      "val, test error% for class  5  :  30.0 27.5\n",
      "val, test error% for class  6  :  0.0 11.2\n",
      "val, test error% for class  7  :  10.0 8.3\n",
      "val, test error% for class  8  :  0.0 8.5\n",
      "val, test error% for class  9  :  10.0 3.2\n",
      "250  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-06-19 15:10:53.205515\n",
      "250 15256 15506\n",
      "After augmentation, size of train_set:  4069  lake set:  15256\n",
      "ROC Curve for train set: ing epoch [ 74 ]  Training Acc:  0.9943475055296142\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 12.102641325676814 0.9943475055296142 20.429294193163514 0.61 2217.8598699513823 0.6022 700.7602381706238\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  80.0 77.2\n",
      "val, test error% for class  1  :  100.0 87.6\n",
      "val, test error% for class  2  :  80.0 72.8\n",
      "val, test error% for class  3  :  50.0 61.6\n",
      "val, test error% for class  4  :  20.0 28.0\n",
      "val, test error% for class  5  :  50.0 36.6\n",
      "val, test error% for class  6  :  0.0 10.1\n",
      "val, test error% for class  7  :  10.0 14.6\n",
      "val, test error% for class  8  :  0.0 4.8\n",
      "val, test error% for class  9  :  0.0 4.5\n",
      "250  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-06-19 15:22:56.373474\n",
      "250 15006 15256\n",
      "After augmentation, size of train_set:  4319  lake set:  15006\n",
      "ROC Curve for train set: ing epoch [ 77 ]  Training Acc:  0.9909701319749942\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.96467267992557 0.9909701319749942 14.509491194039583 0.66 1951.000434577465 0.6312 771.5351691246033\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  70.0 68.5\n",
      "val, test error% for class  1  :  90.0 73.1\n",
      "val, test error% for class  2  :  70.0 79.8\n",
      "val, test error% for class  3  :  40.0 56.2\n",
      "val, test error% for class  4  :  40.0 30.9\n",
      "val, test error% for class  5  :  20.0 31.4\n",
      "val, test error% for class  6  :  0.0 9.7\n",
      "val, test error% for class  7  :  10.0 10.0\n",
      "val, test error% for class  8  :  0.0 4.5\n",
      "val, test error% for class  9  :  0.0 4.7\n",
      "250  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-06-19 15:36:10.199494\n",
      "250 14756 15006\n",
      "After augmentation, size of train_set:  4569  lake set:  14756\n",
      "ROC Curve for train set: ing epoch [ 76 ]  Training Acc:  0.9929962792733648\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.080500787764322 0.992996279273364 18.869697247631848 0.67 2187.360551662743 0.6103 798.3854908943176\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  70.0 77.8\n",
      "val, test error% for class  1  :  100.0 92.3\n",
      "val, test error% for class  2  :  40.0 70.5\n",
      "val, test error% for class  3  :  60.0 60.4\n",
      "val, test error% for class  4  :  10.0 37.1\n",
      "val, test error% for class  5  :  20.0 24.7\n",
      "val, test error% for class  6  :  0.0 9.5\n",
      "val, test error% for class  7  :  10.0 7.7\n",
      "val, test error% for class  8  :  10.0 5.6\n",
      "val, test error% for class  9  :  10.0 4.1\n",
      "250  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-06-19 15:49:51.480158\n",
      "250 14506 14756\n",
      "After augmentation, size of train_set:  4819  lake set:  14506\n",
      "ROC Curve for train set: ing epoch [ 79 ]  Training Acc:  0.9908694749948121\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 16.821817617033958 0.9908694749948121 17.31151340622455 0.67 1990.3115527965128 0.6371 888.7491602897644\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  70.0 75.1\n",
      "val, test error% for class  1  :  90.0 79.9\n",
      "val, test error% for class  2  :  70.0 67.9\n",
      "val, test error% for class  3  :  50.0 53.4\n",
      "val, test error% for class  4  :  10.0 33.0\n",
      "val, test error% for class  5  :  20.0 29.5\n",
      "val, test error% for class  6  :  0.0 11.4\n",
      "val, test error% for class  7  :  20.0 6.1\n",
      "val, test error% for class  8  :  0.0 3.8\n",
      "val, test error% for class  9  :  0.0 2.8\n",
      "250  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-06-19 16:05:02.485555\n",
      "250 14256 14506\n",
      "After augmentation, size of train_set:  5069  lake set:  14256\n",
      "ROC Curve for train set: ing epoch [ 79 ]  Training Acc:  0.9903333990925232\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 21.438302113994723 0.9903333990925232 17.46037003956735 0.69 1831.3109662936768 0.6725 930.6049435138702\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  60.0 54.0\n",
      "val, test error% for class  1  :  80.0 73.2\n",
      "val, test error% for class  2  :  70.0 69.8\n",
      "val, test error% for class  3  :  60.0 58.0\n",
      "val, test error% for class  4  :  10.0 16.9\n",
      "val, test error% for class  5  :  10.0 29.0\n",
      "val, test error% for class  6  :  0.0 9.5\n",
      "val, test error% for class  7  :  20.0 8.6\n",
      "val, test error% for class  8  :  0.0 4.7\n",
      "val, test error% for class  9  :  0.0 3.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-06-19 16:20:55.262226\n",
      "250 14006 14256\n",
      "After augmentation, size of train_set:  5319  lake set:  14006\n",
      "ROC Curve for train set: ing epoch [ 84 ]  Training Acc:  0.9913517578492198\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 17.267263524088776 0.9913517578492198 15.332551185972989 0.66 1649.1507547274232 0.6867 1042.8926486968994\n",
      "val, test error% for class  0  :  70.0 54.4\n",
      "val, test error% for class  1  :  90.0 74.3\n",
      "val, test error% for class  2  :  60.0 48.9\n",
      "val, test error% for class  3  :  60.0 54.8\n",
      "val, test error% for class  4  :  10.0 26.6\n",
      "val, test error% for class  5  :  10.0 26.0\n",
      "val, test error% for class  6  :  0.0 10.8\n",
      "val, test error% for class  7  :  40.0 6.4\n",
      "val, test error% for class  8  :  0.0 8.6\n",
      "val, test error% for class  9  :  0.0 2.5\n",
      "[[98.8, 99.7, 95.1, 94.3, 77.1, 72.0, 52.7, 21.0, 9.6, 3.5, 62.38000000000001], [92.3, 97.6, 91.2, 82.3, 48.6, 38.4, 17.7, 13.0, 6.3, 3.2, 49.059999999999995], [86.2, 94.9, 61.8, 65.0, 42.7, 50.0, 18.1, 7.5, 9.4, 3.0, 43.86], [79.9, 91.2, 72.1, 72.0, 57.8, 27.5, 11.2, 8.3, 8.5, 3.2, 43.17], [77.2, 87.6, 72.8, 61.6, 28.0, 36.6, 10.1, 14.6, 4.8, 4.5, 39.780000000000015], [68.5, 73.1, 79.8, 56.2, 30.9, 31.4, 9.7, 10.0, 4.5, 4.7, 36.87999999999999], [77.8, 92.3, 70.5, 60.4, 37.1, 24.7, 9.5, 7.7, 5.6, 4.1, 38.970000000000006], [75.1, 79.9, 67.9, 53.4, 33.0, 29.5, 11.4, 6.1, 3.8, 2.8, 36.290000000000006], [54.0, 73.2, 69.8, 58.0, 16.9, 29.0, 9.5, 8.6, 4.7, 3.8, 32.75], [54.4, 74.3, 48.9, 54.8, 26.6, 26.0, 10.8, 6.4, 8.6, 2.5, 31.330000000000002]]\n"
     ]
    }
   ],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f390f",
   "metadata": {},
   "source": [
    "# GLISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10abc1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL glister-tss\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  3069 Val size:  100 Lake size:  16256\n",
      "selected classes are:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Saving results to:  active_learning_results/cifar10/longtail/glister-tss/250/1\n",
      "cifar10_longtail_AL_10_glister-tss_budget:250_epochs:10_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ResNet18_0.01_{0: 3, 1: 6, 2: 12, 3: 24, 4: 48, 5: 96, 6: 192, 7: 384, 8: 768, 9: 1536}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 98.8\n",
      "val, test error% for class  1  :  100.0 99.7\n",
      "val, test error% for class  2  :  100.0 95.1\n",
      "val, test error% for class  3  :  90.0 94.3\n",
      "val, test error% for class  4  :  100.0 77.1\n",
      "val, test error% for class  5  :  80.0 72.0\n",
      "val, test error% for class  6  :  90.0 52.7\n",
      "val, test error% for class  7  :  80.0 21.0\n",
      "val, test error% for class  8  :  60.0 9.6\n",
      "val, test error% for class  9  :  10.0 3.5\n",
      "total misclassified ex from imb classes:  81\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/strategy.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_trn = torch.tensor(Y[idxs])\n",
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-06-20 01:01:56.701314\n",
      "250 16006 16256\n",
      "After augmentation, size of train_set:  3319  lake set:  16006\n",
      "ROC Curve for train set: ing epoch [ 71 ]  Training Acc:  0.9957818620066285\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 6.379328084098233 0.9957818620066285 24.021292354911566 0.56 2890.6853822860867 0.5256 561.0333876609802\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  60.0 73.0\n",
      "val, test error% for class  1  :  100.0 93.4\n",
      "val, test error% for class  2  :  70.0 77.5\n",
      "val, test error% for class  3  :  80.0 79.5\n",
      "val, test error% for class  4  :  70.0 72.0\n",
      "val, test error% for class  5  :  40.0 40.3\n",
      "val, test error% for class  6  :  0.0 13.1\n",
      "val, test error% for class  7  :  10.0 17.1\n",
      "val, test error% for class  8  :  10.0 6.0\n",
      "val, test error% for class  9  :  0.0 2.5\n",
      "total misclassified ex from imb classes:  44\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-06-20 01:11:44.590799\n",
      "250 15756 16006\n",
      "After augmentation, size of train_set:  3569  lake set:  15756\n",
      "ROC Curve for train set: ing epoch [ 73 ]  Training Acc:  0.9921546651723172\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.7875901843654 0.9921546651723172 25.038797814399004 0.57 2639.6917184628546 0.5624 619.014411687851\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  70.0 61.0\n",
      "val, test error% for class  1  :  100.0 86.9\n",
      "val, test error% for class  2  :  70.0 76.0\n",
      "val, test error% for class  3  :  80.0 82.1\n",
      "val, test error% for class  4  :  50.0 55.0\n",
      "val, test error% for class  5  :  30.0 33.5\n",
      "val, test error% for class  6  :  0.0 18.1\n",
      "val, test error% for class  7  :  0.0 8.2\n",
      "val, test error% for class  8  :  10.0 13.0\n",
      "val, test error% for class  9  :  20.0 3.8\n",
      "total misclassified ex from imb classes:  43\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-06-20 01:22:31.088893\n",
      "250 15506 15756\n",
      "After augmentation, size of train_set:  3819  lake set:  15506\n",
      "ROC Curve for train set: ing epoch [ 72 ]  Training Acc:  0.9910971458496989\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.36443514021812 0.9910971458496989 21.240777004044503 0.64 2183.5780135281384 0.5839 653.3426055908203\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  80.0 84.1\n",
      "val, test error% for class  1  :  70.0 65.4\n",
      "val, test error% for class  2  :  80.0 71.8\n",
      "val, test error% for class  3  :  70.0 82.2\n",
      "val, test error% for class  4  :  40.0 47.4\n",
      "val, test error% for class  5  :  20.0 31.8\n",
      "val, test error% for class  6  :  0.0 11.9\n",
      "val, test error% for class  7  :  0.0 11.0\n",
      "val, test error% for class  8  :  0.0 5.6\n",
      "val, test error% for class  9  :  0.0 4.9\n",
      "total misclassified ex from imb classes:  36\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-06-20 01:33:52.274112\n",
      "250 15256 15506\n",
      "After augmentation, size of train_set:  4069  lake set:  15256\n",
      "ROC Curve for train set: ing epoch [ 74 ]  Training Acc:  0.9904153354632588\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.563908373034792 0.9904153354632588 17.29133477434516 0.61 1905.7668328508735 0.6304 717.8086395263672\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  80.0 67.8\n",
      "val, test error% for class  1  :  80.0 64.2\n",
      "val, test error% for class  2  :  70.0 68.4\n",
      "val, test error% for class  3  :  80.0 63.6\n",
      "val, test error% for class  4  :  20.0 43.3\n",
      "val, test error% for class  5  :  20.0 28.1\n",
      "val, test error% for class  6  :  0.0 16.5\n",
      "val, test error% for class  7  :  30.0 9.9\n",
      "val, test error% for class  8  :  10.0 4.8\n",
      "val, test error% for class  9  :  0.0 3.0\n",
      "total misclassified ex from imb classes:  39\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-06-20 01:46:17.481974\n",
      "250 15006 15256\n",
      "After augmentation, size of train_set:  4319  lake set:  15006\n",
      "ROC Curve for train set: ing epoch [ 72 ]  Training Acc:  0.9918962722852512\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.334129528317135 0.9918962722852512 17.878022275865078 0.58 1611.7445885580964 0.6597 747.0083289146423\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  60.0 63.6\n",
      "val, test error% for class  1  :  80.0 49.9\n",
      "val, test error% for class  2  :  70.0 68.1\n",
      "val, test error% for class  3  :  90.0 62.7\n",
      "val, test error% for class  4  :  50.0 44.1\n",
      "val, test error% for class  5  :  30.0 25.9\n",
      "val, test error% for class  6  :  10.0 6.9\n",
      "val, test error% for class  7  :  20.0 10.0\n",
      "val, test error% for class  8  :  10.0 5.8\n",
      "val, test error% for class  9  :  0.0 3.3\n",
      "total misclassified ex from imb classes:  42\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-06-20 01:59:11.175243\n",
      "250 14756 15006\n",
      "After augmentation, size of train_set:  4569  lake set:  14756\n",
      "ROC Curve for train set: ing epoch [ 81 ]  Training Acc:  0.9923396804552419\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.402085062654805 0.9923396804552419 14.03385517001152 0.68 1594.12716185709 0.6835 861.4486434459686\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  70.0 62.3\n",
      "val, test error% for class  1  :  60.0 40.2\n",
      "val, test error% for class  2  :  50.0 61.7\n",
      "val, test error% for class  3  :  70.0 63.7\n",
      "val, test error% for class  4  :  40.0 26.9\n",
      "val, test error% for class  5  :  20.0 32.7\n",
      "val, test error% for class  6  :  10.0 11.9\n",
      "val, test error% for class  7  :  0.0 9.2\n",
      "val, test error% for class  8  :  0.0 2.9\n",
      "val, test error% for class  9  :  0.0 5.0\n",
      "total misclassified ex from imb classes:  32\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-06-20 02:13:59.782873\n",
      "250 14506 14756\n",
      "After augmentation, size of train_set:  4819  lake set:  14506\n",
      "ROC Curve for train set: ing epoch [ 77 ]  Training Acc:  0.9904544511309439\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 18.03445039914368 0.99045445113094 14.638382891193032 0.69 1398.0263657253236 0.7009 893.2568829059601\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  60.0 58.2\n",
      "val, test error% for class  1  :  70.0 46.2\n",
      "val, test error% for class  2  :  60.0 54.6\n",
      "val, test error% for class  3  :  70.0 57.0\n",
      "val, test error% for class  4  :  10.0 33.2\n",
      "val, test error% for class  5  :  20.0 24.2\n",
      "val, test error% for class  6  :  10.0 8.8\n",
      "val, test error% for class  7  :  0.0 10.4\n",
      "val, test error% for class  8  :  0.0 2.9\n",
      "val, test error% for class  9  :  10.0 3.6\n",
      "total misclassified ex from imb classes:  31\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-06-20 02:29:20.216569\n",
      "250 14256 14506\n",
      "After augmentation, size of train_set:  5069  lake set:  14256\n",
      "ROC Curve for train set: ing epoch [ 78 ]  Training Acc:  0.9901361215229828\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 16.65276038005686 0.9901361215229828 16.70490389841143 0.65 1503.1723329448141 0.6925 935.3542416095734\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  80.0 61.7\n",
      "val, test error% for class  1  :  70.0 51.6\n",
      "val, test error% for class  2  :  70.0 49.8\n",
      "val, test error% for class  3  :  60.0 47.8\n",
      "val, test error% for class  4  :  30.0 37.2\n",
      "val, test error% for class  5  :  20.0 35.8\n",
      "val, test error% for class  6  :  0.0 8.4\n",
      "val, test error% for class  7  :  10.0 8.6\n",
      "val, test error% for class  8  :  0.0 3.5\n",
      "val, test error% for class  9  :  10.0 3.1\n",
      "total misclassified ex from imb classes:  35\n",
      "reinit AL with targeted miscls samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch4/snk170001/code/distil/distil/active_learning_strategies/glister.py:206: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_Val = torch.tensor(self.Y_Val,device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-06-20 02:45:22.481541\n",
      "250 14006 14256\n",
      "After augmentation, size of train_set:  5319  lake set:  14006\n",
      "ROC Curve for train set: ing epoch [ 76 ]  Training Acc:  0.9904117315284828\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 18.839684868115 0.9904117315284828 13.314054071903229 0.66 1290.6160087757744 0.7132 959.8735342025757\n",
      "val, test error% for class  0  :  70.0 65.9\n",
      "val, test error% for class  1  :  70.0 34.9\n",
      "val, test error% for class  2  :  50.0 59.4\n",
      "val, test error% for class  3  :  50.0 45.3\n",
      "val, test error% for class  4  :  30.0 26.7\n",
      "val, test error% for class  5  :  30.0 31.8\n",
      "val, test error% for class  6  :  0.0 8.0\n",
      "val, test error% for class  7  :  20.0 7.4\n",
      "val, test error% for class  8  :  10.0 2.4\n",
      "val, test error% for class  9  :  10.0 5.0\n",
      "[[98.8, 99.7, 95.1, 94.3, 77.1, 72.0, 52.7, 21.0, 9.6, 3.5, 62.38000000000001], [73.0, 93.4, 77.5, 79.5, 72.0, 40.3, 13.1, 17.1, 6.0, 2.5, 47.440000000000005], [61.0, 86.9, 76.0, 82.1, 55.0, 33.5, 18.1, 8.2, 13.0, 3.8, 43.760000000000005], [84.1, 65.4, 71.8, 82.2, 47.4, 31.8, 11.9, 11.0, 5.6, 4.9, 41.61], [67.8, 64.2, 68.4, 63.6, 43.3, 28.1, 16.5, 9.9, 4.8, 3.0, 36.96], [63.6, 49.9, 68.1, 62.7, 44.1, 25.9, 6.9, 10.0, 5.8, 3.3, 34.03], [62.3, 40.2, 61.7, 63.7, 26.9, 32.7, 11.9, 9.2, 2.9, 5.0, 31.649999999999995], [58.2, 46.2, 54.6, 57.0, 33.2, 24.2, 8.8, 10.4, 2.9, 3.6, 29.909999999999997], [61.7, 51.6, 49.8, 47.8, 37.2, 35.8, 8.4, 8.6, 3.5, 3.1, 30.750000000000007], [65.9, 34.9, 59.4, 45.3, 26.7, 31.8, 8.0, 7.4, 2.4, 5.0, 28.679999999999996]]\n"
     ]
    }
   ],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"glister-tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc91145",
   "metadata": {},
   "source": [
    "# GradMatch-Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf2ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradmatch_tst, gradmatch_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"AL\",\"gradmatch-tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e21230",
   "metadata": {},
   "source": [
    "# Coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "693e1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL coreset\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  3069 Val size:  100 Lake size:  16256\n",
      "selected classes are:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Saving results to:  active_learning_results/cifar10/longtail/coreset/250/1\n",
      "cifar10_longtail_AL_10_coreset_budget:250_epochs:10_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ResNet18_0.01_{0: 3, 1: 6, 2: 12, 3: 24, 4: 48, 5: 96, 6: 192, 7: 384, 8: 768, 9: 1536}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 98.8\n",
      "val, test error% for class  1  :  100.0 99.7\n",
      "val, test error% for class  2  :  100.0 95.1\n",
      "val, test error% for class  3  :  90.0 94.3\n",
      "val, test error% for class  4  :  90.0 77.1\n",
      "val, test error% for class  5  :  80.0 72.0\n",
      "val, test error% for class  6  :  90.0 52.7\n",
      "val, test error% for class  7  :  80.0 21.0\n",
      "val, test error% for class  8  :  50.0 9.6\n",
      "val, test error% for class  9  :  10.0 3.5\n",
      "250  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-06-19 16:39:07.016098\n",
      "250 16006 16256\n",
      "After augmentation, size of train_set:  3319  lake set:  16006\n",
      "ROC Curve for train set: ing epoch [ 62 ]  Training Acc:  0.9918650195842121\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.649609484571556 0.9918650195842121 26.49666702351533 0.51 3042.4629711434245 0.4767 479.12281703948975\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  100.0 98.0\n",
      "val, test error% for class  1  :  100.0 95.2\n",
      "val, test error% for class  2  :  80.0 83.5\n",
      "val, test error% for class  3  :  90.0 90.9\n",
      "val, test error% for class  4  :  70.0 71.3\n",
      "val, test error% for class  5  :  40.0 47.2\n",
      "val, test error% for class  6  :  0.0 14.4\n",
      "val, test error% for class  7  :  10.0 12.3\n",
      "val, test error% for class  8  :  0.0 6.0\n",
      "val, test error% for class  9  :  0.0 4.5\n",
      "250  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-06-19 16:47:41.993581\n",
      "250 15756 16006\n",
      "After augmentation, size of train_set:  3569  lake set:  15756\n",
      "ROC Curve for train set: ing epoch [ 62 ]  Training Acc:  0.9915942841131969\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.545513575489167 0.9915942841131969 22.504940882325172 0.53 2533.675104610622 0.5264 510.42519664764404\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  100.0 84.0\n",
      "val, test error% for class  1  :  100.0 95.5\n",
      "val, test error% for class  2  :  80.0 74.2\n",
      "val, test error% for class  3  :  60.0 78.6\n",
      "val, test error% for class  4  :  70.0 65.6\n",
      "val, test error% for class  5  :  30.0 37.2\n",
      "val, test error% for class  6  :  10.0 14.2\n",
      "val, test error% for class  7  :  0.0 14.5\n",
      "val, test error% for class  8  :  10.0 6.4\n",
      "val, test error% for class  9  :  10.0 3.4\n",
      "250  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-06-19 16:56:47.974129\n",
      "250 15506 15756\n",
      "After augmentation, size of train_set:  3819  lake set:  15506\n",
      "ROC Curve for train set: ing epoch [ 66 ]  Training Acc:  0.9950248756218906\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.182381061342312 0.9950248756218906 24.064301954582334 0.52 2495.7530209235847 0.5437 576.5457048416138\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  90.0 86.5\n",
      "val, test error% for class  1  :  90.0 90.0\n",
      "val, test error% for class  2  :  70.0 79.3\n",
      "val, test error% for class  3  :  90.0 75.4\n",
      "val, test error% for class  4  :  60.0 52.7\n",
      "val, test error% for class  5  :  50.0 42.3\n",
      "val, test error% for class  6  :  0.0 10.9\n",
      "val, test error% for class  7  :  20.0 10.8\n",
      "val, test error% for class  8  :  0.0 5.3\n",
      "val, test error% for class  9  :  10.0 3.1\n",
      "250  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-06-19 17:06:59.492065\n",
      "250 15256 15506\n",
      "After augmentation, size of train_set:  4069  lake set:  15256\n",
      "ROC Curve for train set: ing epoch [ 66 ]  Training Acc:  0.9904153354632588\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.896257926593535 0.9904153354632588 16.16085009276867 0.58 2033.2084426078945 0.5806 615.3895936012268\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  80.0 85.3\n",
      "val, test error% for class  1  :  80.0 58.7\n",
      "val, test error% for class  2  :  60.0 80.5\n",
      "val, test error% for class  3  :  60.0 70.2\n",
      "val, test error% for class  4  :  40.0 54.0\n",
      "val, test error% for class  5  :  60.0 34.2\n",
      "val, test error% for class  6  :  10.0 15.5\n",
      "val, test error% for class  7  :  10.0 11.2\n",
      "val, test error% for class  8  :  0.0 6.0\n",
      "val, test error% for class  9  :  20.0 3.8\n",
      "250  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-06-19 17:17:50.265418\n",
      "250 15006 15256\n",
      "After augmentation, size of train_set:  4319  lake set:  15006\n",
      "ROC Curve for train set: ing epoch [ 65 ]  Training Acc:  0.9907385968974299\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.654139573409338 0.9907385968974299 13.026507759466767 0.59 1848.314993460197 0.6239 647.5813148021698\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  60.0 70.6\n",
      "val, test error% for class  1  :  70.0 54.8\n",
      "val, test error% for class  2  :  80.0 80.0\n",
      "val, test error% for class  3  :  100.0 66.1\n",
      "val, test error% for class  4  :  30.0 26.3\n",
      "val, test error% for class  5  :  50.0 31.8\n",
      "val, test error% for class  6  :  0.0 24.6\n",
      "val, test error% for class  7  :  10.0 11.4\n",
      "val, test error% for class  8  :  10.0 8.2\n",
      "val, test error% for class  9  :  0.0 2.3\n",
      "250  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-06-19 17:29:12.911244\n",
      "250 14756 15006\n",
      "After augmentation, size of train_set:  4569  lake set:  14756\n",
      "ROC Curve for train set: ing epoch [ 66 ]  Training Acc:  0.9905887502735828\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 16.10585281989188 0.9905887502735828 14.522803192958236 0.67 1728.194627174642 0.6416 704.3129422664642\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  50.0 72.6\n",
      "val, test error% for class  1  :  60.0 45.4\n",
      "val, test error% for class  2  :  60.0 56.7\n",
      "val, test error% for class  3  :  70.0 68.1\n",
      "val, test error% for class  4  :  50.0 51.9\n",
      "val, test error% for class  5  :  30.0 36.7\n",
      "val, test error% for class  6  :  0.0 10.2\n",
      "val, test error% for class  7  :  10.0 8.7\n",
      "val, test error% for class  8  :  0.0 3.9\n",
      "val, test error% for class  9  :  0.0 4.2\n",
      "250  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-06-19 17:41:32.343094\n",
      "250 14506 14756\n",
      "After augmentation, size of train_set:  4819  lake set:  14506\n",
      "ROC Curve for train set: ing epoch [ 69 ]  Training Acc:  0.9910769869267483\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 18.677513828253723 0.9910769869267483 17.37648705812171 0.62 1795.9575828816742 0.6414 771.9371373653412\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  80.0 68.3\n",
      "val, test error% for class  1  :  80.0 61.9\n",
      "val, test error% for class  2  :  80.0 63.1\n",
      "val, test error% for class  3  :  70.0 67.2\n",
      "val, test error% for class  4  :  20.0 46.4\n",
      "val, test error% for class  5  :  30.0 21.4\n",
      "val, test error% for class  6  :  0.0 10.0\n",
      "val, test error% for class  7  :  10.0 10.8\n",
      "val, test error% for class  8  :  10.0 7.9\n",
      "val, test error% for class  9  :  0.0 1.6\n",
      "250  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-06-19 17:54:58.416651\n",
      "250 14256 14506\n",
      "After augmentation, size of train_set:  5069  lake set:  14256\n",
      "ROC Curve for train set: ing epoch [ 69 ]  Training Acc:  0.9938843953442493\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.579013110444066 0.9938843953442493 12.139613969251513 0.67 1566.2349893040955 0.67 813.978346824646\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  60.0 62.7\n",
      "val, test error% for class  1  :  80.0 52.8\n",
      "val, test error% for class  2  :  60.0 64.1\n",
      "val, test error% for class  3  :  60.0 62.4\n",
      "val, test error% for class  4  :  20.0 23.5\n",
      "val, test error% for class  5  :  30.0 33.6\n",
      "val, test error% for class  6  :  0.0 11.6\n",
      "val, test error% for class  7  :  10.0 13.9\n",
      "val, test error% for class  8  :  0.0 2.5\n",
      "val, test error% for class  9  :  10.0 2.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-06-19 18:09:07.440750\n",
      "250 14006 14256\n",
      "After augmentation, size of train_set:  5319  lake set:  14006\n",
      "ROC Curve for train set: ing epoch [ 73 ]  Training Acc:  0.9904117315284828\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 17.024865324696293 0.9904117315284828 12.562836399301887 0.68 1379.333356671501 0.6862 910.4367280006409\n",
      "val, test error% for class  0  :  80.0 59.5\n",
      "val, test error% for class  1  :  50.0 43.7\n",
      "val, test error% for class  2  :  40.0 58.6\n",
      "val, test error% for class  3  :  80.0 61.7\n",
      "val, test error% for class  4  :  30.0 37.7\n",
      "val, test error% for class  5  :  20.0 21.6\n",
      "val, test error% for class  6  :  10.0 13.3\n",
      "val, test error% for class  7  :  0.0 9.5\n",
      "val, test error% for class  8  :  10.0 4.9\n",
      "val, test error% for class  9  :  0.0 3.3\n",
      "[[98.8, 99.7, 95.1, 94.3, 77.1, 72.0, 52.7, 21.0, 9.6, 3.5, 62.38000000000001], [98.0, 95.2, 83.5, 90.9, 71.3, 47.2, 14.4, 12.3, 6.0, 4.5, 52.33], [84.0, 95.5, 74.2, 78.6, 65.6, 37.2, 14.2, 14.5, 6.4, 3.4, 47.35999999999999], [86.5, 90.0, 79.3, 75.4, 52.7, 42.3, 10.9, 10.8, 5.3, 3.1, 45.63000000000001], [85.3, 58.7, 80.5, 70.2, 54.0, 34.2, 15.5, 11.2, 6.0, 3.8, 41.94], [70.6, 54.8, 80.0, 66.1, 26.3, 31.8, 24.6, 11.4, 8.2, 2.3, 37.61], [72.6, 45.4, 56.7, 68.1, 51.9, 36.7, 10.2, 8.7, 3.9, 4.2, 35.83999999999999], [68.3, 61.9, 63.1, 67.2, 46.4, 21.4, 10.0, 10.8, 7.9, 1.6, 35.86], [62.7, 52.8, 64.1, 62.4, 23.5, 33.6, 11.6, 13.9, 2.5, 2.9, 33.0], [59.5, 43.7, 58.6, 61.7, 37.7, 21.6, 13.3, 9.5, 4.9, 3.3, 31.380000000000003]]\n"
     ]
    }
   ],
   "source": [
    "coreset_tst, coreset_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"coreset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83e3bb",
   "metadata": {},
   "source": [
    "# Least Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5f37fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL leastconf\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  3069 Val size:  100 Lake size:  16256\n",
      "selected classes are:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Saving results to:  active_learning_results/cifar10/longtail/leastconf/250/1\n",
      "cifar10_longtail_AL_10_leastconf_budget:250_epochs:10_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ResNet18_0.01_{0: 3, 1: 6, 2: 12, 3: 24, 4: 48, 5: 96, 6: 192, 7: 384, 8: 768, 9: 1536}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 98.8\n",
      "val, test error% for class  1  :  100.0 99.7\n",
      "val, test error% for class  2  :  90.0 95.1\n",
      "val, test error% for class  3  :  90.0 94.3\n",
      "val, test error% for class  4  :  90.0 77.1\n",
      "val, test error% for class  5  :  90.0 72.0\n",
      "val, test error% for class  6  :  80.0 52.7\n",
      "val, test error% for class  7  :  80.0 21.0\n",
      "val, test error% for class  8  :  30.0 9.6\n",
      "val, test error% for class  9  :  0.0 3.5\n",
      "250  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-06-19 18:24:53.664637\n",
      "250 16006 16256\n",
      "After augmentation, size of train_set:  3319  lake set:  16006\n",
      "ROC Curve for train set: ing epoch [ 71 ]  Training Acc:  0.9906598373003916\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.537362233131716 0.9906598373003916 24.497772300615907 0.57 3203.1958420574665 0.5115 552.4667928218842\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  90.0 93.1\n",
      "val, test error% for class  1  :  100.0 98.4\n",
      "val, test error% for class  2  :  60.0 75.7\n",
      "val, test error% for class  3  :  90.0 89.2\n",
      "val, test error% for class  4  :  30.0 56.3\n",
      "val, test error% for class  5  :  40.0 35.0\n",
      "val, test error% for class  6  :  10.0 15.7\n",
      "val, test error% for class  7  :  0.0 15.0\n",
      "val, test error% for class  8  :  0.0 5.0\n",
      "val, test error% for class  9  :  10.0 5.1\n",
      "250  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-06-19 18:34:29.384304\n",
      "250 15756 16006\n",
      "After augmentation, size of train_set:  3569  lake set:  15756\n",
      "ROC Curve for train set: ing epoch [ 69 ]  Training Acc:  0.9907537125245167\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.082330282632029 0.9907537125245167 23.09284421056509 0.58 2569.755517143756 0.5617 569.5340943336487\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  60.0 72.6\n",
      "val, test error% for class  1  :  100.0 94.4\n",
      "val, test error% for class  2  :  50.0 66.4\n",
      "val, test error% for class  3  :  90.0 82.1\n",
      "val, test error% for class  4  :  50.0 45.8\n",
      "val, test error% for class  5  :  50.0 35.8\n",
      "val, test error% for class  6  :  0.0 13.8\n",
      "val, test error% for class  7  :  0.0 15.1\n",
      "val, test error% for class  8  :  10.0 9.2\n",
      "val, test error% for class  9  :  10.0 3.1\n",
      "250  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-06-19 18:44:21.561700\n",
      "250 15506 15756\n",
      "After augmentation, size of train_set:  3819  lake set:  15506\n",
      "ROC Curve for train set: ing epoch [ 73 ]  Training Acc:  0.9913589945011784\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 12.357461716455873 0.9913589945011784 22.809295061975718 0.56 2662.161752525717 0.5753 637.6447262763977\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  90.0 81.0\n",
      "val, test error% for class  1  :  100.0 96.2\n",
      "val, test error% for class  2  :  60.0 69.9\n",
      "val, test error% for class  3  :  70.0 61.3\n",
      "val, test error% for class  4  :  30.0 33.3\n",
      "val, test error% for class  5  :  50.0 39.9\n",
      "val, test error% for class  6  :  20.0 18.6\n",
      "val, test error% for class  7  :  20.0 13.6\n",
      "val, test error% for class  8  :  0.0 7.7\n",
      "val, test error% for class  9  :  0.0 3.2\n",
      "250  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-06-19 18:55:22.070660\n",
      "250 15256 15506\n",
      "After augmentation, size of train_set:  4069  lake set:  15256\n",
      "ROC Curve for train set: ing epoch [ 77 ]  Training Acc:  0.9938559842713197\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.492237447644584 0.9938559842713197 21.563359081745148 0.61 2491.6892914716154 0.5895 728.9247512817383\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  80.0 73.3\n",
      "val, test error% for class  1  :  100.0 97.6\n",
      "val, test error% for class  2  :  60.0 61.8\n",
      "val, test error% for class  3  :  60.0 70.6\n",
      "val, test error% for class  4  :  30.0 40.1\n",
      "val, test error% for class  5  :  30.0 35.1\n",
      "val, test error% for class  6  :  0.0 17.1\n",
      "val, test error% for class  7  :  10.0 7.5\n",
      "val, test error% for class  8  :  0.0 5.5\n",
      "val, test error% for class  9  :  20.0 1.9\n",
      "250  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-06-19 19:07:53.248439\n",
      "250 15006 15256\n",
      "After augmentation, size of train_set:  4319  lake set:  15006\n",
      "ROC Curve for train set: ing epoch [ 76 ]  Training Acc:  0.9912016670525584\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 16.14323071057879 0.9912016670525584 21.58037194609642 0.63 2278.1363089014776 0.6184 762.4351711273193\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  60.0 73.2\n",
      "val, test error% for class  1  :  100.0 90.3\n",
      "val, test error% for class  2  :  70.0 72.2\n",
      "val, test error% for class  3  :  50.0 52.3\n",
      "val, test error% for class  4  :  10.0 28.8\n",
      "val, test error% for class  5  :  30.0 32.9\n",
      "val, test error% for class  6  :  10.0 14.9\n",
      "val, test error% for class  7  :  10.0 6.7\n",
      "val, test error% for class  8  :  20.0 8.2\n",
      "val, test error% for class  9  :  10.0 2.1\n",
      "250  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-06-19 19:20:58.076184\n",
      "250 14756 15006\n",
      "After augmentation, size of train_set:  4569  lake set:  14756\n",
      "ROC Curve for train set: ing epoch [ 79 ]  Training Acc:  0.9923396804552419\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.371892295013822 0.9923396804552419 21.55426245741546 0.61 2225.195302415639 0.624 799.653829574585\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  70.0 67.1\n",
      "val, test error% for class  1  :  100.0 92.3\n",
      "val, test error% for class  2  :  60.0 65.8\n",
      "val, test error% for class  3  :  60.0 55.5\n",
      "val, test error% for class  4  :  40.0 33.1\n",
      "val, test error% for class  5  :  20.0 38.0\n",
      "val, test error% for class  6  :  0.0 8.0\n",
      "val, test error% for class  7  :  30.0 8.6\n",
      "val, test error% for class  8  :  0.0 5.0\n",
      "val, test error% for class  9  :  10.0 2.6\n",
      "250  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-06-19 19:34:39.782830\n",
      "250 14506 14756\n",
      "After augmentation, size of train_set:  4819  lake set:  14506\n",
      "ROC Curve for train set: ing epoch [ 80 ]  Training Acc:  0.9919070346544926\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.700615641530021 0.9919070346544926 17.184786546975374 0.66 1806.8450457882136 0.6623 885.2140576839447\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  50.0 57.0\n",
      "val, test error% for class  1  :  100.0 86.6\n",
      "val, test error% for class  2  :  50.0 49.7\n",
      "val, test error% for class  3  :  60.0 60.3\n",
      "val, test error% for class  4  :  30.0 22.3\n",
      "val, test error% for class  5  :  10.0 30.3\n",
      "val, test error% for class  6  :  10.0 16.8\n",
      "val, test error% for class  7  :  20.0 7.3\n",
      "val, test error% for class  8  :  0.0 3.2\n",
      "val, test error% for class  9  :  10.0 4.2\n",
      "250  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-06-19 19:49:48.052907\n",
      "250 14256 14506\n",
      "After augmentation, size of train_set:  5069  lake set:  14256\n",
      "ROC Curve for train set: ing epoch [ 83 ]  Training Acc:  0.9938843953442493\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.713773017792846 0.9938843953442493 15.526021096855402 0.68 1573.6775231501088 0.6927 983.5817279815674\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  50.0 59.0\n",
      "val, test error% for class  1  :  90.0 73.7\n",
      "val, test error% for class  2  :  50.0 49.3\n",
      "val, test error% for class  3  :  50.0 44.6\n",
      "val, test error% for class  4  :  20.0 27.3\n",
      "val, test error% for class  5  :  10.0 21.6\n",
      "val, test error% for class  6  :  10.0 17.2\n",
      "val, test error% for class  7  :  20.0 6.3\n",
      "val, test error% for class  8  :  0.0 5.2\n",
      "val, test error% for class  9  :  20.0 3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-06-19 20:06:33.594653\n",
      "250 14006 14256\n",
      "After augmentation, size of train_set:  5319  lake set:  14006\n",
      "ROC Curve for train set: ing epoch [ 83 ]  Training Acc:  0.9902237262643354\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 19.334926577532315 0.9902237262643354 18.395845658611506 0.67 1787.3371587841684 0.6837 1017.254768371582\n",
      "val, test error% for class  0  :  60.0 53.0\n",
      "val, test error% for class  1  :  80.0 77.8\n",
      "val, test error% for class  2  :  70.0 68.3\n",
      "val, test error% for class  3  :  50.0 44.9\n",
      "val, test error% for class  4  :  20.0 20.9\n",
      "val, test error% for class  5  :  10.0 27.9\n",
      "val, test error% for class  6  :  0.0 7.9\n",
      "val, test error% for class  7  :  20.0 7.6\n",
      "val, test error% for class  8  :  0.0 4.4\n",
      "val, test error% for class  9  :  20.0 3.6\n",
      "[[98.8, 99.7, 95.1, 94.3, 77.1, 72.0, 52.7, 21.0, 9.6, 3.5, 62.38000000000001], [93.1, 98.4, 75.7, 89.2, 56.3, 35.0, 15.7, 15.0, 5.0, 5.1, 48.85], [72.6, 94.4, 66.4, 82.1, 45.8, 35.8, 13.8, 15.1, 9.2, 3.1, 43.830000000000005], [81.0, 96.2, 69.9, 61.3, 33.3, 39.9, 18.6, 13.6, 7.7, 3.2, 42.47], [73.3, 97.6, 61.8, 70.6, 40.1, 35.1, 17.1, 7.5, 5.5, 1.9, 41.05], [73.2, 90.3, 72.2, 52.3, 28.8, 32.9, 14.9, 6.7, 8.2, 2.1, 38.16], [67.1, 92.3, 65.8, 55.5, 33.1, 38.0, 8.0, 8.6, 5.0, 2.6, 37.60000000000001], [57.0, 86.6, 49.7, 60.3, 22.3, 30.3, 16.8, 7.3, 3.2, 4.2, 33.77], [59.0, 73.7, 49.3, 44.6, 27.3, 21.6, 17.2, 6.3, 5.2, 3.1, 30.73], [53.0, 77.8, 68.3, 44.9, 20.9, 27.9, 7.9, 7.6, 4.4, 3.6, 31.630000000000003]]\n"
     ]
    }
   ],
   "source": [
    "leastconf_tst, leastconf_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"leastconf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a515b45",
   "metadata": {},
   "source": [
    "# Margin Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb69cde1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL margin\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 Custom dataset stats: Train size:  3069 Val size:  100 Lake size:  16256\n",
      "selected classes are:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Saving results to:  active_learning_results/cifar10/longtail/margin/250/1\n",
      "cifar10_longtail_AL_10_margin_budget:250_epochs:10_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/cifar10_ResNet18_0.01_{0: 3, 1: 6, 2: 12, 3: 24, 4: 48, 5: 96, 6: 192, 7: 384, 8: 768, 9: 1536}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 98.8\n",
      "val, test error% for class  1  :  100.0 99.7\n",
      "val, test error% for class  2  :  90.0 95.1\n",
      "val, test error% for class  3  :  100.0 94.3\n",
      "val, test error% for class  4  :  90.0 77.1\n",
      "val, test error% for class  5  :  90.0 72.0\n",
      "val, test error% for class  6  :  90.0 52.7\n",
      "val, test error% for class  7  :  90.0 21.0\n",
      "val, test error% for class  8  :  40.0 9.6\n",
      "val, test error% for class  9  :  0.0 3.5\n",
      "250  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-06-19 20:24:06.098859\n",
      "250 16006 16256\n",
      "After augmentation, size of train_set:  3319  lake set:  16006\n",
      "ROC Curve for train set: ing epoch [ 65 ]  Training Acc:  0.9915637240132577\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.10469672950785 0.991563724013257 28.49376222677529 0.48 3101.0371272936463 0.512 500.9528720378876\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  100.0 95.5\n",
      "val, test error% for class  1  :  100.0 97.8\n",
      "val, test error% for class  2  :  90.0 86.3\n",
      "val, test error% for class  3  :  80.0 85.4\n",
      "val, test error% for class  4  :  40.0 41.1\n",
      "val, test error% for class  5  :  70.0 43.5\n",
      "val, test error% for class  6  :  20.0 17.2\n",
      "val, test error% for class  7  :  0.0 10.1\n",
      "val, test error% for class  8  :  10.0 7.0\n",
      "val, test error% for class  9  :  10.0 4.1\n",
      "250  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-06-19 20:32:49.699616\n",
      "250 15756 16006\n",
      "After augmentation, size of train_set:  3569  lake set:  15756\n",
      "ROC Curve for train set: ing epoch [ 68 ]  Training Acc:  0.9935556178201177\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 10.027643257053569 0.9935556178201177 22.940692064352334 0.56 2409.4579555708915 0.553 565.1597208976746\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  90.0 89.0\n",
      "val, test error% for class  1  :  100.0 94.0\n",
      "val, test error% for class  2  :  70.0 70.3\n",
      "val, test error% for class  3  :  60.0 82.2\n",
      "val, test error% for class  4  :  40.0 37.2\n",
      "val, test error% for class  5  :  50.0 39.8\n",
      "val, test error% for class  6  :  10.0 13.2\n",
      "val, test error% for class  7  :  10.0 12.9\n",
      "val, test error% for class  8  :  10.0 6.0\n",
      "val, test error% for class  9  :  0.0 2.4\n",
      "250  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-06-19 20:42:37.704922\n",
      "250 15506 15756\n",
      "After augmentation, size of train_set:  3819  lake set:  15506\n",
      "ROC Curve for train set: ing epoch [ 72 ]  Training Acc:  0.9924063891070961\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 11.49085779422603 0.9924063891070961 24.920436412096024 0.57 2424.5143575891852 0.5785 642.2312219142914\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  90.0 91.3\n",
      "val, test error% for class  1  :  100.0 90.6\n",
      "val, test error% for class  2  :  70.0 61.0\n",
      "val, test error% for class  3  :  70.0 76.8\n",
      "val, test error% for class  4  :  30.0 39.0\n",
      "val, test error% for class  5  :  30.0 26.0\n",
      "val, test error% for class  6  :  10.0 18.1\n",
      "val, test error% for class  7  :  10.0 9.3\n",
      "val, test error% for class  8  :  10.0 5.7\n",
      "val, test error% for class  9  :  10.0 3.7\n",
      "250  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-06-19 20:53:42.848141\n",
      "250 15256 15506\n",
      "After augmentation, size of train_set:  4069  lake set:  15256\n",
      "ROC Curve for train set: ing epoch [ 76 ]  Training Acc:  0.9911526173507004\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 13.089932841365226 0.9911526173507004 21.249044531956315 0.63 2227.442701226566 0.5926 707.8933117389679\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  80.0 82.0\n",
      "val, test error% for class  1  :  100.0 94.1\n",
      "val, test error% for class  2  :  60.0 72.2\n",
      "val, test error% for class  3  :  60.0 66.6\n",
      "val, test error% for class  4  :  20.0 28.0\n",
      "val, test error% for class  5  :  30.0 33.6\n",
      "val, test error% for class  6  :  0.0 11.0\n",
      "val, test error% for class  7  :  20.0 11.7\n",
      "val, test error% for class  8  :  0.0 4.1\n",
      "val, test error% for class  9  :  0.0 4.1\n",
      "250  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-06-19 21:05:53.386809\n",
      "250 15006 15256\n",
      "After augmentation, size of train_set:  4319  lake set:  15006\n",
      "ROC Curve for train set: ing epoch [ 75 ]  Training Acc:  0.9916647372076871\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.519900871207938 0.991664737207687 19.07446128875017 0.62 2130.713454861194 0.6165 727.870936870575\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  90.0 85.7\n",
      "val, test error% for class  1  :  100.0 77.9\n",
      "val, test error% for class  2  :  60.0 56.7\n",
      "val, test error% for class  3  :  80.0 72.7\n",
      "val, test error% for class  4  :  20.0 33.0\n",
      "val, test error% for class  5  :  10.0 26.4\n",
      "val, test error% for class  6  :  0.0 16.4\n",
      "val, test error% for class  7  :  10.0 7.4\n",
      "val, test error% for class  8  :  0.0 2.9\n",
      "val, test error% for class  9  :  10.0 4.4\n",
      "250  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-06-19 21:18:23.492620\n",
      "250 14756 15006\n",
      "After augmentation, size of train_set:  4569  lake set:  14756\n",
      "ROC Curve for train set: ing epoch [ 76 ]  Training Acc:  0.9929962792733644\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.728664179536281 0.992996279273364 18.304745070636272 0.6 1774.5879240520298 0.6495 801.5952689647675\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  70.0 71.8\n",
      "val, test error% for class  1  :  100.0 71.1\n",
      "val, test error% for class  2  :  60.0 56.4\n",
      "val, test error% for class  3  :  70.0 50.4\n",
      "val, test error% for class  4  :  40.0 42.9\n",
      "val, test error% for class  5  :  10.0 31.7\n",
      "val, test error% for class  6  :  10.0 9.7\n",
      "val, test error% for class  7  :  20.0 7.0\n",
      "val, test error% for class  8  :  10.0 6.9\n",
      "val, test error% for class  9  :  10.0 2.6\n",
      "250  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-06-19 21:32:06.770090\n",
      "250 14506 14756\n",
      "After augmentation, size of train_set:  4819  lake set:  14506\n",
      "ROC Curve for train set: ing epoch [ 76 ]  Training Acc:  0.9931521062461092\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 14.736688364107977 0.9931521062461092 19.40497132204473 0.64 1773.5778978122398 0.6592 845.5350482463837\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  70.0 71.4\n",
      "val, test error% for class  1  :  90.0 73.5\n",
      "val, test error% for class  2  :  70.0 60.4\n",
      "val, test error% for class  3  :  80.0 60.1\n",
      "val, test error% for class  4  :  10.0 22.8\n",
      "val, test error% for class  5  :  0.0 26.3\n",
      "val, test error% for class  6  :  0.0 10.3\n",
      "val, test error% for class  7  :  20.0 8.1\n",
      "val, test error% for class  8  :  10.0 5.3\n",
      "val, test error% for class  9  :  10.0 2.6\n",
      "250  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-06-19 21:46:34.056977\n",
      "250 14256 14506\n",
      "After augmentation, size of train_set:  5069  lake set:  14256\n",
      "ROC Curve for train set: ing epoch [ 81 ]  Training Acc:  0.9915170645097653\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 16.330947344114975 0.9915170645097653 16.785376807674766 0.65 2011.7103018027265 0.6369 935.6869575977325\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  80.0 76.9\n",
      "val, test error% for class  1  :  80.0 73.3\n",
      "val, test error% for class  2  :  70.0 67.5\n",
      "val, test error% for class  3  :  70.0 70.3\n",
      "val, test error% for class  4  :  20.0 27.8\n",
      "val, test error% for class  5  :  20.0 21.8\n",
      "val, test error% for class  6  :  0.0 7.0\n",
      "val, test error% for class  7  :  10.0 12.6\n",
      "val, test error% for class  8  :  0.0 2.3\n",
      "val, test error% for class  9  :  0.0 3.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-06-19 22:02:31.548147\n",
      "250 14006 14256\n",
      "After augmentation, size of train_set:  5319  lake set:  14006\n",
      "ROC Curve for train set: ing epoch [ 83 ]  Training Acc:  0.9926677946982515\n",
      "\n",
      "ROC Curve for validation set: \n",
      "\n",
      "ROC Curve for test set: \n",
      "\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 15.4223115209752 0.9926677946982515 17.397684478200972 0.64 1666.3359357242007 0.6781 1029.2016246318817\n",
      "val, test error% for class  0  :  70.0 60.1\n",
      "val, test error% for class  1  :  70.0 57.3\n",
      "val, test error% for class  2  :  80.0 72.4\n",
      "val, test error% for class  3  :  70.0 55.5\n",
      "val, test error% for class  4  :  30.0 33.5\n",
      "val, test error% for class  5  :  30.0 22.7\n",
      "val, test error% for class  6  :  0.0 6.4\n",
      "val, test error% for class  7  :  10.0 7.2\n",
      "val, test error% for class  8  :  0.0 5.3\n",
      "val, test error% for class  9  :  0.0 1.5\n",
      "[[98.8, 99.7, 95.1, 94.3, 77.1, 72.0, 52.7, 21.0, 9.6, 3.5, 62.38000000000001], [95.5, 97.8, 86.3, 85.4, 41.1, 43.5, 17.2, 10.1, 7.0, 4.1, 48.800000000000004], [89.0, 94.0, 70.3, 82.2, 37.2, 39.8, 13.2, 12.9, 6.0, 2.4, 44.699999999999996], [91.3, 90.6, 61.0, 76.8, 39.0, 26.0, 18.1, 9.3, 5.7, 3.7, 42.15], [82.0, 94.1, 72.2, 66.6, 28.0, 33.6, 11.0, 11.7, 4.1, 4.1, 40.74], [85.7, 77.9, 56.7, 72.7, 33.0, 26.4, 16.4, 7.4, 2.9, 4.4, 38.34999999999999], [71.8, 71.1, 56.4, 50.4, 42.9, 31.7, 9.7, 7.0, 6.9, 2.6, 35.05], [71.4, 73.5, 60.4, 60.1, 22.8, 26.3, 10.3, 8.1, 5.3, 2.6, 34.08000000000001], [76.9, 73.3, 67.5, 70.3, 27.8, 21.8, 7.0, 12.6, 2.3, 3.6, 36.31000000000001], [60.1, 57.3, 72.4, 55.5, 33.5, 22.7, 6.4, 7.2, 5.3, 1.5, 32.19]]\n"
     ]
    }
   ],
   "source": [
    "margin_tst, margin_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"margin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da748bb5",
   "metadata": {},
   "source": [
    "# GCMI+DIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a5f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gcmidiv_tst, gcmidiv_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'div-gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d37b4",
   "metadata": {},
   "source": [
    "# GCMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464fefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gcmi_tst, gcmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12daf4",
   "metadata": {},
   "source": [
    "# LOGDETMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1376261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdetmi\n",
      "Breast-density Custom dataset stats: Train size:  241 Val size:  40 Lake size:  2139\n",
      "selected classes are:  [0, 3]\n",
      "Saving results to:  SMI_active_learning_results_woVal/breast_density/classimb/logdetmi/50/1\n",
      "breast_density_classimb_SIM_2_logdetmi_budget:50_epochs:20_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/breast_density_MobileNetV2_0.01_{0: 35, 3: 40}_{1: 93, 2: 73}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  90.0 89.23\n",
      "val, test error% for class  1  :  60.0 46.15\n",
      "val, test error% for class  2  :  80.0 72.64\n",
      "val, test error% for class  3  :  80.0 86.73\n",
      "total misclassified ex from imb classes:  31\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2139, 5124])\n",
      "val minibatch gradients shape  torch.Size([31, 5124])\n",
      "kernel compute time:  0.8748178482055664\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  31  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 1., 2., 2., 2., 2., 2., 0., 1., 1., 1., 1., 2., 2., 0., 1., 2., 1.,\n",
      "        2., 2., 1., 2., 1., 1., 1., 2., 0., 2., 1., 2., 1., 2., 2., 2., 1., 1.,\n",
      "        1., 2., 2., 0., 1., 3., 2., 1., 1., 0., 2., 2., 0., 2.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 0., 1., 2., 1., 0., 3., 2., 0., 3., 3., 3., 0., 0., 2., 3.,\n",
      "        2., 1., 2., 1., 1., 3., 1., 0., 0., 1., 3., 2., 3., 2., 1., 2., 3., 3.,\n",
      "        0., 2., 0., 0., 2., 3., 0., 1., 2., 1., 1., 0., 3., 1.])\n",
      "50 2089 2139\n",
      "After augmentation, size of train_set:  291  lake set:  2089\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 28.880467474460602 0.5498281786941581 3.506541132926941 0.675 67.58897793292999 0.49921752738654146 1955.1519181728363\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  30.0 50.77\n",
      "val, test error% for class  1  :  30.0 46.15\n",
      "val, test error% for class  2  :  20.0 44.78\n",
      "val, test error% for class  3  :  50.0 68.14\n",
      "total misclassified ex from imb classes:  13\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2089, 5124])\n",
      "val minibatch gradients shape  torch.Size([13, 5124])\n",
      "kernel compute time:  0.8339412212371826\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  13  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 2., 2., 2., 1., 2., 0., 2., 0., 1., 3., 0., 1., 2., 1., 3.,\n",
      "        2., 1., 2., 2., 1., 1., 2., 2., 1., 1., 2., 0., 1., 0., 2., 1., 1., 1.,\n",
      "        2., 1., 0., 1., 3., 3., 1., 2., 1., 3., 1., 3., 3., 0.])\n",
      "Hypothesized targets of subset:  tensor([3., 1., 2., 3., 1., 1., 1., 3., 0., 1., 0., 0., 3., 0., 2., 1., 0., 3.,\n",
      "        2., 1., 2., 2., 3., 1., 3., 3., 0., 1., 3., 0., 0., 0., 2., 1., 2., 1.,\n",
      "        3., 0., 0., 1., 3., 3., 0., 3., 2., 3., 3., 2., 3., 3.])\n",
      "50 2039 2089\n",
      "After augmentation, size of train_set:  341  lake set:  2039\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 33.790658831596375 0.5454545454545454 3.7355234026908875 0.525 69.21352082490921 0.513302034428795 2284.8894572257996\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  60.0 58.46\n",
      "val, test error% for class  1  :  30.0 30.38\n",
      "val, test error% for class  2  :  40.0 54.23\n",
      "val, test error% for class  3  :  60.0 75.22\n",
      "total misclassified ex from imb classes:  19\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2039, 5124])\n",
      "val minibatch gradients shape  torch.Size([19, 5124])\n",
      "kernel compute time:  1.5687751770019531\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  19  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 2., 2., 1., 3., 0., 2., 1., 0., 0., 3., 0., 0., 0., 1., 3., 0., 2.,\n",
      "        0., 2., 0., 3., 0., 2., 2., 2., 1., 3., 1., 0., 3., 1., 2., 1., 2., 0.,\n",
      "        2., 2., 1., 2., 0., 0., 3., 0., 2., 1., 1., 0., 1., 3.])\n",
      "Hypothesized targets of subset:  tensor([0., 2., 2., 1., 3., 0., 3., 2., 0., 0., 3., 0., 0., 0., 1., 2., 0., 2.,\n",
      "        3., 2., 1., 3., 0., 1., 3., 3., 3., 3., 0., 0., 3., 1., 1., 2., 3., 0.,\n",
      "        1., 2., 1., 3., 2., 1., 3., 0., 2., 0., 3., 2., 0., 2.])\n",
      "50 1989 2039\n",
      "After augmentation, size of train_set:  391  lake set:  1989\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 37.443259835243225 0.6035805626598465 3.651133894920349 0.475 64.02156925201416 0.543035993740219 2582.5423481464386\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  60.0 44.62\n",
      "val, test error% for class  1  :  40.0 39.62\n",
      "val, test error% for class  2  :  70.0 51.24\n",
      "val, test error% for class  3  :  40.0 50.44\n",
      "total misclassified ex from imb classes:  21\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1989, 5124])\n",
      "val minibatch gradients shape  torch.Size([21, 5124])\n",
      "kernel compute time:  0.7523863315582275\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  21  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 0., 1., 3., 1., 3., 0., 1., 0., 1., 2., 3., 3., 1., 3., 1., 1., 3.,\n",
      "        3., 0., 2., 3., 0., 2., 0., 2., 3., 0., 3., 0., 3., 1., 3., 1., 1., 3.,\n",
      "        3., 2., 2., 1., 2., 3., 0., 0., 1., 0., 2., 1., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 0., 1., 1., 3., 1., 1., 0., 0., 2., 2., 3., 0., 3., 1., 0., 1.,\n",
      "        3., 0., 2., 2., 0., 3., 0., 3., 3., 1., 2., 0., 2., 0., 2., 0., 1., 3.,\n",
      "        3., 2., 1., 2., 2., 2., 0., 0., 3., 0., 2., 1., 3., 2.])\n",
      "50 1939 1989\n",
      "After augmentation, size of train_set:  441  lake set:  1939\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 39.08375081419945 0.6439909297052154 3.5143336057662964 0.475 68.301185131073 0.5179968701095462 2909.7497520446777\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  40.0 29.23\n",
      "val, test error% for class  1  :  60.0 48.08\n",
      "val, test error% for class  2  :  90.0 56.72\n",
      "val, test error% for class  3  :  20.0 44.25\n",
      "total misclassified ex from imb classes:  21\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1939, 5124])\n",
      "val minibatch gradients shape  torch.Size([21, 5124])\n",
      "kernel compute time:  0.7352032661437988\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  21  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True targets of subset:  tensor([1., 0., 1., 2., 0., 2., 2., 1., 2., 1., 2., 1., 2., 2., 1., 1., 2., 3.,\n",
      "        2., 2., 3., 1., 1., 1., 2., 2., 2., 1., 1., 2., 1., 2., 0., 1., 1., 1.,\n",
      "        3., 3., 0., 0., 3., 1., 1., 3., 2., 3., 3., 2., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 2., 1., 1., 0., 1., 2., 0., 1., 3., 2., 2., 2., 2., 0., 1., 1., 3.,\n",
      "        3., 2., 3., 2., 0., 1., 1., 1., 3., 0., 0., 2., 1., 2., 0., 0., 2., 1.,\n",
      "        3., 2., 0., 0., 2., 0., 0., 2., 3., 3., 2., 3., 1., 2.])\n",
      "50 1889 1939\n",
      "After augmentation, size of train_set:  491  lake set:  1889\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 43.30398443341255 0.6272912423625254 3.7448381781578064 0.5 65.81571018695831 0.5289514866979655 3235.0083799362183\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  50.0 36.92\n",
      "val, test error% for class  1  :  60.0 51.92\n",
      "val, test error% for class  2  :  60.0 47.26\n",
      "val, test error% for class  3  :  30.0 41.59\n",
      "total misclassified ex from imb classes:  20\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1889, 5124])\n",
      "val minibatch gradients shape  torch.Size([20, 5124])\n",
      "kernel compute time:  1.540198802947998\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  20  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 3., 3., 1., 2., 3., 3., 3., 1., 1., 3., 0., 1., 1., 0., 1., 2., 2.,\n",
      "        0., 3., 0., 3., 1., 0., 2., 1., 2., 3., 1., 0., 2., 3., 1., 2., 3., 3.,\n",
      "        3., 3., 0., 1., 1., 1., 2., 2., 1., 1., 1., 2., 1., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 2., 1., 2., 2., 3., 1., 2., 3., 1., 1., 0., 2., 1., 1., 2.,\n",
      "        0., 3., 0., 3., 2., 0., 0., 1., 3., 3., 1., 0., 0., 2., 0., 3., 2., 2.,\n",
      "        3., 0., 0., 1., 1., 0., 2., 2., 1., 0., 0., 3., 1., 1.])\n",
      "50 1839 1889\n",
      "After augmentation, size of train_set:  541  lake set:  1839\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 50.93702311813831 0.5933456561922366 3.451745331287384 0.55 70.84301155805588 0.5289514866979655 3595.7915720939636\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  30.0 32.31\n",
      "val, test error% for class  1  :  30.0 41.92\n",
      "val, test error% for class  2  :  70.0 50.25\n",
      "val, test error% for class  3  :  50.0 61.95\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1839, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.6483516693115234\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  18  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 1., 0., 1., 2., 1., 3., 1., 2., 1., 0., 2., 2., 0., 1., 1., 0., 3.,\n",
      "        3., 1., 3., 2., 3., 1., 2., 2., 1., 0., 3., 3., 2., 3., 1., 0., 0., 3.,\n",
      "        1., 0., 3., 2., 2., 3., 1., 0., 2., 2., 2., 2., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 1., 0., 2., 1., 1., 3., 0., 2., 1., 0., 2., 1., 0., 1., 1., 2., 2.,\n",
      "        3., 3., 0., 2., 2., 1., 3., 3., 1., 0., 2., 3., 1., 3., 2., 1., 0., 3.,\n",
      "        0., 0., 3., 2., 2., 3., 1., 1., 3., 3., 2., 0., 3., 2.])\n",
      "50 1789 1839\n",
      "After augmentation, size of train_set:  591  lake set:  1789\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 53.319718688726425 0.6074450084602369 3.3266775012016296 0.55 70.44995683431625 0.5164319248826291 3917.0299949645996\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  50.0 36.92\n",
      "val, test error% for class  1  :  40.0 51.92\n",
      "val, test error% for class  2  :  70.0 54.73\n",
      "val, test error% for class  3  :  20.0 35.4\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1789, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.7918078899383545\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  18  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 1., 1., 0., 1., 2., 1., 2., 0., 1., 2., 3., 1., 2., 0., 1., 2., 3.,\n",
      "        3., 1., 2., 1., 1., 3., 2., 3., 3., 0., 0., 3., 3., 1., 2., 3., 1., 1.,\n",
      "        1., 0., 3., 1., 1., 0., 3., 0., 0., 2., 1., 3., 2., 0.])\n",
      "Hypothesized targets of subset:  tensor([1., 1., 2., 0., 1., 3., 0., 2., 0., 1., 1., 2., 1., 2., 0., 1., 2., 3.,\n",
      "        1., 1., 3., 0., 2., 2., 2., 2., 3., 0., 0., 3., 3., 2., 3., 0., 1., 0.,\n",
      "        1., 0., 2., 3., 2., 0., 2., 0., 1., 0., 2., 3., 2., 0.])\n",
      "50 1739 1789\n",
      "After augmentation, size of train_set:  641  lake set:  1739\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 56.077756851911545 0.625585023400936 3.2603082060813904 0.55 62.901981234550476 0.5602503912363067 4333.192449331284\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  50.0 27.69\n",
      "val, test error% for class  1  :  40.0 46.54\n",
      "val, test error% for class  2  :  70.0 47.76\n",
      "val, test error% for class  3  :  20.0 40.71\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1739, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.6120522022247314\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  18  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 0., 0., 3., 0., 1., 1., 2., 1., 2., 0., 2., 2., 2., 2., 1., 2., 0.,\n",
      "        2., 1., 0., 0., 3., 1., 0., 1., 1., 2., 3., 1., 0., 2., 1., 3., 2., 2.,\n",
      "        1., 2., 1., 2., 0., 2., 1., 2., 3., 2., 2., 1., 3., 2.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 0., 3., 0., 1., 1., 2., 1., 2., 0., 2., 1., 2., 2., 3., 1., 0.,\n",
      "        2., 0., 0., 2., 2., 1., 0., 1., 1., 1., 2., 1., 0., 3., 2., 3., 2., 3.,\n",
      "        0., 2., 0., 1., 0., 2., 1., 0., 3., 1., 1., 0., 3., 1.])\n",
      "50 1689 1739\n",
      "After augmentation, size of train_set:  691  lake set:  1689\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 57.574298709630966 0.6526772793053546 3.2057632207870483 0.575 64.90503972768784 0.5383411580594679 4696.375428676605\n",
      "AL epoch:  10\n",
      "val, test error% for class  0  :  40.0 38.46\n",
      "val, test error% for class  1  :  50.0 46.54\n",
      "val, test error% for class  2  :  50.0 40.3\n",
      "val, test error% for class  3  :  30.0 60.18\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1689, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel compute time:  0.6493377685546875\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  17  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 2., 1., 1., 3., 2., 3., 3., 0., 2., 1., 0., 1., 1., 3., 1., 3., 1.,\n",
      "        3., 1., 1., 0., 0., 3., 1., 2., 3., 3., 1., 1., 3., 1., 3., 1., 3., 0.,\n",
      "        1., 3., 2., 3., 3., 2., 1., 1., 2., 1., 2., 0., 2., 2.])\n",
      "Hypothesized targets of subset:  tensor([1., 2., 3., 1., 3., 2., 0., 2., 0., 1., 0., 0., 0., 1., 2., 0., 3., 1.,\n",
      "        3., 1., 1., 0., 0., 3., 0., 2., 1., 3., 2., 0., 3., 1., 2., 1., 3., 0.,\n",
      "        2., 3., 3., 2., 2., 2., 1., 3., 1., 0., 1., 0., 2., 2.])\n",
      "50 1639 1689\n",
      "After augmentation, size of train_set:  741  lake set:  1639\n",
      "Epoch: 11 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 65.8680009841919 0.6167341430499326 2.8853878676891327 0.65 66.14696830511093 0.543035993740219 5063.139382839203\n",
      "AL epoch:  11\n",
      "val, test error% for class  0  :  40.0 35.38\n",
      "val, test error% for class  1  :  30.0 47.69\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  20.0 39.82\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1639, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.5868003368377686\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 1., 3., 1., 1., 2., 3., 3., 1., 3., 2., 3., 1., 1., 1., 2., 1., 2.,\n",
      "        3., 2., 0., 3., 2., 1., 3., 2., 2., 0., 1., 1., 1., 3., 3., 1., 3., 3.,\n",
      "        2., 0., 1., 3., 2., 1., 1., 0., 1., 2., 1., 1., 2., 3.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 0., 0., 2., 2., 0., 2., 2., 1., 3., 0., 0., 2., 1., 1., 2.,\n",
      "        3., 3., 0., 3., 1., 1., 2., 2., 3., 0., 0., 1., 1., 1., 3., 0., 2., 3.,\n",
      "        0., 2., 2., 3., 1., 0., 1., 0., 0., 3., 0., 1., 3., 2.])\n",
      "50 1589 1639\n",
      "After augmentation, size of train_set:  791  lake set:  1589\n",
      "Epoch: 12 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 71.64822047948837 0.606826801517067 3.187554359436035 0.575 69.83818084001541 0.5101721439749609 5426.219938755035\n",
      "AL epoch:  12\n",
      "val, test error% for class  0  :  40.0 30.77\n",
      "val, test error% for class  1  :  50.0 50.38\n",
      "val, test error% for class  2  :  40.0 52.74\n",
      "val, test error% for class  3  :  40.0 49.56\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1589, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n",
      "kernel compute time:  0.7422590255737305\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  17  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 3., 2., 1., 2., 2., 2., 2., 1., 0., 1., 1., 2., 1., 2., 1.,\n",
      "        2., 3., 3., 1., 0., 2., 2., 3., 3., 1., 3., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        2., 2., 3., 2., 1., 2., 2., 2., 0., 2., 1., 3., 0., 1.])\n",
      "Hypothesized targets of subset:  tensor([0., 1., 3., 3., 0., 1., 0., 2., 0., 2., 0., 0., 0., 1., 2., 1., 1., 0.,\n",
      "        1., 3., 3., 1., 0., 3., 2., 3., 2., 1., 3., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        2., 3., 2., 2., 1., 2., 2., 0., 0., 1., 3., 3., 3., 2.])\n",
      "50 1539 1589\n",
      "After augmentation, size of train_set:  841  lake set:  1539\n",
      "Epoch: 13 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 70.6075593829155 0.6444708680142688 3.172585964202881 0.65 61.7319073677063 0.5586854460093896 5838.497944116592\n",
      "AL epoch:  13\n",
      "val, test error% for class  0  :  30.0 36.92\n",
      "val, test error% for class  1  :  40.0 52.69\n",
      "val, test error% for class  2  :  20.0 37.81\n",
      "val, test error% for class  3  :  50.0 39.82\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1539, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.4929676055908203\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 2., 1., 1., 2., 1., 3., 3., 1., 2., 3., 2., 3., 2., 2., 2., 0., 3.,\n",
      "        0., 2., 1., 1., 1., 1., 3., 3., 2., 1., 0., 2., 0., 1., 1., 3., 2., 1.,\n",
      "        0., 3., 3., 3., 2., 0., 2., 1., 3., 2., 2., 1., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([3., 2., 3., 0., 2., 1., 3., 3., 0., 2., 3., 1., 3., 2., 2., 1., 0., 3.,\n",
      "        1., 2., 1., 1., 1., 1., 3., 3., 1., 0., 0., 2., 0., 0., 0., 3., 2., 1.,\n",
      "        0., 2., 3., 2., 3., 0., 3., 1., 1., 3., 3., 0., 2., 0.])\n",
      "50 1489 1539\n",
      "After augmentation, size of train_set:  891  lake set:  1489\n",
      "Epoch: 14 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 75.81520435214043 0.6352413019079686 2.9937873482704163 0.65 63.763407826423645 0.5555555555555556 6182.668705940247\n",
      "AL epoch:  14\n",
      "val, test error% for class  0  :  30.0 36.92\n",
      "val, test error% for class  1  :  40.0 51.15\n",
      "val, test error% for class  2  :  30.0 44.78\n",
      "val, test error% for class  3  :  40.0 32.74\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1489, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.6919732093811035\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 0., 1., 3., 2., 3., 1., 0., 1., 1., 2., 2., 2., 3., 2., 3., 2., 1.,\n",
      "        2., 1., 1., 2., 2., 1., 3., 2., 1., 3., 3., 2., 2., 0., 0., 0., 3., 1.,\n",
      "        2., 1., 1., 0., 1., 3., 1., 1., 1., 0., 1., 1., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([3., 2., 0., 3., 2., 3., 1., 1., 0., 3., 2., 3., 1., 3., 3., 3., 2., 0.,\n",
      "        0., 0., 1., 3., 2., 0., 3., 2., 0., 2., 2., 2., 3., 0., 1., 0., 3., 1.,\n",
      "        2., 2., 0., 1., 1., 3., 0., 1., 0., 0., 1., 1., 2., 1.])\n",
      "50 1439 1489\n",
      "After augmentation, size of train_set:  941  lake set:  1439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 82.72060441970825 0.6323060573857598 3.2334787845611572 0.625 61.430308520793915 0.568075117370892 6534.424946784973\n",
      "AL epoch:  15\n",
      "val, test error% for class  0  :  30.0 29.23\n",
      "val, test error% for class  1  :  40.0 47.31\n",
      "val, test error% for class  2  :  50.0 47.76\n",
      "val, test error% for class  3  :  30.0 33.63\n",
      "total misclassified ex from imb classes:  15\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1439, 5124])\n",
      "val minibatch gradients shape  torch.Size([15, 5124])\n",
      "kernel compute time:  0.5626442432403564\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  15  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 2., 1., 1., 1., 2., 2., 1., 3., 1., 1., 2., 0., 2., 2., 2., 3., 1.,\n",
      "        3., 1., 1., 2., 3., 2., 2., 3., 2., 1., 3., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        2., 1., 3., 2., 2., 2., 3., 1., 1., 0., 1., 0., 2., 3.])\n",
      "Hypothesized targets of subset:  tensor([2., 3., 3., 0., 0., 2., 2., 1., 2., 1., 0., 3., 0., 2., 3., 1., 3., 2.,\n",
      "        0., 1., 0., 3., 2., 1., 0., 3., 0., 2., 2., 1., 0., 2., 1., 0., 1., 0.,\n",
      "        3., 0., 2., 2., 1., 2., 3., 1., 1., 0., 3., 2., 3., 3.])\n",
      "50 1389 1439\n",
      "After augmentation, size of train_set:  991  lake set:  1389\n",
      "Epoch: 16 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 77.78142288327217 0.6821392532795156 2.9425390362739563 0.65 61.032109409570694 0.5492957746478874 6867.935442447662\n",
      "AL epoch:  16\n",
      "val, test error% for class  0  :  10.0 29.23\n",
      "val, test error% for class  1  :  30.0 47.69\n",
      "val, test error% for class  2  :  60.0 54.23\n",
      "val, test error% for class  3  :  40.0 31.86\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1389, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.5280313491821289\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 3., 1., 1., 2., 1., 3., 2., 1., 2., 0., 1., 2., 1., 3., 1., 1., 1.,\n",
      "        3., 2., 3., 1., 1., 2., 2., 3., 3., 2., 1., 3., 2., 3., 1., 0., 2., 3.,\n",
      "        1., 0., 1., 2., 3., 2., 3., 1., 1., 1., 2., 2., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 3., 1., 0., 3., 3., 3., 2., 1., 3., 0., 1., 2., 2., 3., 1., 1., 1.,\n",
      "        2., 2., 3., 0., 0., 1., 2., 3., 3., 2., 1., 0., 2., 3., 0., 1., 2., 0.,\n",
      "        1., 0., 2., 3., 3., 2., 3., 1., 1., 2., 2., 2., 2., 2.])\n",
      "50 1339 1389\n",
      "After augmentation, size of train_set:  1041  lake set:  1339\n",
      "Epoch: 17 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 88.16091111302376 0.6407300672430355 2.7356441020965576 0.7 62.42228502035141 0.564945226917058 7241.87736082077\n",
      "AL epoch:  17\n",
      "val, test error% for class  0  :  40.0 32.31\n",
      "val, test error% for class  1  :  20.0 41.54\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  10.0 43.36\n",
      "total misclassified ex from imb classes:  12\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1339, 5124])\n",
      "val minibatch gradients shape  torch.Size([12, 5124])\n",
      "kernel compute time:  0.592418909072876\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  12  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 1., 0., 2., 2., 3., 0., 1., 1., 2., 2., 3., 2., 1., 1., 2.,\n",
      "        1., 2., 2., 0., 2., 3., 2., 2., 2., 1., 0., 2., 2., 2., 1., 2., 2., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 2., 1., 2., 0., 1., 3., 3., 0.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 1., 0., 2., 2., 3., 0., 1., 0., 2., 1., 3., 3., 0., 0., 2.,\n",
      "        0., 2., 2., 0., 3., 2., 2., 1., 1., 2., 0., 2., 3., 1., 1., 2., 1., 2.,\n",
      "        2., 0., 0., 0., 2., 1., 2., 0., 1., 0., 0., 0., 3., 0.])\n",
      "50 1289 1339\n",
      "After augmentation, size of train_set:  1091  lake set:  1289\n",
      "Epoch: 18 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 99.78812801837921 0.5802016498625114 3.349065363407135 0.575 64.5497316122055 0.5117370892018779 7301.767403841019\n",
      "AL epoch:  18\n",
      "val, test error% for class  0  :  40.0 24.62\n",
      "val, test error% for class  1  :  50.0 50.38\n",
      "val, test error% for class  2  :  50.0 66.67\n",
      "val, test error% for class  3  :  30.0 27.43\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1289, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n",
      "kernel compute time:  0.37871479988098145\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  17  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 2., 1., 1., 2., 0., 2., 2., 2., 1., 3., 2., 2., 1., 2., 0., 1., 1.,\n",
      "        2., 3., 3., 1., 0., 3., 2., 2., 1., 2., 2., 2., 1., 3., 2., 1., 3., 2.,\n",
      "        2., 1., 2., 2., 0., 1., 1., 1., 1., 2., 2., 1., 0., 1.])\n",
      "Hypothesized targets of subset:  tensor([0., 2., 1., 1., 2., 0., 1., 2., 3., 1., 3., 3., 2., 1., 3., 0., 1., 0.,\n",
      "        1., 3., 3., 0., 0., 3., 1., 1., 0., 2., 1., 2., 0., 2., 1., 0., 3., 3.,\n",
      "        0., 0., 3., 1., 0., 0., 0., 2., 2., 2., 2., 3., 0., 1.])\n",
      "50 1239 1289\n",
      "After augmentation, size of train_set:  1141  lake set:  1239\n",
      "Epoch: 19 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 98.99988523125648 0.6143733567046451 2.815349206328392 0.7 63.19356516003609 0.5524256651017214 7790.68147110939\n",
      "AL epoch:  19\n",
      "val, test error% for class  0  :  20.0 29.23\n",
      "val, test error% for class  1  :  50.0 49.23\n",
      "val, test error% for class  2  :  50.0 56.72\n",
      "val, test error% for class  3  :  0.0 22.12\n",
      "total misclassified ex from imb classes:  12\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1239, 5124])\n",
      "val minibatch gradients shape  torch.Size([12, 5124])\n",
      "kernel compute time:  0.376605749130249\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  12  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True targets of subset:  tensor([1., 0., 3., 1., 2., 1., 0., 3., 2., 3., 0., 1., 2., 2., 2., 1., 2., 3.,\n",
      "        3., 1., 2., 2., 1., 1., 0., 1., 1., 2., 2., 0., 1., 3., 1., 2., 1., 1.,\n",
      "        2., 1., 1., 1., 2., 2., 1., 2., 3., 1., 2., 1., 1., 2.])\n",
      "Hypothesized targets of subset:  tensor([1., 0., 3., 2., 1., 0., 1., 3., 1., 2., 0., 1., 2., 2., 2., 0., 2., 1.,\n",
      "        3., 0., 2., 0., 2., 2., 1., 1., 0., 2., 2., 0., 1., 3., 1., 3., 1., 0.,\n",
      "        1., 0., 1., 0., 2., 2., 1., 2., 3., 0., 1., 1., 0., 2.])\n",
      "50 1189 1239\n",
      "After augmentation, size of train_set:  1191  lake set:  1189\n",
      "Epoch: 20 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 93.03159859776497 0.6809403862300588 2.8329659700393677 0.7 57.633639603853226 0.6087636932707355 8080.754028558731\n",
      "val, test error% for class  0  :  30.0 33.85\n",
      "val, test error% for class  1  :  50.0 40.38\n",
      "val, test error% for class  2  :  30.0 43.78\n",
      "val, test error% for class  3  :  10.0 30.97\n",
      "[[89.23, 46.15, 72.64, 86.73, 73.6875], [50.77, 46.15, 44.78, 68.14, 52.459999999999994], [58.46, 30.38, 54.23, 75.22, 54.5725], [44.62, 39.62, 51.24, 50.44, 46.48], [29.23, 48.08, 56.72, 44.25, 44.57], [36.92, 51.92, 47.26, 41.59, 44.4225], [32.31, 41.92, 50.25, 61.95, 46.6075], [36.92, 51.92, 54.73, 35.4, 44.7425], [27.69, 46.54, 47.76, 40.71, 40.675000000000004], [38.46, 46.54, 40.3, 60.18, 46.37], [35.38, 47.69, 49.75, 39.82, 43.16], [30.77, 50.38, 52.74, 49.56, 45.862500000000004], [36.92, 52.69, 37.81, 39.82, 41.81], [36.92, 51.15, 44.78, 32.74, 41.3975], [29.23, 47.31, 47.76, 33.63, 39.4825], [29.23, 47.69, 54.23, 31.86, 40.7525], [32.31, 41.54, 49.75, 43.36, 41.739999999999995], [24.62, 50.38, 66.67, 27.43, 42.275000000000006], [29.23, 49.23, 56.72, 22.12, 39.325], [33.85, 40.38, 43.78, 30.97, 37.245000000000005]]\n"
     ]
    }
   ],
   "source": [
    "logdetmi_tst, logdetmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9065bb",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c7d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl_tst, fl_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a1388",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f2323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc_tst, gc_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'gc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8087c",
   "metadata": {},
   "source": [
    "# LOGDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b3b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logdet_tst, logdet_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'logdet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0690c",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee7e13b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random random\n",
      "Breast-density Custom dataset stats: Train size:  261 Val size:  40 Lake size:  2139\n",
      "selected classes are:  [0, 3]\n",
      "Saving results to:  SMI_active_learning_results_woVal/breast_density/classimb/random/50/2\n",
      "breast_density_classimb_random_2_random_budget:50_epochs:20_linear:True_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/breast_density_MobileNetV2_0.01_{0: 35, 3: 40}_{1: 93, 2: 73}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  80.0 89.23\n",
      "val, test error% for class  1  :  50.0 46.15\n",
      "val, test error% for class  2  :  70.0 72.64\n",
      "val, test error% for class  3  :  100.0 86.73\n",
      "50 2089 2139\n",
      "After augmentation, size of train_set:  311  lake set:  2089\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 31.729574978351593 0.572347266881029 3.69535756111145 0.575 70.8421282172203 0.47104851330203446 2019.6698310375214\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  40.0 30.77\n",
      "val, test error% for class  1  :  60.0 56.92\n",
      "val, test error% for class  2  :  50.0 65.17\n",
      "val, test error% for class  3  :  20.0 34.51\n",
      "50 2039 2089\n",
      "After augmentation, size of train_set:  361  lake set:  2039\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 36.04263839125633 0.5706371191135734 3.4721153378486633 0.625 64.00381964445114 0.5179968701095462 2360.5135147571564\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  20.0 40.0\n",
      "val, test error% for class  1  :  20.0 40.0\n",
      "val, test error% for class  2  :  90.0 65.67\n",
      "val, test error% for class  3  :  20.0 40.71\n",
      "50 1989 2039\n",
      "After augmentation, size of train_set:  411  lake set:  1989\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 123.27385005354881 0.3284671532846715 14.016320884227753 0.35 168.07931531965733 0.23943661971830985 2669.418761253357\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  10.0 13.85\n",
      "val, test error% for class  1  :  50.0 66.92\n",
      "val, test error% for class  2  :  100.0 95.02\n",
      "val, test error% for class  3  :  100.0 99.12\n",
      "50 1939 1989\n",
      "After augmentation, size of train_set:  461  lake set:  1939\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 60.78976488113403 0.49023861171366595 5.860463500022888 0.475 73.39931386709213 0.4491392801251956 3001.713359117508\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  20.0 30.77\n",
      "val, test error% for class  1  :  50.0 42.69\n",
      "val, test error% for class  2  :  70.0 71.14\n",
      "val, test error% for class  3  :  70.0 69.03\n",
      "50 1889 1939\n",
      "After augmentation, size of train_set:  511  lake set:  1889\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 48.76243484020233 0.5870841487279843 3.0712939500808716 0.65 69.7135682106018 0.5007824726134585 3315.3356070518494\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  30.0 21.54\n",
      "val, test error% for class  1  :  30.0 50.77\n",
      "val, test error% for class  2  :  70.0 55.72\n",
      "val, test error% for class  3  :  10.0 53.98\n",
      "50 1839 1889\n",
      "After augmentation, size of train_set:  561  lake set:  1839\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 56.81001389026642 0.5525846702317291 3.8604289293289185 0.475 69.948421895504 0.460093896713615 3665.68163895607\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  50.0 38.46\n",
      "val, test error% for class  1  :  70.0 65.38\n",
      "val, test error% for class  2  :  60.0 56.72\n",
      "val, test error% for class  3  :  30.0 31.86\n",
      "50 1789 1839\n",
      "After augmentation, size of train_set:  611  lake set:  1789\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 59.69584399461746 0.5941080196399345 2.90768626332283 0.75 64.52295517921448 0.5336463223787168 3972.7148768901825\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  40.0 44.62\n",
      "val, test error% for class  1  :  30.0 46.54\n",
      "val, test error% for class  2  :  10.0 46.77\n",
      "val, test error% for class  3  :  20.0 47.79\n",
      "50 1739 1789\n",
      "After augmentation, size of train_set:  661  lake set:  1739\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 59.6582750082016 0.5748865355521936 3.452463537454605 0.65 67.17151951789856 0.5101721439749609 4346.620023012161\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  40.0 27.69\n",
      "val, test error% for class  1  :  40.0 56.54\n",
      "val, test error% for class  2  :  40.0 47.76\n",
      "val, test error% for class  3  :  20.0 46.02\n",
      "50 1689 1739\n",
      "After augmentation, size of train_set:  711  lake set:  1689\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 74.57207798957825 0.5457102672292545 4.039910435676575 0.475 67.3516834974289 0.5117370892018779 4672.6437520980835\n",
      "AL epoch:  10\n",
      "val, test error% for class  0  :  40.0 26.15\n",
      "val, test error% for class  1  :  50.0 43.46\n",
      "val, test error% for class  2  :  70.0 62.19\n",
      "val, test error% for class  3  :  50.0 50.44\n",
      "50 1639 1689\n",
      "After augmentation, size of train_set:  761  lake set:  1639\n",
      "Epoch: 11 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 75.86768567562103 0.5834428383705651 3.3240476548671722 0.6 64.49960833787918 0.5665101721439749 4968.161025762558\n",
      "AL epoch:  11\n",
      "val, test error% for class  0  :  40.0 46.15\n",
      "val, test error% for class  1  :  40.0 33.46\n",
      "val, test error% for class  2  :  60.0 50.75\n",
      "val, test error% for class  3  :  20.0 51.33\n",
      "50 1589 1639\n",
      "After augmentation, size of train_set:  811  lake set:  1589\n",
      "Epoch: 12 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 73.83511891961098 0.6017262638717632 3.3716474175453186 0.625 61.47042894363403 0.5524256651017214 5268.381823539734\n",
      "AL epoch:  12\n",
      "val, test error% for class  0  :  30.0 41.54\n",
      "val, test error% for class  1  :  40.0 42.31\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  30.0 43.36\n",
      "50 1539 1589\n",
      "After augmentation, size of train_set:  861  lake set:  1539\n",
      "Epoch: 13 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 94.155921459198 0.49361207897793263 4.062448665499687 0.55 75.85044610500336 0.42566510172143973 5630.066622972488\n",
      "AL epoch:  13\n",
      "val, test error% for class  0  :  30.0 16.92\n",
      "val, test error% for class  1  :  60.0 70.0\n",
      "val, test error% for class  2  :  90.0 74.13\n",
      "val, test error% for class  3  :  0.0 22.12\n",
      "50 1489 1539\n",
      "After augmentation, size of train_set:  911  lake set:  1489\n",
      "Epoch: 14 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 83.09704878926277 0.6256860592755215 2.851384937763214 0.75 59.0727396607399 0.568075117370892 5921.996418237686\n",
      "AL epoch:  14\n",
      "val, test error% for class  0  :  30.0 52.31\n",
      "val, test error% for class  1  :  40.0 35.38\n",
      "val, test error% for class  2  :  20.0 48.76\n",
      "val, test error% for class  3  :  10.0 46.02\n",
      "50 1439 1489\n",
      "After augmentation, size of train_set:  961  lake set:  1439\n",
      "Epoch: 15 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 90.03280314803123 0.5848074921956296 2.966593623161316 0.7 62.81592047214508 0.5336463223787168 6263.190521240234\n",
      "AL epoch:  15\n",
      "val, test error% for class  0  :  10.0 24.62\n",
      "val, test error% for class  1  :  50.0 51.92\n",
      "val, test error% for class  2  :  60.0 62.69\n",
      "val, test error% for class  3  :  0.0 18.58\n",
      "50 1389 1439\n",
      "After augmentation, size of train_set:  1011  lake set:  1389\n",
      "Epoch: 16 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 97.71351191401482 0.5687438180019783 2.879222333431244 0.65 60.165692150592804 0.5805946791862285 6594.137750864029\n",
      "AL epoch:  16\n",
      "val, test error% for class  0  :  30.0 40.0\n",
      "val, test error% for class  1  :  20.0 38.85\n",
      "val, test error% for class  2  :  50.0 36.32\n",
      "val, test error% for class  3  :  40.0 60.18\n",
      "50 1339 1389\n",
      "After augmentation, size of train_set:  1061  lake set:  1339\n",
      "Epoch: 17 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 93.0324255824089 0.6060320452403393 3.2871562838554382 0.575 60.47174718976021 0.5712050078247262 6944.252722024918\n",
      "AL epoch:  17\n",
      "val, test error% for class  0  :  50.0 41.54\n",
      "val, test error% for class  1  :  60.0 42.31\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  10.0 32.74\n",
      "50 1289 1339\n",
      "After augmentation, size of train_set:  1111  lake set:  1289\n",
      "Epoch: 18 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 104.64263242483139 0.5670567056705671 3.160198152065277 0.6 60.3795505464077 0.5461658841940532 7312.527335643768\n",
      "AL epoch:  18\n",
      "val, test error% for class  0  :  80.0 84.62\n",
      "val, test error% for class  1  :  0.0 23.85\n",
      "val, test error% for class  2  :  60.0 74.13\n",
      "val, test error% for class  3  :  20.0 21.24\n",
      "50 1239 1289\n",
      "After augmentation, size of train_set:  1161  lake set:  1239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 97.54228103160858 0.632213608957795 2.8004336059093475 0.75 58.62907028198242 0.5821596244131455 7615.39546918869\n",
      "AL epoch:  19\n",
      "val, test error% for class  0  :  40.0 46.15\n",
      "val, test error% for class  1  :  0.0 36.54\n",
      "val, test error% for class  2  :  60.0 51.24\n",
      "val, test error% for class  3  :  0.0 34.51\n",
      "50 1189 1239\n",
      "After augmentation, size of train_set:  1211  lake set:  1189\n",
      "Epoch: 20 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 104.68075922131538 0.6077621800165153 3.2187494337558746 0.65 57.81700223684311 0.5727699530516432 7946.918114185333\n",
      "val, test error% for class  0  :  50.0 47.69\n",
      "val, test error% for class  1  :  30.0 43.08\n",
      "val, test error% for class  2  :  40.0 42.29\n",
      "val, test error% for class  3  :  20.0 39.82\n",
      "[[89.23, 46.15, 72.64, 86.73, 73.6875], [30.77, 56.92, 65.17, 34.51, 46.8425], [40.0, 40.0, 65.67, 40.71, 46.595000000000006], [13.85, 66.92, 95.02, 99.12, 68.72749999999999], [30.77, 42.69, 71.14, 69.03, 53.4075], [21.54, 50.77, 55.72, 53.98, 45.5025], [38.46, 65.38, 56.72, 31.86, 48.105000000000004], [44.62, 46.54, 46.77, 47.79, 46.43], [27.69, 56.54, 47.76, 46.02, 44.502500000000005], [26.15, 43.46, 62.19, 50.44, 45.56], [46.15, 33.46, 50.75, 51.33, 45.4225], [41.54, 42.31, 49.75, 43.36, 44.239999999999995], [16.92, 70.0, 74.13, 22.12, 45.792500000000004], [52.31, 35.38, 48.76, 46.02, 45.6175], [24.62, 51.92, 62.69, 18.58, 39.4525], [40.0, 38.85, 36.32, 60.18, 43.8375], [41.54, 42.31, 49.75, 32.74, 41.585], [84.62, 23.85, 74.13, 21.24, 50.96], [46.15, 36.54, 51.24, 34.51, 42.11], [47.69, 43.08, 42.29, 39.82, 43.22]]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1,6):\n",
    "random_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"random\",'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285906f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
