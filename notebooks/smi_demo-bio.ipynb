{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from cords.cords.selectionstrategies.supervisedlearning import DataSelectionStrategy\n",
    "# from cords.cords.utils.models import ResNet18\n",
    "from distil.distil.utils.models.resnet import ResNet18\n",
    "from gable.gable.utils.models import MobileNetV2\n",
    "from gable.gable.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from distil.distil.active_learning_strategies import BADGE, EntropySampling, GLISTER, GradMatchActive, CoreSet, LeastConfidence, MarginSampling \n",
    "from distil.distil.utils.DataHandler import DataHandler_CIFAR10, DataHandler_MNIST\n",
    "\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "# for cuda\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2420\n",
      "639\n",
      "train: 0 354\n",
      "test: 0 65\n",
      "train: 1 932\n",
      "test: 1 260\n",
      "train: 2 734\n",
      "test: 2 201\n",
      "train: 3 400\n",
      "test: 3 113\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/snk170001/research/data/custom/bc_train_test\"\n",
    "input_size=224\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "#         transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "fullset = datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train'])\n",
    "testset = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "print(len(fullset))\n",
    "print(len(testset))\n",
    "for i in range(4): #all_classes\n",
    "    full_idx_class = list(torch.where(torch.Tensor(fullset.targets) == i)[0].cpu().numpy())\n",
    "    print(\"train: \" + str(i) , len(full_idx_class))\n",
    "    full_idx_class = list(torch.where(torch.Tensor(testset.targets) == i)[0].cpu().numpy())\n",
    "    print(\"test: \" + str(i) , len(full_idx_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for i in range(len(fullset)):\n",
    "    X.append(fullset[i][0].tolist())\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "X = np.array(X)\n",
    "# print(X[0])\n",
    "X_r = X.transpose(0,2,3,1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FE7E17D20D0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = X.astype(np.int)\n",
    "print(X[0].shape)\n",
    "Image.fromarray((X_r[0]).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class custom_subset(Dataset):\n",
    "    r\"\"\"\n",
    "    Subset of a dataset at specified indices.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): The whole Dataset\n",
    "        indices (sequence): Indices in the whole set selected for subset\n",
    "        labels(sequence) : targets as required for the indices. will be the same length as indices\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, labels):\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        self.targets = labels.type(torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx][0]\n",
    "        target = self.targets[idx]\n",
    "        return (image, target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler_Medical(Dataset):\n",
    "    def __init__(self, X, Y=None, select=True, use_test_transform = False):\n",
    "        self.select = select\n",
    "        self.use_test_transform=use_test_transform\n",
    "        self.training_gen_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        self.test_gen_transform = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        if not self.select:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "        else:\n",
    "            self.X = X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not self.select:\n",
    "            x, y = self.X[index], self.Y[index]\n",
    "            x = Image.fromarray(x)\n",
    "            if self.use_test_transform:\n",
    "                x = self.test_gen_transform(x)\n",
    "            else:\n",
    "                x = self.training_gen_transform(x)\n",
    "            return x, y, index\n",
    "\n",
    "        else:\n",
    "            x = self.X[index]\n",
    "            x = Image.fromarray(x)\n",
    "            if self.use_test_transform:\n",
    "                x = self.test_gen_transform(x)\n",
    "            else:\n",
    "                x = self.training_gen_transform(x)\n",
    "            return x, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "def __init__(self, X, Y=None, select=True, use_test_transform=False):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        self.select = select\n",
    "        self.use_test_transform=use_test_transform\n",
    "        self.training_gen_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "        self.test_gen_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) # ImageNet mean/std\n",
    "        if not self.select:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "        else:\n",
    "            self.X = X\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device):\n",
    "    if name == 'ResNet18':\n",
    "        model = ResNet18(num_cls)\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    elif name == 'MobileNetV2':\n",
    "        model = MobileNetV2(num_cls)\n",
    "#         model = models.mobilenet_v2(pretrained=True)\n",
    "#         model.classifier = nn.Sequential(\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(model.last_channel, num_cls),\n",
    "#         ) \n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "\n",
    "def kernel(x, y, measure=\"cosine\", exp=2):\n",
    "    if(measure==\"eu_sim\"):\n",
    "        dist = pairwise_distances(x.cpu().numpy(), y.cpu().numpy())\n",
    "        sim = max(dist.ravel()) - dist\n",
    "#         n = x.size(0)\n",
    "#         m = y.size(0)\n",
    "#         d = x.size(1)\n",
    "#         x = x.unsqueeze(1).expand(n, m, d)\n",
    "#         y = y.unsqueeze(0).expand(n, m, d)\n",
    "#         dist = torch.pow(x - y, exp).sum(2)\n",
    "#         const = torch.max(dist).item()\n",
    "#         sim = (const - dist)\n",
    "    \n",
    "        #dist = torch.exp(-1 * torch.pow(x - y, 2).sum(2))\n",
    "    if(measure==\"cosine\"):\n",
    "        sim = cosine_similarity(x.cpu().numpy(), y.cpu().numpy())\n",
    "    return sim\n",
    "\n",
    "\n",
    "def save_kernel_hdf5(lake_kernel, lake_target_kernel, target_kernel=[], lake_private_kernel=[], pp_kernel=[], numpy=True):\n",
    "    if(not(numpy)):\n",
    "        lake_kernel = lake_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_kernel_\" + device.split(\":\")[1] +\".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_target_kernel = lake_target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_target_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_target_kernel)\n",
    "    if(not(numpy)):\n",
    "        target_kernel = target_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_target_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=target_kernel)\n",
    "    if(not(numpy)):\n",
    "        lake_private_kernel = lake_private_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_lake_private_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=lake_private_kernel)\n",
    "    if(not(numpy)):\n",
    "        pp_kernel = pp_kernel.cpu().numpy()\n",
    "    with h5py.File(\"smi_pp_kernel_\" + device.split(\":\")[1] + \".hdf5\", 'w') as hf:\n",
    "        hf.create_dataset(\"kernel\",  data=pp_kernel)\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "#     saveDir = os.path.join(saveDir, prefix)\n",
    "#     if(not(os.path.exists(saveDir))):\n",
    "#         os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "        print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = custom_subset(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    if(feature==\"ood\"): \n",
    "        ood_lake_idx = list(set(lake_subset_idxs)-set(subset))\n",
    "        private_set =  custom_subset(true_lake_set, ood_lake_idx, torch.Tensor(np.array([split_cfg['num_cls_idc']]*len(ood_lake_idx))).float())\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = custom_subset(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    remain_true_lake_set = custom_subset(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "    print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    if(feature!=\"ood\"): assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    if(feature==\"ood\"): \n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set, private_set, lake_ss\n",
    "    else:\n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set\n",
    "                        \n",
    "def getMisclsSet(val_set, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "def getMisclsSetNumpy(X_val, y_val, val_class_err_idxs, imb_cls_idx):\n",
    "    miscls_idx = []\n",
    "    for i in range(len(val_class_err_idxs)):\n",
    "        if i in imb_cls_idx:\n",
    "            miscls_idx += val_class_err_idxs[i]\n",
    "    print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    return X_val[miscls_idx], y_val[miscls_idx]\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def getSMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, sf):\n",
    "    if(sf==\"fl1mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf == \"logdetmi\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \"  -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryqueryKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"fl2mi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries  \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"gcmi\" or sf==\"div-gcmi\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode query -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -queryKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"gccg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -gcLambda 1 -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numPrivates \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"fl1cg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numPrivates \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"logdetcg\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode private -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numPrivates \" + numQueries + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateprivateKernelFile \" + os.path.join(hdf5Path, \"smi_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf==\"fl\" or sf==\"logdet\"):\n",
    "        command = os.path.join(datkbuildPath, \"cifarSubsetSelector_ng\") + \" -mode generic -naiveOrRandom naive -logDetLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf ==\"gc\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    elif(sf ==\"dsum\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode generic -naiveOrRandom naive -gcLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -genericOptimizer \" + sf + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path,\"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    print(\"Executing SIM command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "def getCMI_ss(datkbuildPath, exePath, hdf5Path, budget, numQueries, numPrivates, sf):\n",
    "    if(sf==\"flmic\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode joint -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -numPrivates \" + numPrivates + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_private_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    if(sf == \"logdetmic\"):\n",
    "        command = os.path.join(datkbuildPath, exePath) + \" -mode joint -naiveOrRandom naive -magnificationLambda 1 -numSummaries 1 -budget \" + str(budget) + \" -queryPrivacyOptimizer \" + sf + \" -numQueries \" + numQueries + \" -numPrivates \" + numPrivates + \" -dontComputeKernel true -imageKernelFile \" + os.path.join(hdf5Path, \"smi_lake_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") +  \" -queryKernelFile \" + os.path.join(hdf5Path, \"smi_lake_target_kernel_\"+str(device).split(\":\")[1]+\".hdf5\") + \" -privateKernelFile \" + os.path.join(hdf5Path,\"smi_lake_private_kernel_\"+str(device).split(\":\")[1]+\".hdf5\")\n",
    "    \n",
    "    print(\"Executing CMI command: \", command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=True, shell=True)\n",
    "    subset = process.communicate()[0]\n",
    "    subset = subset.decode(\"utf-8\")\n",
    "    subset = subset.strip().split(\" \")\n",
    "    subset = list(map(int, subset))\n",
    "    return subset\n",
    "\n",
    "def remove_ood_points(lake_set, subset, idc_idx):\n",
    "    idx_subset = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in idc_idx:\n",
    "        idc_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        idx_subset += list(np.array(subset)[idc_subset_idx])\n",
    "    print(len(idx_subset),\"/\",len(subset), \" idc points.\")\n",
    "    return idx_subset\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n",
    "\n",
    "#check overlap with prev selections\n",
    "def check_overlap(prev_idx, prev_idx_hist, idx):\n",
    "    prev_idx = [int(x/num_rep) for x in prev_idx]\n",
    "    prev_idx_hist = [int(x/num_rep) for x in prev_idx_hist]\n",
    "    idx = [int(x/num_rep) for x in idx]\n",
    "    # overlap = set(prev_idx).intersection(set(idx))\n",
    "    overlap = [value for value in idx if value in prev_idx] \n",
    "    # overlap_hist = set(prev_idx_hist).intersection(set(idx))\n",
    "    overlap_hist = [value for value in idx if value in prev_idx_hist]\n",
    "    new_points = set(idx) - set(prev_idx_hist)\n",
    "    total_unique_points = set(idx+prev_idx_hist)\n",
    "    print(\"New unique points: \", len(new_points))\n",
    "    print(\"Total unique points: \", len(total_unique_points))\n",
    "    print(\"overlap % of sel with prev idx: \", len(overlap)/len(idx))\n",
    "    print(\"overlap % of sel with all prev idx: \", len(overlap_hist)/len(idx))\n",
    "    return len(overlap)/len(idx), len(overlap_hist)/len(idx)\n",
    "\n",
    "def getFeatures(model, dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "datadir = 'data/'\n",
    "data_name = 'breast_density'\n",
    "num_cls=4\n",
    "fraction = float(0.1)\n",
    "budget=50\n",
    "num_epochs = int(20)\n",
    "num_rep = 10\n",
    "model_name = 'MobileNetV2'\n",
    "learning_rate = 0.01\n",
    "feature = 'classimb'\n",
    "split_cfg = {\"sel_cls_idx\":[0,3], \"per_imbclass_train\":{0:35,3:40}, \"per_imbclass_val\":{0:10,3:10}, \"per_imbclass_lake\":{0:309,3:350}, \"per_class_train\":{1:93,2:73}, \"per_class_val\":{1:10,2:10}, \"per_class_lake\":{1:829,2:651}} #cifar10\n",
    "initModelPath = \"weights/\"+data_name + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_imbclass_train\"]) + \"_\" + str(split_cfg[\"per_class_train\"])\n",
    "num_runs = 1  # number of random runs\n",
    "computeClassErrorLog = True\n",
    "run=2\n",
    "magnification = 1\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "datkbuildPath = \"/home/snk170001/bioml/dss/notebooks/datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "print(\"Using Device:\", device)\n",
    "doublePrecision = True\n",
    "linearLayer = True\n",
    "handler = DataHandler_Medical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    if(feature==\"classimb\" or feature==\"ood\"):\n",
    "        if(strategy == \"SIM\" or strategy == \"SF\" or strategy==\"random\"):\n",
    "            if(strategy == \"SF\" or strategy==\"random\"):\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, True)\n",
    "            else:\n",
    "                train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, False)\n",
    "        elif(strategy==\"AL\"):\n",
    "            if(sf==\"badge\" or sf==\"us\"):\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "            else: #dont augment train with valid\n",
    "                X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True, False)\n",
    "        print(\"selected classes are: \", sel_cls_idx)\n",
    "    if(feature==\"duplicate\" or feature==\"vanilla\"):\n",
    "        sel_cls_idx = None\n",
    "        if(strategy == \"SIM\" or strategy==\"random\"):\n",
    "            train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg)\n",
    "        elif(strategy==\"AL\"):\n",
    "            X_tr, y_tr, X_val, y_val, X_unlabeled, y_unlabeled, train_set, val_set, test_set, lake_set, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, True)\n",
    "    if(feature==\"ood\"): num_cls+=1 #Add one class for OOD class\n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 10\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 10\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    val_csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "    all_logs_dir = 'SMI_active_learning_results_woVal/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_linear:'  + str(linearLayer) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \"feature\":feature, \"sel_func\":sf, \"sel_budget\":budget, \"num_selections\":num_epochs, \"model\":model_name, \"learning_rate\":learning_rate, \"setting\":split_cfg, \"all_class_acc\":None, \"test_acc\":[],\"sel_per_cls\":[], \"sel_cls_idx\":sel_cls_idx}\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device)\n",
    "    model1 = create_model(model_name, num_cls, device)\n",
    "    if(strategy == \"AL\"):\n",
    "        strategy_args = {'batch_size' : budget, 'lr':float(0.001), 'device':device}\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"glister\" or sf==\"glister-tss\"):\n",
    "            strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args, valid=True, X_val=X_val, Y_val=y_val, typeOf='rand', lam=0.1)\n",
    "        elif(sf==\"gradmatch-tss\"):\n",
    "            strategy_args = {'batch_size' : 1, 'lr':float(0.01)}\n",
    "            strategy_sel = GradMatchActive(X_tr, y_tr, X_unlabeled, model, F.cross_entropy, handler, num_cls, strategy_args[\"lr\"], \"PerBatch\", False, strategy_args, valid=True, X_val=X_val, Y_val=y_val)\n",
    "        elif(sf==\"coreset\"):\n",
    "            strategy_sel = CoreSet(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"leastconf\"):\n",
    "            strategy_sel = LeastConfidence(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "        elif(sf==\"margin\"):\n",
    "            strategy_sel = MarginSampling(X_tr, y_tr, X_unlabeled, model, handler, num_cls, strategy_args)\n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "    optimizer, scheduler = optimizer_with_scheduler(model, 150, learning_rate)\n",
    "#     optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "    #overlap vars\n",
    "    prev_idx = None\n",
    "    prev_idx_hist = []\n",
    "    sel_hist = []\n",
    "    per_ep_overlap = []\n",
    "    overall_overlap = []\n",
    "    idx_tracker = np.array(list(range(len(lake_set))))\n",
    "    #kernels\n",
    "    train_private_kernel = []\n",
    "    train_val_kernel = []\n",
    "    pp_kernel = []\n",
    "    val_kernel = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        final_tst_predictions = []\n",
    "                        final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        if((val_correct/val_total) > best_val_acc):\n",
    "                            final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                            final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    if((val_correct/val_total) > best_val_acc):\n",
    "                        best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "                continue\n",
    "        else:\n",
    "#             if(full_trn_acc[i-1] >= 0.99): #The model has already trained on the seed dataset\n",
    "            #use misclassifications on validation set as queries\n",
    "            #compute hypothesized labels\n",
    "            hyp_lake_labels = []\n",
    "            for batch_idx, (inputs, _) in enumerate(lakeloader):\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                hyp_lake_labels += list(predicted)\n",
    "            lake_set = custom_subset(lake_set, list(range(len(hyp_lake_labels))), torch.Tensor(hyp_lake_labels))\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "#             sys.exit()\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append(tst_err_log)\n",
    "                val_csvlog.append(val_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\" or strategy==\"SF\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set = getMisclsSet(val_set, val_class_err_idxs, list(range(num_cls)))\n",
    "                        misclsloader = torch.utils.data.DataLoader(miscls_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, misclsloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                elif(sf.endswith(\"cg\")): #atleast one selection must be done for private set in cond gain functions\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, privateSetloader, model1, num_cls, linearLayer, device) #set last arg to true for linear layer\n",
    "                    else:\n",
    "                        #compute subset with private set a NULL\n",
    "                        setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                elif(sf.endswith(\"mic\")): #configured for the OOD setting\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device) #In-dist samples are in Val\n",
    "                    if(len(private_set)!=0):\n",
    "                        privateSetloader = torch.utils.data.DataLoader(private_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "                        setf_model_private = DataSelectionStrategy(privateSetloader, privateSetloader, model1, num_cls, linearLayer, device)\n",
    "                else:\n",
    "                    setf_model = DataSelectionStrategy(lakeloader, valloader, model1, num_cls, linearLayer, device)\n",
    "                start_time = time.time()\n",
    "                cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "                clone_dict = copy.deepcopy(model.state_dict())\n",
    "                #update the selection strategy model with new params for gradient computation\n",
    "                setf_model.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mic\") and len(private_set)!=0): setf_model_private.update_model(clone_dict)\n",
    "                if(sf.endswith(\"mi\")): #SMI functions need the target set gradients\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "#                     print(setf_model.grads_per_elem)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "#                     print(setf_model.val_grads_per_elem)\n",
    "                    if(doublePrecision):\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    else:\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                    numQueryPrivate = train_val_kernel.shape[1]\n",
    "                elif(sf.endswith(\"cg\")):\n",
    "                    if(len(private_set)!=0):\n",
    "                        setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                        print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                        print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                        if(doublePrecision):\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                        else:\n",
    "                            train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                        numQueryPrivate = train_val_kernel.shape[1]\n",
    "                    else:\n",
    "#                         assert(((i + 1)/select_every)==1)\n",
    "                        setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        train_val_kernel = []\n",
    "                        numQueryPrivate = 0\n",
    "                elif(sf.endswith(\"mic\")):\n",
    "                    setf_model.compute_gradients(valid=True, batch=False, perClass=False)\n",
    "                    print(\"train minibatch gradients shape \", setf_model.grads_per_elem.shape)\n",
    "                    print(\"val minibatch gradients shape \", setf_model.val_grads_per_elem.shape)\n",
    "                    if(doublePrecision):\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.val_grads_per_elem.double())#img_query_kernel\n",
    "                    else:\n",
    "                        train_val_kernel = kernel(setf_model.grads_per_elem, setf_model.val_grads_per_elem)#img_query_kernel\n",
    "                    numQuery = train_val_kernel.shape[1]\n",
    "                    if(len(private_set)!=0): \n",
    "                        setf_model_private.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                        print(\"private gradients shape: \", setf_model_private.grads_per_elem.shape)\n",
    "                        if(doublePrecision):\n",
    "                            train_private_kernel = kernel(setf_model.grads_per_elem.double(), setf_model_private.grads_per_elem.double()) #img_private_kernel\n",
    "                        else:\n",
    "                            train_private_kernel = kernel(setf_model.grads_per_elem, setf_model_private.grads_per_elem) #img_private_kernel\n",
    "                        numPrivate = train_private_kernel.shape[1]\n",
    "                    else:\n",
    "                        train_private_kernel = []\n",
    "                        numPrivate = 0\n",
    "                else: # For other submodular functions needing only image kernel\n",
    "                    setf_model.compute_gradients(valid=False, batch=False, perClass=False)\n",
    "                    numQueryPrivate = 0\n",
    "\n",
    "                kernel_time = time.time()\n",
    "                if(doublePrecision):\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem.double(), setf_model.grads_per_elem.double()) #img_img_kernel\n",
    "                else:\n",
    "                    train_kernel = kernel(setf_model.grads_per_elem, setf_model.grads_per_elem) #img_img_kernel\n",
    "                if(sf==\"logdetmi\" or sf==\"logdetcg\" or sf==\"logdetmic\"):\n",
    "                    if(sf==\"logdetcg\"):\n",
    "                        if(len(private_set)!=0):\n",
    "                            val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#private_private_kernel\n",
    "                        else:\n",
    "                            val_kernel = []\n",
    "                    if(sf==\"logdetmi\"):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                    if(sf==\"logdetmic\"):\n",
    "                        val_kernel = kernel(setf_model.val_grads_per_elem, setf_model.val_grads_per_elem)#query_query_kernel\n",
    "                        pp_kernel = kernel(setf_model_private.grads_per_elem, setf_model_private.grads_per_elem)#query_query_kernel\n",
    "                save_kernel_hdf5(train_kernel, train_val_kernel, val_kernel, train_private_kernel, pp_kernel)\n",
    "#                 else:\n",
    "#                     save_kernel_hdf5(train_kernel, train_val_kernel)\n",
    "                print(\"kernel compute time: \", time.time()-kernel_time)\n",
    "                #call the c++ exec to read kernel and compute subset of selected minibatches\n",
    "                if(sf.endswith(\"mic\")):\n",
    "                    subset = getCMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQuery), str(numPrivate), sf)\n",
    "                else:\n",
    "                    subset = getSMI_ss(datkbuildPath, exePath, os.getcwd(), budget, str(numQueryPrivate), sf)\n",
    "                print(\"True targets of subset: \", torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "                print(\"Hypothesized targets of subset: \", torch.Tensor(lake_set.targets.float())[subset])\n",
    "                model.load_state_dict(cached_state_dict)\n",
    "                if(sf.endswith(\"cg\")): #for first selection\n",
    "                    if(len(private_set)==0):\n",
    "                        private_set = custom_subset(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "                    else:\n",
    "                        private_set = getPrivateSet(lake_set, subset, private_set)\n",
    "                    print(\"size of private set: \", len(private_set))\n",
    "\n",
    "    #           temp = np.array(list(trainloader.batch_sampler))[subset] #if per batch\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\" or sf==\"gradmatch-tss\" and feature==\"classimb\"):\n",
    "                    miscls_X_val, miscls_y_val = getMisclsSetNumpy(X_val, y_val, val_class_err_idxs, sel_cls_idx)\n",
    "                    if(sf==\"glister-tss\"): strategy_sel = GLISTER(X_tr, y_tr, X_unlabeled, model, handler, num_cls, device, strategy_args, valid=True, X_val=miscls_X_val, Y_val=miscls_y_val, typeOf='rand', lam=0.1)\n",
    "                    if(sf==\"gradmatch-tss\"): strategy_sel = GradMatchActive(X_tr, y_tr, X_unlabeled, model, F.cross_entropy, handler, num_cls, strategy_args[\"lr\"], \"PerBatch\", False, strategy_args, valid=True, X_val=miscls_X_val, Y_val=miscls_y_val)\n",
    "                    print(\"reinit AL with targeted miscls samples\")\n",
    "                strategy_sel.update_model(model)\n",
    "                if(sf==\"badge\" or sf==\"glister\" or sf==\"glister-tss\" or sf==\"coreset\" or sf==\"margin\"):\n",
    "                    subset = strategy_sel.select(budget)\n",
    "                if(sf==\"us\" or sf==\"leastconf\"):\n",
    "                    subset = list(strategy_sel.select(budget).cpu().numpy())\n",
    "                if(sf==\"gradmatch-tss\"):\n",
    "                    subset = strategy_sel.select(budget, False) #Fixed weight gradmatch\n",
    "                print(len(subset), \" samples selected\")\n",
    "                \n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            if(i>1 and sf.endswith(\"cg\")):\n",
    "                per_ep, overall = check_overlap(prev_idx, prev_idx_hist, list(idx_tracker[subset]))\n",
    "                per_ep_overlap.append(per_ep)\n",
    "                overall_overlap.append(overall)\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            if(feature==\"ood\"): #remove ood points from the subset\n",
    "                subset = remove_ood_points(true_lake_set, subset, sel_cls_idx)\n",
    "            \n",
    "            if(strategy==\"AL\"):#augment train and remove from lake for AL startegies\n",
    "                X_tr = np.concatenate((X_tr, X_unlabeled[subset]), axis=0)\n",
    "                X_unlabeled = np.delete(X_unlabeled, lake_subset_idxs, axis = 0)\n",
    "                y_tr = np.concatenate((y_tr, y_unlabeled[subset]), axis = 0)\n",
    "                y_unlabeled = np.delete(y_unlabeled, lake_subset_idxs, axis = 0)\n",
    "                strategy_sel.update_data(X_tr, y_tr, X_unlabeled)\n",
    "                print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "            perClsSel = getPerClassSel(true_lake_set, lake_subset_idxs, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            prev_idx = list(idx_tracker[lake_subset_idxs])\n",
    "            prev_idx_hist += list(idx_tracker[lake_subset_idxs])\n",
    "            sel_hist.append(list(idx_tracker[lake_subset_idxs]))\n",
    "            idx_tracker = np.delete(idx_tracker, lake_subset_idxs, axis=0)\n",
    "            #augment the train_set with selected indices from the lake\n",
    "            if(feature==\"classimb\"):\n",
    "                train_set, lake_set, true_lake_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "            elif(feature==\"ood\"):\n",
    "                train_set, lake_set, true_lake_set, new_private_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "                train_set = torch.utils.data.ConcatDataset([train_set, new_private_set]) #Add the OOD samples with a common OOD class\n",
    "                val_set = torch.utils.data.ConcatDataset([val_set, add_val_set])\n",
    "                if(len(private_set)!=0):\n",
    "                    private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "                else:\n",
    "                    private_set = new_private_set\n",
    "            else:\n",
    "                train_set, lake_set, true_lake_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set))\n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "            valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, shuffle=False, pin_memory=True)\n",
    "            assert(len(idx_tracker)==len(lake_set))\n",
    "#             model =  model.apply(weight_reset).cuda()\n",
    "            model = create_model(model_name, num_cls, device)\n",
    "#             optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "            optimizer, scheduler = optimizer_with_scheduler(model, 150, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<150):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                # print(batch_idx)\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "                # sys.exit()\n",
    "\n",
    "#             if((val_correct/val_total) > best_val_acc):\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                if(feature==\"ood\"): \n",
    "                    _, predicted = outputs[...,:-1].max(1)\n",
    "                else:\n",
    "                    _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "#                 if((val_correct/val_total) > best_val_acc):\n",
    "    #                 if(i == (num_epochs-1)):\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "#             if((val_correct/val_total) > best_val_acc):\n",
    "#                 best_val_acc = (val_correct/val_total)\n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append(tst_err_log)\n",
    "        val_csvlog.append(val_err_log)\n",
    "        print(csvlog)\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "        res_dict[\"all_val_class_acc\"] = val_csvlog\n",
    "#         with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "#             writer = csv.writer(f)\n",
    "#             writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)\n",
    "    return tst_acc, csvlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL2MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM fl2mi\n",
      "Breast-density Custom dataset stats: Train size:  241 Val size:  40 Lake size:  2139\n",
      "selected classes are:  [0, 3]\n",
      "Saving results to:  SMI_active_learning_results_woVal/breast_density/classimb/fl2mi/50/1\n",
      "breast_density_classimb_SIM_2_fl2mi_budget:50_epochs:20_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/breast_density_MobileNetV2_0.01_{0: 35, 3: 40}_{1: 93, 2: 73}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  100.0 89.23\n",
      "val, test error% for class  1  :  60.0 46.15\n",
      "val, test error% for class  2  :  70.0 72.64\n",
      "val, test error% for class  3  :  90.0 86.73\n",
      "total misclassified ex from imb classes:  32\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2139, 5124])\n",
      "val minibatch gradients shape  torch.Size([32, 5124])\n",
      "kernel compute time:  1.3711655139923096\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  32 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 1., 2., 0., 1., 1., 0., 1., 3., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        2., 1., 0., 2., 2., 1., 3., 1., 2., 1., 3., 2., 1., 2., 1., 1., 1., 2.,\n",
      "        1., 1., 2., 2., 1., 3., 1., 1., 2., 2., 1., 2., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 0., 1., 3., 1., 1., 3., 3., 1., 1., 1., 2., 0., 1., 3., 0., 3., 3.,\n",
      "        1., 2., 1., 1., 1., 1., 1., 2., 2., 0., 2., 1., 1., 2., 1., 3., 1., 3.,\n",
      "        3., 1., 3., 1., 2., 1., 1., 1., 0., 1., 1., 2., 1., 1.])\n",
      "50 2089 2139\n",
      "After augmentation, size of train_set:  291  lake set:  2089\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 27.86268012225628 0.5876288659793815 4.13439404964447 0.55 68.80152037739754 0.5336463223787168 2044.8381791114807\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  40.0 67.69\n",
      "val, test error% for class  1  :  60.0 33.08\n",
      "val, test error% for class  2  :  30.0 53.73\n",
      "val, test error% for class  3  :  50.0 53.1\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2089, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  1.1600043773651123\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  18 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 0., 2., 3., 1., 1., 2., 3., 2., 1., 2., 2., 1., 2., 1., 3., 3., 0.,\n",
      "        1., 0., 3., 3., 2., 2., 1., 2., 1., 1., 3., 1., 0., 0., 0., 2., 1., 0.,\n",
      "        2., 1., 3., 2., 3., 1., 2., 2., 2., 2., 2., 1., 2., 3.])\n",
      "Hypothesized targets of subset:  tensor([2., 1., 0., 2., 1., 1., 2., 2., 2., 1., 1., 2., 1., 1., 2., 2., 3., 1.,\n",
      "        0., 2., 3., 2., 1., 1., 0., 1., 3., 2., 2., 1., 2., 1., 0., 1., 2., 1.,\n",
      "        2., 1., 2., 1., 3., 1., 2., 1., 1., 1., 2., 1., 1., 2.])\n",
      "50 2039 2089\n",
      "After augmentation, size of train_set:  341  lake set:  2039\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 33.75607192516327 0.5659824046920822 3.851499855518341 0.55 65.6095033288002 0.5320813771517997 2330.3483204841614\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  50.0 63.08\n",
      "val, test error% for class  1  :  40.0 41.54\n",
      "val, test error% for class  2  :  30.0 43.28\n",
      "val, test error% for class  3  :  60.0 55.75\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2039, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.9377024173736572\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  18 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 1., 1., 3., 1., 2., 0., 1., 2., 2., 2., 1., 1., 0., 2., 0., 3., 2.,\n",
      "        1., 2., 1., 1., 3., 3., 1., 1., 0., 1., 2., 2., 2., 0., 1., 2., 2., 3.,\n",
      "        2., 3., 2., 1., 3., 0., 3., 1., 2., 0., 2., 2., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 2., 1., 1., 1., 1., 0., 1., 2., 2., 2., 1., 1., 0., 3., 0., 3., 3.,\n",
      "        1., 1., 1., 0., 2., 2., 2., 2., 0., 1., 3., 1., 2., 0., 1., 1., 1., 2.,\n",
      "        1., 3., 1., 1., 2., 0., 3., 1., 1., 0., 3., 1., 2., 1.])\n",
      "50 1989 2039\n",
      "After augmentation, size of train_set:  391  lake set:  1989\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 37.96187925338745 0.5677749360613811 3.5557801723480225 0.675 66.111093044281 0.5226917057902973 2677.7694840431213\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  40.0 46.15\n",
      "val, test error% for class  1  :  0.0 42.31\n",
      "val, test error% for class  2  :  40.0 54.73\n",
      "val, test error% for class  3  :  50.0 48.67\n",
      "total misclassified ex from imb classes:  13\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1989, 5124])\n",
      "val minibatch gradients shape  torch.Size([13, 5124])\n",
      "kernel compute time:  0.9656264781951904\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  13 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 3., 3., 2., 1., 2., 2., 2., 3., 2., 0., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        3., 1., 3., 3., 2., 1., 3., 3., 1., 2., 1., 2., 2., 3., 2., 1., 2., 2.,\n",
      "        2., 3., 3., 0., 2., 1., 3., 1., 2., 2., 2., 2., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 2., 3., 2., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 3., 2., 3., 2., 0., 2., 0., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 3., 0., 2., 2., 2., 0., 2., 0., 2., 2., 2., 2.])\n",
      "50 1939 1989\n",
      "After augmentation, size of train_set:  441  lake set:  1939\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 41.590670585632324 0.5804988662131519 3.587135136127472 0.6 67.50022011995316 0.513302034428795 3036.11589384079\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  30.0 52.31\n",
      "val, test error% for class  1  :  30.0 47.31\n",
      "val, test error% for class  2  :  40.0 52.24\n",
      "val, test error% for class  3  :  60.0 43.36\n",
      "total misclassified ex from imb classes:  16\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1939, 5124])\n",
      "val minibatch gradients shape  torch.Size([16, 5124])\n",
      "kernel compute time:  0.9999704360961914\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  16 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 2., 1., 1., 2., 1., 1., 3., 2., 1., 3., 1., 1., 1., 0., 0., 2., 2.,\n",
      "        1., 3., 1., 3., 1., 2., 2., 1., 1., 2., 3., 1., 2., 1., 1., 0., 2., 0.,\n",
      "        1., 1., 2., 1., 3., 1., 2., 2., 3., 2., 3., 2., 2., 2.])\n",
      "Hypothesized targets of subset:  tensor([3., 2., 1., 1., 2., 1., 1., 2., 1., 1., 2., 1., 1., 1., 1., 1., 2., 2.,\n",
      "        1., 3., 2., 3., 2., 1., 2., 2., 0., 3., 3., 1., 1., 1., 2., 2., 2., 1.,\n",
      "        1., 0., 1., 1., 3., 3., 2., 2., 3., 3., 2., 1., 3., 1.])\n",
      "50 1889 1939\n",
      "After augmentation, size of train_set:  491  lake set:  1889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 46.147663712501526 0.6109979633401222 3.8206467032432556 0.525 65.77283203601837 0.5273865414710485 3360.9063975811005\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  50.0 40.0\n",
      "val, test error% for class  1  :  50.0 49.23\n",
      "val, test error% for class  2  :  70.0 51.24\n",
      "val, test error% for class  3  :  20.0 39.82\n",
      "total misclassified ex from imb classes:  19\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1889, 5124])\n",
      "val minibatch gradients shape  torch.Size([19, 5124])\n",
      "kernel compute time:  0.8892714977264404\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  19 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 1., 0., 1., 1., 1., 1., 0., 1., 1., 3., 2., 2., 3., 1., 0., 1., 1.,\n",
      "        1., 3., 1., 0., 1., 1., 1., 3., 0., 3., 1., 2., 1., 1., 1., 2., 1., 1.,\n",
      "        1., 2., 2., 3., 1., 1., 2., 0., 1., 1., 2., 0., 1., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 1., 2., 1., 1., 2., 1., 0., 1., 1., 3., 2., 1., 3., 1., 2., 1., 1.,\n",
      "        2., 2., 1., 0., 1., 1., 1., 2., 1., 1., 0., 1., 1., 1., 1., 0., 1., 2.,\n",
      "        1., 2., 1., 1., 1., 0., 2., 0., 1., 1., 1., 1., 1., 1.])\n",
      "50 1839 1889\n",
      "After augmentation, size of train_set:  541  lake set:  1839\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 49.95937329530716 0.5804066543438078 3.750705987215042 0.575 66.89322239160538 0.48826291079812206 3723.3484823703766\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  50.0 47.69\n",
      "val, test error% for class  1  :  40.0 50.38\n",
      "val, test error% for class  2  :  80.0 68.16\n",
      "val, test error% for class  3  :  0.0 24.78\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1839, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n",
      "kernel compute time:  0.8681814670562744\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  17 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 1., 0., 2., 1., 1., 0., 3., 0., 0., 2., 0., 1., 1., 1., 0., 2., 3.,\n",
      "        3., 1., 0., 2., 3., 1., 0., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 2., 1., 1., 0., 2., 1., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 1., 0., 2., 1., 0., 2., 0., 0., 2., 1., 1., 2., 2., 0., 1., 2.,\n",
      "        2., 1., 0., 1., 2., 1., 0., 2., 0., 1., 1., 2., 1., 0., 1., 2., 1., 1.,\n",
      "        2., 1., 0., 1., 0., 0., 1., 2., 2., 0., 1., 2., 0., 1.])\n",
      "50 1789 1839\n",
      "After augmentation, size of train_set:  591  lake set:  1789\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 52.67280238866806 0.6074450084602369 3.3212957978248596 0.625 64.67636513710022 0.5633802816901409 4080.5872745513916\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  60.0 49.23\n",
      "val, test error% for class  1  :  20.0 36.54\n",
      "val, test error% for class  2  :  40.0 53.23\n",
      "val, test error% for class  3  :  30.0 39.82\n",
      "total misclassified ex from imb classes:  15\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1789, 5124])\n",
      "val minibatch gradients shape  torch.Size([15, 5124])\n",
      "kernel compute time:  0.9608345031738281\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  15 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 1., 3., 1., 0., 0., 1., 1., 3., 1., 2., 1., 2., 1., 2., 3., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 2., 1., 1., 3., 0., 1., 1., 2., 1.,\n",
      "        2., 3., 0., 0., 3., 0., 0., 2., 2., 2., 1., 0., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([3., 3., 3., 1., 1., 1., 1., 1., 3., 0., 0., 1., 1., 1., 2., 3., 0., 0.,\n",
      "        1., 1., 1., 2., 2., 3., 1., 1., 1., 3., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
      "        3., 2., 1., 1., 1., 0., 0., 3., 2., 2., 1., 0., 3., 1.])\n",
      "50 1739 1789\n",
      "After augmentation, size of train_set:  641  lake set:  1739\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 54.152642637491226 0.6583463338533542 3.6106818318367004 0.55 63.668279469013214 0.5179968701095462 4428.193833112717\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  50.0 44.62\n",
      "val, test error% for class  1  :  30.0 46.54\n",
      "val, test error% for class  2  :  70.0 58.21\n",
      "val, test error% for class  3  :  30.0 36.28\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1739, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.7574284076690674\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  18 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 1., 0., 2., 2., 1., 3., 2., 1., 3., 2., 3., 3., 2., 2., 1.,\n",
      "        1., 3., 2., 2., 0., 2., 1., 1., 1., 2., 3., 0., 0., 2., 1., 2., 0., 1.,\n",
      "        2., 3., 2., 3., 1., 1., 2., 2., 1., 3., 2., 2., 0., 3.])\n",
      "Hypothesized targets of subset:  tensor([2., 2., 3., 1., 0., 2., 2., 1., 3., 2., 2., 2., 2., 3., 3., 2., 1., 1.,\n",
      "        0., 2., 2., 2., 0., 2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 3., 0., 2.,\n",
      "        2., 1., 1., 2., 2., 1., 2., 3., 2., 2., 1., 2., 2., 2.])\n",
      "50 1689 1739\n",
      "After augmentation, size of train_set:  691  lake set:  1689\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 77.69575127959251 0.5253256150506512 4.64147475361824 0.5 83.26294085383415 0.42566510172143973 4756.990800619125\n",
      "AL epoch:  10\n",
      "val, test error% for class  0  :  20.0 13.85\n",
      "val, test error% for class  1  :  60.0 80.77\n",
      "val, test error% for class  2  :  90.0 60.7\n",
      "val, test error% for class  3  :  30.0 23.01\n",
      "total misclassified ex from imb classes:  20\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1689, 5124])\n",
      "val minibatch gradients shape  torch.Size([20, 5124])\n",
      "kernel compute time:  0.8108854293823242\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  20 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 2., 1., 1., 1., 0., 2., 1., 1., 2., 2., 1., 2., 1., 3., 2., 0., 1.,\n",
      "        1., 1., 2., 2., 2., 1., 1., 3., 1., 3., 2., 1., 2., 2., 2., 0., 2., 2.,\n",
      "        3., 2., 2., 2., 1., 1., 1., 3., 2., 2., 3., 2., 1., 0.])\n",
      "Hypothesized targets of subset:  tensor([0., 3., 3., 3., 2., 0., 1., 1., 1., 3., 3., 3., 1., 0., 3., 2., 0., 0.,\n",
      "        2., 1., 2., 3., 2., 2., 3., 1., 2., 3., 0., 0., 2., 3., 1., 0., 3., 2.,\n",
      "        2., 3., 2., 3., 0., 0., 1., 3., 1., 3., 2., 2., 0., 0.])\n",
      "50 1639 1689\n",
      "After augmentation, size of train_set:  741  lake set:  1639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 58.79530096054077 0.6423751686909581 2.969281256198883 0.65 62.522213846445084 0.5446009389671361 5104.128470659256\n",
      "AL epoch:  11\n",
      "val, test error% for class  0  :  30.0 27.69\n",
      "val, test error% for class  1  :  30.0 48.46\n",
      "val, test error% for class  2  :  50.0 52.24\n",
      "val, test error% for class  3  :  30.0 37.17\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1639, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.7328085899353027\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  14 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 0., 1., 1., 2., 3., 2., 2., 3., 1., 3., 3., 1., 2., 2., 0., 0., 3.,\n",
      "        2., 0., 2., 1., 1., 3., 0., 2., 1., 2., 3., 2., 1., 2., 3., 0., 3., 0.,\n",
      "        2., 3., 0., 1., 2., 1., 3., 3., 3., 2., 1., 3., 1., 0.])\n",
      "Hypothesized targets of subset:  tensor([1., 1., 2., 1., 1., 2., 2., 2., 2., 1., 2., 2., 1., 2., 2., 0., 0., 3.,\n",
      "        2., 0., 1., 2., 3., 0., 1., 2., 3., 2., 3., 1., 1., 1., 3., 0., 3., 0.,\n",
      "        2., 3., 1., 1., 3., 1., 3., 3., 3., 1., 2., 3., 1., 1.])\n",
      "50 1589 1639\n",
      "After augmentation, size of train_set:  791  lake set:  1589\n",
      "Epoch: 12 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 69.05578449368477 0.629582806573957 3.3933841586112976 0.625 64.4078258574009 0.5164319248826291 5435.126485824585\n",
      "AL epoch:  12\n",
      "val, test error% for class  0  :  40.0 33.85\n",
      "val, test error% for class  1  :  20.0 55.77\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  40.0 37.17\n",
      "total misclassified ex from imb classes:  15\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1589, 5124])\n",
      "val minibatch gradients shape  torch.Size([15, 5124])\n",
      "kernel compute time:  0.7247300148010254\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  15 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 3., 2., 2., 2., 1., 0., 2., 3., 1., 2., 2., 3., 2., 0., 1., 2., 2.,\n",
      "        3., 2., 2., 2., 3., 2., 2., 0., 3., 2., 1., 0., 0., 2., 1., 3., 2., 2.,\n",
      "        2., 2., 1., 2., 0., 2., 1., 2., 3., 2., 1., 2., 1., 2.])\n",
      "Hypothesized targets of subset:  tensor([2., 3., 2., 2., 1., 0., 0., 3., 3., 1., 1., 0., 3., 2., 0., 1., 2., 2.,\n",
      "        3., 2., 3., 2., 3., 2., 2., 0., 3., 2., 1., 1., 0., 2., 0., 3., 2., 3.,\n",
      "        1., 3., 1., 2., 0., 2., 2., 2., 3., 2., 1., 2., 0., 1.])\n",
      "50 1539 1589\n",
      "After augmentation, size of train_set:  841  lake set:  1539\n",
      "Epoch: 13 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 67.90159357339144 0.6349583828775267 3.421257734298706 0.575 64.05571433901787 0.5633802816901409 5768.296828508377\n",
      "AL epoch:  13\n",
      "val, test error% for class  0  :  50.0 43.08\n",
      "val, test error% for class  1  :  50.0 48.08\n",
      "val, test error% for class  2  :  50.0 38.31\n",
      "val, test error% for class  3  :  20.0 43.36\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1539, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n",
      "kernel compute time:  0.715965747833252\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  17 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 0., 1., 2., 1., 1., 3., 1., 2., 3., 1., 0., 1., 1., 3., 2., 1., 2.,\n",
      "        1., 0., 1., 1., 3., 0., 2., 2., 1., 0., 2., 1., 3., 1., 1., 0., 2., 1.,\n",
      "        2., 1., 1., 2., 1., 1., 1., 0., 1., 1., 1., 0., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 0., 1., 1., 1., 1., 2., 1., 1., 3., 1., 1., 0., 1., 2., 1., 1., 2.,\n",
      "        0., 0., 1., 1., 2., 2., 2., 1., 1., 0., 1., 1., 2., 1., 1., 1., 2., 1.,\n",
      "        2., 1., 2., 2., 1., 1., 0., 1., 1., 1., 1., 1., 2., 1.])\n",
      "50 1489 1539\n",
      "After augmentation, size of train_set:  891  lake set:  1489\n",
      "Epoch: 14 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 73.57923114299774 0.6442199775533108 2.8103349804878235 0.675 66.80489307641983 0.5414710485133021 6157.625535964966\n",
      "AL epoch:  14\n",
      "val, test error% for class  0  :  30.0 26.15\n",
      "val, test error% for class  1  :  30.0 53.08\n",
      "val, test error% for class  2  :  50.0 47.26\n",
      "val, test error% for class  3  :  20.0 38.05\n",
      "total misclassified ex from imb classes:  13\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1489, 5124])\n",
      "val minibatch gradients shape  torch.Size([13, 5124])\n",
      "kernel compute time:  0.6570744514465332\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  13 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 2., 2., 1., 2., 1., 1., 2., 0., 2., 3., 1., 1., 0., 1., 1., 1., 2.,\n",
      "        1., 0., 3., 1., 0., 3., 2., 0., 3., 2., 1., 3., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 2., 3., 0., 1., 0., 2., 3., 2., 1., 2., 3., 1., 1.])\n",
      "Hypothesized targets of subset:  tensor([0., 2., 1., 0., 2., 3., 0., 2., 0., 2., 3., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 3., 0., 0., 3., 1., 0., 2., 1., 0., 1., 0., 0., 1., 2., 2., 2.,\n",
      "        2., 2., 3., 0., 1., 0., 2., 2., 3., 1., 1., 2., 2., 1.])\n",
      "50 1439 1489\n",
      "After augmentation, size of train_set:  941  lake set:  1439\n",
      "Epoch: 15 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 76.83698447048664 0.6567481402763018 2.638527035713196 0.7 65.41551607847214 0.5446009389671361 6542.210714817047\n",
      "AL epoch:  15\n",
      "val, test error% for class  0  :  30.0 26.15\n",
      "val, test error% for class  1  :  40.0 51.54\n",
      "val, test error% for class  2  :  30.0 47.26\n",
      "val, test error% for class  3  :  20.0 39.82\n",
      "total misclassified ex from imb classes:  12\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1439, 5124])\n",
      "val minibatch gradients shape  torch.Size([12, 5124])\n",
      "kernel compute time:  0.5826261043548584\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  12 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 2., 2., 3., 2., 2., 2., 3., 1., 1., 2., 2., 3., 2., 0., 2., 3., 2.,\n",
      "        2., 1., 2., 0., 2., 1., 1., 3., 3., 1., 1., 3., 2., 2., 2., 3., 1., 2.,\n",
      "        1., 1., 0., 2., 1., 2., 3., 1., 1., 2., 1., 1., 2., 2.])\n",
      "Hypothesized targets of subset:  tensor([1., 1., 3., 3., 3., 3., 1., 2., 1., 2., 2., 3., 3., 2., 0., 2., 3., 3.,\n",
      "        2., 0., 2., 1., 1., 2., 1., 3., 3., 2., 1., 3., 2., 1., 2., 3., 1., 3.,\n",
      "        1., 1., 0., 1., 1., 1., 3., 2., 2., 2., 2., 1., 2., 1.])\n",
      "50 1389 1439\n",
      "After augmentation, size of train_set:  991  lake set:  1389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 83.55153679847717 0.6488395560040363 2.571258306503296 0.725 60.925535798072815 0.5727699530516432 6892.166962862015\n",
      "AL epoch:  16\n",
      "val, test error% for class  0  :  30.0 24.62\n",
      "val, test error% for class  1  :  20.0 43.46\n",
      "val, test error% for class  2  :  40.0 50.25\n",
      "val, test error% for class  3  :  20.0 38.05\n",
      "total misclassified ex from imb classes:  11\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1389, 5124])\n",
      "val minibatch gradients shape  torch.Size([11, 5124])\n",
      "kernel compute time:  0.5189847946166992\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  11 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 1., 2., 3., 1., 1., 1., 2., 1., 1., 2., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 3., 1., 3., 1., 1., 0., 1., 1., 1., 2., 2., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 2., 1., 0., 0., 1., 0., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 1., 2., 1., 2., 1., 2., 1., 2., 1., 0., 1., 1., 1., 0., 0., 0., 2.,\n",
      "        1., 1., 1., 1., 3., 3., 2., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 3., 2., 1., 0., 1., 2., 3., 1.])\n",
      "50 1339 1389\n",
      "After augmentation, size of train_set:  1041  lake set:  1339\n",
      "Epoch: 17 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 85.00867286324501 0.6695485110470701 2.526817202568054 0.7 66.21687939763069 0.5414710485133021 7207.660317182541\n",
      "AL epoch:  17\n",
      "val, test error% for class  0  :  10.0 26.15\n",
      "val, test error% for class  1  :  40.0 53.46\n",
      "val, test error% for class  2  :  50.0 48.76\n",
      "val, test error% for class  3  :  20.0 34.51\n",
      "total misclassified ex from imb classes:  12\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1339, 5124])\n",
      "val minibatch gradients shape  torch.Size([12, 5124])\n",
      "kernel compute time:  0.4206418991088867\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  12 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 0., 2., 3., 1., 2., 1., 3., 1., 2., 0., 3., 2., 1., 1., 0., 3., 1.,\n",
      "        2., 2., 1., 3., 3., 2., 2., 1., 1., 0., 1., 3., 1., 3., 2., 2., 3., 3.,\n",
      "        1., 1., 1., 3., 2., 2., 1., 0., 2., 2., 0., 1., 0., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 1., 1., 1., 2., 1., 1., 3., 1., 2., 1., 2., 2., 1., 0., 1., 3., 1.,\n",
      "        2., 1., 1., 3., 2., 3., 1., 1., 1., 0., 2., 3., 0., 1., 1., 2., 3., 3.,\n",
      "        2., 0., 1., 2., 2., 1., 1., 1., 1., 2., 0., 2., 1., 1.])\n",
      "50 1289 1339\n",
      "After augmentation, size of train_set:  1091  lake set:  1289\n",
      "Epoch: 18 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 88.88071474432945 0.6498625114573785 2.710372745990753 0.625 66.46289533376694 0.5461658841940532 7504.785460948944\n",
      "AL epoch:  18\n",
      "val, test error% for class  0  :  40.0 26.15\n",
      "val, test error% for class  1  :  50.0 50.38\n",
      "val, test error% for class  2  :  40.0 47.26\n",
      "val, test error% for class  3  :  20.0 41.59\n",
      "total misclassified ex from imb classes:  15\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1289, 5124])\n",
      "val minibatch gradients shape  torch.Size([15, 5124])\n",
      "kernel compute time:  0.42630767822265625\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  15 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 2., 1., 2., 1., 1., 1., 3., 1., 3., 1., 1., 2., 2., 2., 1., 2., 3.,\n",
      "        3., 3., 1., 2., 0., 1., 0., 1., 2., 1., 3., 1., 1., 1., 2., 1., 2., 2.,\n",
      "        2., 3., 1., 2., 1., 1., 3., 2., 2., 2., 2., 1., 1., 2.])\n",
      "Hypothesized targets of subset:  tensor([1., 1., 1., 2., 1., 1., 0., 1., 0., 3., 1., 1., 1., 0., 0., 1., 0., 2.,\n",
      "        2., 1., 1., 2., 0., 1., 1., 1., 2., 0., 3., 0., 1., 1., 1., 1., 2., 1.,\n",
      "        1., 2., 0., 2., 2., 1., 2., 1., 1., 1., 1., 1., 2., 1.])\n",
      "50 1239 1289\n",
      "After augmentation, size of train_set:  1141  lake set:  1239\n",
      "Epoch: 19 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 93.14609250426292 0.6310254163014899 3.0461188554763794 0.675 62.299234718084335 0.5790297339593115 7857.197120428085\n",
      "AL epoch:  19\n",
      "val, test error% for class  0  :  50.0 29.23\n",
      "val, test error% for class  1  :  20.0 37.69\n",
      "val, test error% for class  2  :  50.0 48.76\n",
      "val, test error% for class  3  :  10.0 47.79\n",
      "total misclassified ex from imb classes:  13\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1239, 5124])\n",
      "val minibatch gradients shape  torch.Size([13, 5124])\n",
      "kernel compute time:  0.4061605930328369\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector -mode query -naiveOrRandom naive -queryDiversityLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer fl2mi -numQueries  13 -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 2., 1., 2., 0., 3., 1., 1., 2., 2., 1., 1., 2., 2., 2., 3., 3., 0.,\n",
      "        3., 1., 3., 1., 3., 1., 1., 2., 1., 3., 2., 2., 2., 1., 0., 3., 2., 2.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 2., 1., 3., 1., 2., 2., 3.])\n",
      "Hypothesized targets of subset:  tensor([3., 1., 2., 2., 1., 3., 2., 3., 2., 2., 0., 1., 3., 0., 2., 2., 2., 0.,\n",
      "        2., 1., 2., 0., 2., 0., 2., 2., 1., 3., 2., 2., 2., 1., 1., 3., 1., 1.,\n",
      "        1., 1., 1., 2., 1., 0., 1., 2., 1., 2., 2., 1., 3., 3.])\n",
      "50 1189 1239\n",
      "After augmentation, size of train_set:  1191  lake set:  1189\n",
      "Epoch: 20 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 97.14789444208145 0.6599496221662469 2.6139341592788696 0.675 60.89069080352783 0.5727699530516432 8121.787153244019\n",
      "val, test error% for class  0  :  40.0 29.23\n",
      "val, test error% for class  1  :  40.0 50.77\n",
      "val, test error% for class  2  :  40.0 43.28\n",
      "val, test error% for class  3  :  10.0 30.97\n",
      "[[89.23, 46.15, 72.64, 86.73, 73.6875], [67.69, 33.08, 53.73, 53.1, 51.9], [63.08, 41.54, 43.28, 55.75, 50.9125], [46.15, 42.31, 54.73, 48.67, 47.965], [52.31, 47.31, 52.24, 43.36, 48.80500000000001], [40.0, 49.23, 51.24, 39.82, 45.0725], [47.69, 50.38, 68.16, 24.78, 47.7525], [49.23, 36.54, 53.23, 39.82, 44.705], [44.62, 46.54, 58.21, 36.28, 46.4125], [13.85, 80.77, 60.7, 23.01, 44.582499999999996], [27.69, 48.46, 52.24, 37.17, 41.39], [33.85, 55.77, 49.75, 37.17, 44.135000000000005], [43.08, 48.08, 38.31, 43.36, 43.207499999999996], [26.15, 53.08, 47.26, 38.05, 41.13499999999999], [26.15, 51.54, 47.26, 39.82, 41.192499999999995], [24.62, 43.46, 50.25, 38.05, 39.095], [26.15, 53.46, 48.76, 34.51, 40.72], [26.15, 50.38, 47.26, 41.59, 41.345], [29.23, 37.69, 48.76, 47.79, 40.8675], [29.23, 50.77, 43.28, 30.97, 38.5625]]\n"
     ]
    }
   ],
   "source": [
    "fl2mi_tst, fl2mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL1MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl1mi_tst, fl1mi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL badge\n",
      "Breast-density Custom dataset stats: Train size:  241 Val size:  40 Lake size:  2139\n",
      "selected classes are:  [0, 3]\n",
      "Saving results to:  SMI_active_learning_results_woVal/breast_density/classimb/badge/50/1\n",
      "breast_density_classimb_AL_2_badge_budget:50_epochs:20_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/breast_density_MobileNetV2_0.01_{0: 35, 3: 40}_{1: 93, 2: 73}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  70.0 89.23\n",
      "val, test error% for class  1  :  30.0 46.15\n",
      "val, test error% for class  2  :  60.0 72.64\n",
      "val, test error% for class  3  :  90.0 86.73\n",
      "50  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-04-29 01:39:07.631415\n",
      "50 2089 2139\n",
      "After augmentation, size of train_set:  291  lake set:  2089\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 30.458039104938507 0.5567010309278351 4.094869375228882 0.45 65.4229965209961 0.5352112676056338 2034.5918307304382\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  70.0 61.54\n",
      "val, test error% for class  1  :  40.0 36.15\n",
      "val, test error% for class  2  :  50.0 56.72\n",
      "val, test error% for class  3  :  60.0 43.36\n",
      "50  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-04-29 02:14:13.117995\n",
      "50 2039 2089\n",
      "After augmentation, size of train_set:  341  lake set:  2039\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 35.69442528486252 0.5630498533724341 4.182897567749023 0.5 72.20450893044472 0.48826291079812206 2395.688277721405\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  50.0 61.54\n",
      "val, test error% for class  1  :  50.0 50.0\n",
      "val, test error% for class  2  :  70.0 61.19\n",
      "val, test error% for class  3  :  30.0 30.09\n",
      "50  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-04-29 02:55:21.708782\n",
      "50 1989 2039\n",
      "After augmentation, size of train_set:  391  lake set:  1989\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 38.19333553314209 0.5601023017902813 3.8976882696151733 0.525 66.81772029399872 0.49139280125195617 2752.5110943317413\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  50.0 69.23\n",
      "val, test error% for class  1  :  30.0 35.38\n",
      "val, test error% for class  2  :  50.0 67.66\n",
      "val, test error% for class  3  :  60.0 46.02\n",
      "50  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-04-29 03:42:24.638095\n",
      "50 1939 1989\n",
      "After augmentation, size of train_set:  441  lake set:  1939\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 45.16496151685715 0.5850340136054422 2.9108182191848755 0.675 69.03265905380249 0.4788732394366197 3075.2196559906006\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  30.0 47.69\n",
      "val, test error% for class  1  :  20.0 46.54\n",
      "val, test error% for class  2  :  60.0 66.17\n",
      "val, test error% for class  3  :  20.0 42.48\n",
      "50  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-04-29 04:34:48.530508\n",
      "50 1889 1939\n",
      "After augmentation, size of train_set:  491  lake set:  1889\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 45.582299172878265 0.5763747454175153 2.9563693702220917 0.65 64.32108694314957 0.5195618153364632 3500.3049857616425\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  30.0 36.92\n",
      "val, test error% for class  1  :  30.0 48.08\n",
      "val, test error% for class  2  :  60.0 61.69\n",
      "val, test error% for class  3  :  20.0 30.09\n",
      "50  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-04-29 05:34:17.626510\n",
      "50 1839 1889\n",
      "After augmentation, size of train_set:  541  lake set:  1839\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 54.212572157382965 0.5489833641404805 3.6625828742980957 0.625 67.14271855354309 0.5258215962441315 3849.338066339493\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  50.0 44.62\n",
      "val, test error% for class  1  :  0.0 35.77\n",
      "val, test error% for class  2  :  50.0 56.72\n",
      "val, test error% for class  3  :  50.0 59.29\n",
      "50  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-04-29 06:39:32.150667\n",
      "50 1789 1839\n",
      "After augmentation, size of train_set:  591  lake set:  1789\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 55.149486005306244 0.6159052453468697 3.565131366252899 0.6 65.18988198041916 0.5211267605633803 4152.845265626907\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  70.0 58.46\n",
      "val, test error% for class  1  :  20.0 35.38\n",
      "val, test error% for class  2  :  50.0 63.18\n",
      "val, test error% for class  3  :  20.0 43.36\n",
      "50  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-04-29 07:49:51.898622\n",
      "50 1739 1789\n",
      "After augmentation, size of train_set:  641  lake set:  1739\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 61.51634752750397 0.5694227769110765 3.7381849884986877 0.625 64.13758844137192 0.5289514866979655 4532.209519863129\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  50.0 60.0\n",
      "val, test error% for class  1  :  40.0 31.54\n",
      "val, test error% for class  2  :  20.0 54.73\n",
      "val, test error% for class  3  :  40.0 61.95\n",
      "50  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-04-29 09:06:28.153183\n",
      "50 1689 1739\n",
      "After augmentation, size of train_set:  691  lake set:  1689\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 65.25010514259338 0.5774240231548481 2.8533623218536377 0.6 65.61845135688782 0.539906103286385 4853.505992889404\n",
      "AL epoch:  10\n",
      "val, test error% for class  0  :  40.0 38.46\n",
      "val, test error% for class  1  :  30.0 36.54\n",
      "val, test error% for class  2  :  60.0 62.69\n",
      "val, test error% for class  3  :  30.0 42.48\n",
      "50  samples selected\n",
      "selEpoch: 10, Selection Ended at: 2021-04-29 10:28:24.664172\n",
      "50 1639 1689\n",
      "After augmentation, size of train_set:  741  lake set:  1639\n",
      "Epoch: 11 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 70.85625946521759 0.5897435897435898 3.4107341170310974 0.55 63.10196256637573 0.5492957746478874 5230.710395336151\n",
      "AL epoch:  11\n",
      "val, test error% for class  0  :  70.0 64.62\n",
      "val, test error% for class  1  :  50.0 35.0\n",
      "val, test error% for class  2  :  40.0 60.7\n",
      "val, test error% for class  3  :  20.0 29.2\n",
      "50  samples selected\n",
      "selEpoch: 11, Selection Ended at: 2021-04-29 11:56:37.197597\n",
      "50 1589 1639\n",
      "After augmentation, size of train_set:  791  lake set:  1589\n",
      "Epoch: 12 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 126.99647498130798 0.39570164348925413 6.606125712394714 0.45 89.33202186226845 0.3755868544600939 5587.733902454376\n",
      "AL epoch:  12\n",
      "val, test error% for class  0  :  10.0 16.92\n",
      "val, test error% for class  1  :  50.0 53.46\n",
      "val, test error% for class  2  :  100.0 92.54\n",
      "val, test error% for class  3  :  60.0 55.75\n",
      "50  samples selected\n",
      "selEpoch: 12, Selection Ended at: 2021-04-29 13:30:45.785902\n",
      "50 1539 1589\n",
      "After augmentation, size of train_set:  841  lake set:  1539\n",
      "Epoch: 13 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 78.67168122529984 0.5897740784780023 3.919119894504547 0.525 62.08584541082382 0.5492957746478874 5932.487601280212\n",
      "AL epoch:  13\n",
      "val, test error% for class  0  :  40.0 50.77\n",
      "val, test error% for class  1  :  50.0 33.85\n",
      "val, test error% for class  2  :  70.0 63.18\n",
      "val, test error% for class  3  :  30.0 35.4\n",
      "50  samples selected\n",
      "selEpoch: 13, Selection Ended at: 2021-04-29 15:10:37.440300\n",
      "50 1489 1539\n",
      "After augmentation, size of train_set:  891  lake set:  1489\n",
      "Epoch: 14 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 84.8870667219162 0.5791245791245792 3.176239788532257 0.65 63.98202496767044 0.5461658841940532 6214.516551971436\n",
      "AL epoch:  14\n",
      "val, test error% for class  0  :  40.0 46.15\n",
      "val, test error% for class  1  :  10.0 24.62\n",
      "val, test error% for class  2  :  50.0 65.67\n",
      "val, test error% for class  3  :  40.0 56.64\n",
      "50  samples selected\n",
      "selEpoch: 14, Selection Ended at: 2021-04-29 16:55:06.609654\n",
      "50 1439 1489\n",
      "After augmentation, size of train_set:  941  lake set:  1439\n",
      "Epoch: 15 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 82.03612467646599 0.6301806588735388 3.325318217277527 0.625 61.2152264714241 0.5758998435054773 6457.673188447952\n",
      "AL epoch:  15\n",
      "val, test error% for class  0  :  50.0 41.54\n",
      "val, test error% for class  1  :  20.0 30.0\n",
      "val, test error% for class  2  :  50.0 61.69\n",
      "val, test error% for class  3  :  30.0 37.17\n",
      "50  samples selected\n",
      "selEpoch: 15, Selection Ended at: 2021-04-29 18:43:40.581853\n",
      "50 1389 1439\n",
      "After augmentation, size of train_set:  991  lake set:  1389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 105.00918346643448 0.5418768920282543 4.616249471902847 0.55 63.8887260556221 0.5367762128325508 6862.687457084656\n",
      "AL epoch:  16\n",
      "val, test error% for class  0  :  50.0 35.38\n",
      "val, test error% for class  1  :  0.0 33.08\n",
      "val, test error% for class  2  :  80.0 69.65\n",
      "val, test error% for class  3  :  50.0 41.59\n",
      "50  samples selected\n",
      "selEpoch: 16, Selection Ended at: 2021-04-29 20:38:57.415971\n",
      "50 1339 1389\n",
      "After augmentation, size of train_set:  1041  lake set:  1339\n",
      "Epoch: 17 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 92.06213483214378 0.6157540826128722 3.434371292591095 0.55 59.564479291439056 0.5805946791862285 7215.629132986069\n",
      "AL epoch:  17\n",
      "val, test error% for class  0  :  70.0 52.31\n",
      "val, test error% for class  1  :  40.0 31.92\n",
      "val, test error% for class  2  :  30.0 48.76\n",
      "val, test error% for class  3  :  40.0 46.9\n",
      "50  samples selected\n",
      "selEpoch: 17, Selection Ended at: 2021-04-29 22:40:04.423517\n",
      "50 1289 1339\n",
      "After augmentation, size of train_set:  1091  lake set:  1289\n",
      "Epoch: 18 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 99.23134487867355 0.615032080659945 3.1657580733299255 0.675 61.74931621551514 0.5743348982785602 7537.629546403885\n",
      "AL epoch:  18\n",
      "val, test error% for class  0  :  40.0 50.77\n",
      "val, test error% for class  1  :  10.0 25.0\n",
      "val, test error% for class  2  :  50.0 52.74\n",
      "val, test error% for class  3  :  30.0 60.18\n",
      "50  samples selected\n",
      "selEpoch: 18, Selection Ended at: 2021-04-30 00:46:32.754840\n",
      "50 1239 1289\n",
      "After augmentation, size of train_set:  1141  lake set:  1239\n",
      "Epoch: 19 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 105.81237006187439 0.5810692375109553 3.274857223033905 0.675 63.06933370232582 0.5539906103286385 7887.8874344825745\n",
      "AL epoch:  19\n",
      "val, test error% for class  0  :  20.0 43.08\n",
      "val, test error% for class  1  :  10.0 30.38\n",
      "val, test error% for class  2  :  70.0 64.68\n",
      "val, test error% for class  3  :  30.0 42.48\n",
      "50  samples selected\n",
      "selEpoch: 19, Selection Ended at: 2021-04-30 02:58:48.981758\n",
      "50 1189 1239\n",
      "After augmentation, size of train_set:  1191  lake set:  1189\n",
      "Epoch: 20 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 184.31152275204659 0.43828715365239296 4.296366035938263 0.625 78.36716675758362 0.460093896713615 8103.323562145233\n",
      "val, test error% for class  0  :  40.0 78.46\n",
      "val, test error% for class  1  :  60.0 71.54\n",
      "val, test error% for class  2  :  50.0 39.3\n",
      "val, test error% for class  3  :  0.0 25.66\n",
      "[[89.23, 46.15, 72.64, 86.73, 73.6875], [61.54, 36.15, 56.72, 43.36, 49.442499999999995], [61.54, 50.0, 61.19, 30.09, 50.705], [69.23, 35.38, 67.66, 46.02, 54.572500000000005], [47.69, 46.54, 66.17, 42.48, 50.71999999999999], [36.92, 48.08, 61.69, 30.09, 44.195], [44.62, 35.77, 56.72, 59.29, 49.1], [58.46, 35.38, 63.18, 43.36, 50.095], [60.0, 31.54, 54.73, 61.95, 52.05499999999999], [38.46, 36.54, 62.69, 42.48, 45.0425], [64.62, 35.0, 60.7, 29.2, 47.379999999999995], [16.92, 53.46, 92.54, 55.75, 54.667500000000004], [50.77, 33.85, 63.18, 35.4, 45.800000000000004], [46.15, 24.62, 65.67, 56.64, 48.269999999999996], [41.54, 30.0, 61.69, 37.17, 42.599999999999994], [35.38, 33.08, 69.65, 41.59, 44.925000000000004], [52.31, 31.92, 48.76, 46.9, 44.972500000000004], [50.77, 25.0, 52.74, 60.18, 47.17250000000001], [43.08, 30.38, 64.68, 42.48, 45.154999999999994], [78.46, 71.54, 39.3, 25.66, 53.74]]\n"
     ]
    }
   ],
   "source": [
    "# train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, False, \"SIM\",'gccg')\n",
    "badge_tst, badge_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL us\n",
      "Breast-density Custom dataset stats: Train size:  241 Val size:  40 Lake size:  2139\n",
      "selected classes are:  [0, 3]\n",
      "Saving results to:  SMI_active_learning_results_woVal/breast_density/classimb/us/50/1\n",
      "breast_density_classimb_AL_2_us_budget:50_epochs:20_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/breast_density_MobileNetV2_0.01_{0: 35, 3: 40}_{1: 93, 2: 73}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  80.0 89.23\n",
      "val, test error% for class  1  :  60.0 46.15\n",
      "val, test error% for class  2  :  70.0 72.64\n",
      "val, test error% for class  3  :  90.0 86.73\n",
      "50  samples selected\n",
      "selEpoch: 1, Selection Ended at: 2021-04-26 18:11:24.120536\n",
      "50 2089 2139\n",
      "After augmentation, size of train_set:  291  lake set:  2089\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 29.484328269958496 0.5395189003436426 4.106535792350769 0.5 75.81113976240158 0.4350547730829421 2023.9583778381348\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  40.0 32.31\n",
      "val, test error% for class  1  :  70.0 73.85\n",
      "val, test error% for class  2  :  50.0 52.24\n",
      "val, test error% for class  3  :  40.0 38.05\n",
      "50  samples selected\n",
      "selEpoch: 2, Selection Ended at: 2021-04-26 18:46:16.673321\n",
      "50 2039 2089\n",
      "After augmentation, size of train_set:  341  lake set:  2039\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 31.538732014596462 0.5718475073313783 3.771481692790985 0.525 71.76461106538773 0.47417840375586856 2367.7800567150116\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  60.0 67.69\n",
      "val, test error% for class  1  :  10.0 44.62\n",
      "val, test error% for class  2  :  80.0 70.65\n",
      "val, test error% for class  3  :  40.0 30.09\n",
      "50  samples selected\n",
      "selEpoch: 3, Selection Ended at: 2021-04-26 19:26:50.917036\n",
      "50 1989 2039\n",
      "After augmentation, size of train_set:  391  lake set:  1989\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 72.42568451166153 0.48081841432225064 9.909042060375214 0.325 120.09794986248016 0.37402190923317685 2744.4186868667603\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  60.0 56.92\n",
      "val, test error% for class  1  :  10.0 25.38\n",
      "val, test error% for class  2  :  100.0 93.03\n",
      "val, test error% for class  3  :  100.0 97.35\n",
      "50  samples selected\n",
      "selEpoch: 4, Selection Ended at: 2021-04-26 20:13:39.716012\n",
      "50 1939 1989\n",
      "After augmentation, size of train_set:  441  lake set:  1939\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 43.63349658250809 0.5555555555555556 3.682671070098877 0.5 65.12044543027878 0.5195618153364632 3119.6847643852234\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  40.0 60.0\n",
      "val, test error% for class  1  :  20.0 29.23\n",
      "val, test error% for class  2  :  90.0 69.15\n",
      "val, test error% for class  3  :  50.0 46.9\n",
      "50  samples selected\n",
      "selEpoch: 5, Selection Ended at: 2021-04-26 21:06:43.366730\n",
      "50 1889 1939\n",
      "After augmentation, size of train_set:  491  lake set:  1889\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 46.531904101371765 0.615071283095723 4.031517565250397 0.6 63.725980043411255 0.5555555555555556 3490.105579853058\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  80.0 69.23\n",
      "val, test error% for class  1  :  30.0 26.54\n",
      "val, test error% for class  2  :  40.0 53.73\n",
      "val, test error% for class  3  :  10.0 54.87\n",
      "50  samples selected\n",
      "selEpoch: 6, Selection Ended at: 2021-04-26 22:05:54.274796\n",
      "50 1839 1889\n",
      "After augmentation, size of train_set:  541  lake set:  1839\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 48.32544267177582 0.6155268022181146 3.289590358734131 0.575 65.31287741661072 0.5226917057902973 3793.360362768173\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  70.0 53.85\n",
      "val, test error% for class  1  :  30.0 43.85\n",
      "val, test error% for class  2  :  50.0 50.75\n",
      "val, test error% for class  3  :  20.0 47.79\n",
      "50  samples selected\n",
      "selEpoch: 7, Selection Ended at: 2021-04-26 23:10:10.082375\n",
      "50 1789 1839\n",
      "After augmentation, size of train_set:  591  lake set:  1789\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 60.11887317895889 0.5245346869712352 3.6836456656455994 0.45 64.98516035079956 0.49921752738654146 4145.134223461151\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  70.0 61.54\n",
      "val, test error% for class  1  :  50.0 43.46\n",
      "val, test error% for class  2  :  70.0 64.18\n",
      "val, test error% for class  3  :  30.0 33.63\n",
      "50  samples selected\n",
      "selEpoch: 8, Selection Ended at: 2021-04-27 00:20:17.146938\n",
      "50 1739 1789\n",
      "After augmentation, size of train_set:  641  lake set:  1739\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 62.960994362831116 0.5834633385335414 4.305320620536804 0.5 66.76867663860321 0.5305164319248826 4503.834222793579\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  70.0 60.0\n",
      "val, test error% for class  1  :  20.0 35.38\n",
      "val, test error% for class  2  :  50.0 56.22\n",
      "val, test error% for class  3  :  60.0 49.56\n",
      "50  samples selected\n",
      "selEpoch: 9, Selection Ended at: 2021-04-27 01:36:21.048291\n",
      "50 1689 1739\n",
      "After augmentation, size of train_set:  691  lake set:  1689\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 63.246243089437485 0.6164978292329957 3.4234238266944885 0.575 62.552370607852936 0.5571205007824727 4873.722120523453\n",
      "AL epoch:  10\n",
      "val, test error% for class  0  :  60.0 41.54\n",
      "val, test error% for class  1  :  30.0 41.54\n",
      "val, test error% for class  2  :  50.0 57.71\n",
      "val, test error% for class  3  :  30.0 28.32\n",
      "50  samples selected\n",
      "selEpoch: 10, Selection Ended at: 2021-04-27 02:58:36.097870\n",
      "50 1639 1689\n",
      "After augmentation, size of train_set:  741  lake set:  1639\n",
      "Epoch: 11 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 69.12843006849289 0.6059379217273954 3.479477047920227 0.55 64.93251639604568 0.5602503912363067 5194.931692361832\n",
      "AL epoch:  11\n",
      "val, test error% for class  0  :  70.0 55.38\n",
      "val, test error% for class  1  :  30.0 36.15\n",
      "val, test error% for class  2  :  60.0 47.26\n",
      "val, test error% for class  3  :  20.0 49.56\n",
      "50  samples selected\n",
      "selEpoch: 11, Selection Ended at: 2021-04-27 04:26:11.238323\n",
      "50 1589 1639\n",
      "After augmentation, size of train_set:  791  lake set:  1589\n",
      "Epoch: 12 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 72.36020967364311 0.6055625790139064 3.3341110944747925 0.675 61.6713245511055 0.5571205007824727 5553.514053344727\n",
      "AL epoch:  12\n",
      "val, test error% for class  0  :  60.0 58.46\n",
      "val, test error% for class  1  :  20.0 34.62\n",
      "val, test error% for class  2  :  30.0 59.7\n",
      "val, test error% for class  3  :  20.0 30.97\n",
      "50  samples selected\n",
      "selEpoch: 12, Selection Ended at: 2021-04-27 05:59:43.477919\n",
      "50 1539 1589\n",
      "After augmentation, size of train_set:  841  lake set:  1539\n",
      "Epoch: 13 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 81.5863493680954 0.5695600475624257 3.7998126745224 0.525 64.012979388237 0.543035993740219 5913.271070480347\n",
      "AL epoch:  13\n",
      "val, test error% for class  0  :  60.0 63.08\n",
      "val, test error% for class  1  :  10.0 34.62\n",
      "val, test error% for class  2  :  80.0 59.7\n",
      "val, test error% for class  3  :  40.0 36.28\n",
      "50  samples selected\n",
      "selEpoch: 13, Selection Ended at: 2021-04-27 07:39:13.474311\n",
      "50 1489 1539\n",
      "After augmentation, size of train_set:  891  lake set:  1489\n",
      "Epoch: 14 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 80.13994413614273 0.6083052749719416 3.231669306755066 0.55 60.61549973487854 0.5712050078247262 6248.133887052536\n",
      "AL epoch:  14\n",
      "val, test error% for class  0  :  70.0 47.69\n",
      "val, test error% for class  1  :  0.0 31.15\n",
      "val, test error% for class  2  :  70.0 52.74\n",
      "val, test error% for class  3  :  40.0 49.56\n",
      "50  samples selected\n",
      "selEpoch: 14, Selection Ended at: 2021-04-27 09:24:17.150466\n",
      "50 1439 1489\n",
      "After augmentation, size of train_set:  941  lake set:  1439\n",
      "Epoch: 15 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 185.68385481834412 0.4782146652497343 9.501690298318863 0.4 111.08107104897499 0.43661971830985913 6586.933725595474\n",
      "AL epoch:  15\n",
      "val, test error% for class  0  :  60.0 64.62\n",
      "val, test error% for class  1  :  10.0 13.85\n",
      "val, test error% for class  2  :  70.0 84.08\n",
      "val, test error% for class  3  :  100.0 100.0\n",
      "50  samples selected\n",
      "selEpoch: 15, Selection Ended at: 2021-04-27 11:14:57.877163\n",
      "50 1389 1439\n",
      "After augmentation, size of train_set:  991  lake set:  1389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 90.12158101797104 0.5933400605449042 3.624807596206665 0.5 58.92980682849884 0.5993740219092332 6912.891916513443\n",
      "AL epoch:  16\n",
      "val, test error% for class  0  :  60.0 46.15\n",
      "val, test error% for class  1  :  50.0 38.46\n",
      "val, test error% for class  2  :  50.0 43.78\n",
      "val, test error% for class  3  :  40.0 33.63\n",
      "50  samples selected\n",
      "selEpoch: 16, Selection Ended at: 2021-04-27 13:11:04.176632\n",
      "50 1339 1389\n",
      "After augmentation, size of train_set:  1041  lake set:  1339\n",
      "Epoch: 17 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 94.25977221131325 0.6003842459173871 3.3683047890663147 0.6 58.921400249004364 0.5602503912363067 7249.35201215744\n",
      "AL epoch:  17\n",
      "val, test error% for class  0  :  40.0 40.0\n",
      "val, test error% for class  1  :  20.0 41.15\n",
      "val, test error% for class  2  :  60.0 53.73\n",
      "val, test error% for class  3  :  40.0 35.4\n",
      "50  samples selected\n",
      "selEpoch: 17, Selection Ended at: 2021-04-27 15:12:45.934728\n",
      "50 1289 1339\n",
      "After augmentation, size of train_set:  1091  lake set:  1289\n",
      "Epoch: 18 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 95.87192073464394 0.614115490375802 3.240273356437683 0.675 57.76399075984955 0.5837245696400626 7841.244266271591\n",
      "AL epoch:  18\n",
      "val, test error% for class  0  :  50.0 38.46\n",
      "val, test error% for class  1  :  0.0 35.38\n",
      "val, test error% for class  2  :  60.0 50.25\n",
      "val, test error% for class  3  :  20.0 42.48\n",
      "50  samples selected\n",
      "selEpoch: 18, Selection Ended at: 2021-04-27 17:24:16.382010\n",
      "50 1239 1289\n",
      "After augmentation, size of train_set:  1141  lake set:  1239\n",
      "Epoch: 19 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 95.99994930624962 0.6292725679228747 2.9478090405464172 0.7 56.78782796859741 0.5727699530516432 8537.019124031067\n",
      "AL epoch:  19\n",
      "val, test error% for class  0  :  40.0 38.46\n",
      "val, test error% for class  1  :  10.0 44.62\n",
      "val, test error% for class  2  :  40.0 45.27\n",
      "val, test error% for class  3  :  30.0 36.28\n",
      "50  samples selected\n",
      "selEpoch: 19, Selection Ended at: 2021-04-27 19:47:31.778300\n",
      "50 1189 1239\n",
      "After augmentation, size of train_set:  1191  lake set:  1189\n",
      "Epoch: 20 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 109.51774829626083 0.5768261964735516 2.8799683451652527 0.725 65.15885818004608 0.5477308294209703 8516.495544433594\n",
      "val, test error% for class  0  :  60.0 40.0\n",
      "val, test error% for class  1  :  20.0 45.0\n",
      "val, test error% for class  2  :  30.0 47.76\n",
      "val, test error% for class  3  :  0.0 44.25\n",
      "[[89.23, 46.15, 72.64, 86.73, 73.6875], [32.31, 73.85, 52.24, 38.05, 49.1125], [67.69, 44.62, 70.65, 30.09, 53.2625], [56.92, 25.38, 93.03, 97.35, 68.16999999999999], [60.0, 29.23, 69.15, 46.9, 51.32], [69.23, 26.54, 53.73, 54.87, 51.0925], [53.85, 43.85, 50.75, 47.79, 49.059999999999995], [61.54, 43.46, 64.18, 33.63, 50.7025], [60.0, 35.38, 56.22, 49.56, 50.29], [41.54, 41.54, 57.71, 28.32, 42.277499999999996], [55.38, 36.15, 47.26, 49.56, 47.0875], [58.46, 34.62, 59.7, 30.97, 45.9375], [63.08, 34.62, 59.7, 36.28, 48.419999999999995], [47.69, 31.15, 52.74, 49.56, 45.285000000000004], [64.62, 13.85, 84.08, 100.0, 65.6375], [46.15, 38.46, 43.78, 33.63, 40.504999999999995], [40.0, 41.15, 53.73, 35.4, 42.57], [38.46, 35.38, 50.25, 42.48, 41.6425], [38.46, 44.62, 45.27, 36.28, 41.1575], [40.0, 45.0, 47.76, 44.25, 44.2525]]\n"
     ]
    }
   ],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "us_tst, us_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"glister-tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradMatch-Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradmatch_tst, gradmatch_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, 1, device, computeClassErrorLog, \"AL\",\"gradmatch-tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreset_tst, coreset_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"coreset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leastconf_tst, leastconf_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"leastconf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "margin_tst, margin_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"margin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCMI+DIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gcmidiv_tst, gcmidiv_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'div-gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gcmi_tst, gcmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDETMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIM logdetmi\n",
      "Breast-density Custom dataset stats: Train size:  241 Val size:  40 Lake size:  2139\n",
      "selected classes are:  [0, 3]\n",
      "Saving results to:  SMI_active_learning_results_woVal/breast_density/classimb/logdetmi/50/1\n",
      "breast_density_classimb_SIM_2_logdetmi_budget:50_epochs:20_linear:True_runs1\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/breast_density_MobileNetV2_0.01_{0: 35, 3: 40}_{1: 93, 2: 73}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  90.0 89.23\n",
      "val, test error% for class  1  :  60.0 46.15\n",
      "val, test error% for class  2  :  80.0 72.64\n",
      "val, test error% for class  3  :  80.0 86.73\n",
      "total misclassified ex from imb classes:  31\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2139, 5124])\n",
      "val minibatch gradients shape  torch.Size([31, 5124])\n",
      "kernel compute time:  0.8748178482055664\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  31  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 1., 2., 2., 2., 2., 2., 0., 1., 1., 1., 1., 2., 2., 0., 1., 2., 1.,\n",
      "        2., 2., 1., 2., 1., 1., 1., 2., 0., 2., 1., 2., 1., 2., 2., 2., 1., 1.,\n",
      "        1., 2., 2., 0., 1., 3., 2., 1., 1., 0., 2., 2., 0., 2.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 0., 1., 2., 1., 0., 3., 2., 0., 3., 3., 3., 0., 0., 2., 3.,\n",
      "        2., 1., 2., 1., 1., 3., 1., 0., 0., 1., 3., 2., 3., 2., 1., 2., 3., 3.,\n",
      "        0., 2., 0., 0., 2., 3., 0., 1., 2., 1., 1., 0., 3., 1.])\n",
      "50 2089 2139\n",
      "After augmentation, size of train_set:  291  lake set:  2089\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 28.880467474460602 0.5498281786941581 3.506541132926941 0.675 67.58897793292999 0.49921752738654146 1955.1519181728363\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  30.0 50.77\n",
      "val, test error% for class  1  :  30.0 46.15\n",
      "val, test error% for class  2  :  20.0 44.78\n",
      "val, test error% for class  3  :  50.0 68.14\n",
      "total misclassified ex from imb classes:  13\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2089, 5124])\n",
      "val minibatch gradients shape  torch.Size([13, 5124])\n",
      "kernel compute time:  0.8339412212371826\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  13  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 2., 2., 2., 1., 2., 0., 2., 0., 1., 3., 0., 1., 2., 1., 3.,\n",
      "        2., 1., 2., 2., 1., 1., 2., 2., 1., 1., 2., 0., 1., 0., 2., 1., 1., 1.,\n",
      "        2., 1., 0., 1., 3., 3., 1., 2., 1., 3., 1., 3., 3., 0.])\n",
      "Hypothesized targets of subset:  tensor([3., 1., 2., 3., 1., 1., 1., 3., 0., 1., 0., 0., 3., 0., 2., 1., 0., 3.,\n",
      "        2., 1., 2., 2., 3., 1., 3., 3., 0., 1., 3., 0., 0., 0., 2., 1., 2., 1.,\n",
      "        3., 0., 0., 1., 3., 3., 0., 3., 2., 3., 3., 2., 3., 3.])\n",
      "50 2039 2089\n",
      "After augmentation, size of train_set:  341  lake set:  2039\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 33.790658831596375 0.5454545454545454 3.7355234026908875 0.525 69.21352082490921 0.513302034428795 2284.8894572257996\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  60.0 58.46\n",
      "val, test error% for class  1  :  30.0 30.38\n",
      "val, test error% for class  2  :  40.0 54.23\n",
      "val, test error% for class  3  :  60.0 75.22\n",
      "total misclassified ex from imb classes:  19\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([2039, 5124])\n",
      "val minibatch gradients shape  torch.Size([19, 5124])\n",
      "kernel compute time:  1.5687751770019531\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  19  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 2., 2., 1., 3., 0., 2., 1., 0., 0., 3., 0., 0., 0., 1., 3., 0., 2.,\n",
      "        0., 2., 0., 3., 0., 2., 2., 2., 1., 3., 1., 0., 3., 1., 2., 1., 2., 0.,\n",
      "        2., 2., 1., 2., 0., 0., 3., 0., 2., 1., 1., 0., 1., 3.])\n",
      "Hypothesized targets of subset:  tensor([0., 2., 2., 1., 3., 0., 3., 2., 0., 0., 3., 0., 0., 0., 1., 2., 0., 2.,\n",
      "        3., 2., 1., 3., 0., 1., 3., 3., 3., 3., 0., 0., 3., 1., 1., 2., 3., 0.,\n",
      "        1., 2., 1., 3., 2., 1., 3., 0., 2., 0., 3., 2., 0., 2.])\n",
      "50 1989 2039\n",
      "After augmentation, size of train_set:  391  lake set:  1989\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 37.443259835243225 0.6035805626598465 3.651133894920349 0.475 64.02156925201416 0.543035993740219 2582.5423481464386\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  60.0 44.62\n",
      "val, test error% for class  1  :  40.0 39.62\n",
      "val, test error% for class  2  :  70.0 51.24\n",
      "val, test error% for class  3  :  40.0 50.44\n",
      "total misclassified ex from imb classes:  21\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1989, 5124])\n",
      "val minibatch gradients shape  torch.Size([21, 5124])\n",
      "kernel compute time:  0.7523863315582275\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  21  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 0., 1., 3., 1., 3., 0., 1., 0., 1., 2., 3., 3., 1., 3., 1., 1., 3.,\n",
      "        3., 0., 2., 3., 0., 2., 0., 2., 3., 0., 3., 0., 3., 1., 3., 1., 1., 3.,\n",
      "        3., 2., 2., 1., 2., 3., 0., 0., 1., 0., 2., 1., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 0., 1., 1., 3., 1., 1., 0., 0., 2., 2., 3., 0., 3., 1., 0., 1.,\n",
      "        3., 0., 2., 2., 0., 3., 0., 3., 3., 1., 2., 0., 2., 0., 2., 0., 1., 3.,\n",
      "        3., 2., 1., 2., 2., 2., 0., 0., 3., 0., 2., 1., 3., 2.])\n",
      "50 1939 1989\n",
      "After augmentation, size of train_set:  441  lake set:  1939\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 39.08375081419945 0.6439909297052154 3.5143336057662964 0.475 68.301185131073 0.5179968701095462 2909.7497520446777\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  40.0 29.23\n",
      "val, test error% for class  1  :  60.0 48.08\n",
      "val, test error% for class  2  :  90.0 56.72\n",
      "val, test error% for class  3  :  20.0 44.25\n",
      "total misclassified ex from imb classes:  21\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1939, 5124])\n",
      "val minibatch gradients shape  torch.Size([21, 5124])\n",
      "kernel compute time:  0.7352032661437988\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  21  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True targets of subset:  tensor([1., 0., 1., 2., 0., 2., 2., 1., 2., 1., 2., 1., 2., 2., 1., 1., 2., 3.,\n",
      "        2., 2., 3., 1., 1., 1., 2., 2., 2., 1., 1., 2., 1., 2., 0., 1., 1., 1.,\n",
      "        3., 3., 0., 0., 3., 1., 1., 3., 2., 3., 3., 2., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 2., 1., 1., 0., 1., 2., 0., 1., 3., 2., 2., 2., 2., 0., 1., 1., 3.,\n",
      "        3., 2., 3., 2., 0., 1., 1., 1., 3., 0., 0., 2., 1., 2., 0., 0., 2., 1.,\n",
      "        3., 2., 0., 0., 2., 0., 0., 2., 3., 3., 2., 3., 1., 2.])\n",
      "50 1889 1939\n",
      "After augmentation, size of train_set:  491  lake set:  1889\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 43.30398443341255 0.6272912423625254 3.7448381781578064 0.5 65.81571018695831 0.5289514866979655 3235.0083799362183\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  50.0 36.92\n",
      "val, test error% for class  1  :  60.0 51.92\n",
      "val, test error% for class  2  :  60.0 47.26\n",
      "val, test error% for class  3  :  30.0 41.59\n",
      "total misclassified ex from imb classes:  20\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1889, 5124])\n",
      "val minibatch gradients shape  torch.Size([20, 5124])\n",
      "kernel compute time:  1.540198802947998\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  20  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 3., 3., 1., 2., 3., 3., 3., 1., 1., 3., 0., 1., 1., 0., 1., 2., 2.,\n",
      "        0., 3., 0., 3., 1., 0., 2., 1., 2., 3., 1., 0., 2., 3., 1., 2., 3., 3.,\n",
      "        3., 3., 0., 1., 1., 1., 2., 2., 1., 1., 1., 2., 1., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 2., 1., 2., 2., 3., 1., 2., 3., 1., 1., 0., 2., 1., 1., 2.,\n",
      "        0., 3., 0., 3., 2., 0., 0., 1., 3., 3., 1., 0., 0., 2., 0., 3., 2., 2.,\n",
      "        3., 0., 0., 1., 1., 0., 2., 2., 1., 0., 0., 3., 1., 1.])\n",
      "50 1839 1889\n",
      "After augmentation, size of train_set:  541  lake set:  1839\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 50.93702311813831 0.5933456561922366 3.451745331287384 0.55 70.84301155805588 0.5289514866979655 3595.7915720939636\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  30.0 32.31\n",
      "val, test error% for class  1  :  30.0 41.92\n",
      "val, test error% for class  2  :  70.0 50.25\n",
      "val, test error% for class  3  :  50.0 61.95\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1839, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.6483516693115234\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  18  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 1., 0., 1., 2., 1., 3., 1., 2., 1., 0., 2., 2., 0., 1., 1., 0., 3.,\n",
      "        3., 1., 3., 2., 3., 1., 2., 2., 1., 0., 3., 3., 2., 3., 1., 0., 0., 3.,\n",
      "        1., 0., 3., 2., 2., 3., 1., 0., 2., 2., 2., 2., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([2., 1., 0., 2., 1., 1., 3., 0., 2., 1., 0., 2., 1., 0., 1., 1., 2., 2.,\n",
      "        3., 3., 0., 2., 2., 1., 3., 3., 1., 0., 2., 3., 1., 3., 2., 1., 0., 3.,\n",
      "        0., 0., 3., 2., 2., 3., 1., 1., 3., 3., 2., 0., 3., 2.])\n",
      "50 1789 1839\n",
      "After augmentation, size of train_set:  591  lake set:  1789\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 53.319718688726425 0.6074450084602369 3.3266775012016296 0.55 70.44995683431625 0.5164319248826291 3917.0299949645996\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  50.0 36.92\n",
      "val, test error% for class  1  :  40.0 51.92\n",
      "val, test error% for class  2  :  70.0 54.73\n",
      "val, test error% for class  3  :  20.0 35.4\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1789, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.7918078899383545\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  18  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([1., 1., 1., 0., 1., 2., 1., 2., 0., 1., 2., 3., 1., 2., 0., 1., 2., 3.,\n",
      "        3., 1., 2., 1., 1., 3., 2., 3., 3., 0., 0., 3., 3., 1., 2., 3., 1., 1.,\n",
      "        1., 0., 3., 1., 1., 0., 3., 0., 0., 2., 1., 3., 2., 0.])\n",
      "Hypothesized targets of subset:  tensor([1., 1., 2., 0., 1., 3., 0., 2., 0., 1., 1., 2., 1., 2., 0., 1., 2., 3.,\n",
      "        1., 1., 3., 0., 2., 2., 2., 2., 3., 0., 0., 3., 3., 2., 3., 0., 1., 0.,\n",
      "        1., 0., 2., 3., 2., 0., 2., 0., 1., 0., 2., 3., 2., 0.])\n",
      "50 1739 1789\n",
      "After augmentation, size of train_set:  641  lake set:  1739\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 56.077756851911545 0.625585023400936 3.2603082060813904 0.55 62.901981234550476 0.5602503912363067 4333.192449331284\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  50.0 27.69\n",
      "val, test error% for class  1  :  40.0 46.54\n",
      "val, test error% for class  2  :  70.0 47.76\n",
      "val, test error% for class  3  :  20.0 40.71\n",
      "total misclassified ex from imb classes:  18\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1739, 5124])\n",
      "val minibatch gradients shape  torch.Size([18, 5124])\n",
      "kernel compute time:  0.6120522022247314\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  18  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 0., 0., 3., 0., 1., 1., 2., 1., 2., 0., 2., 2., 2., 2., 1., 2., 0.,\n",
      "        2., 1., 0., 0., 3., 1., 0., 1., 1., 2., 3., 1., 0., 2., 1., 3., 2., 2.,\n",
      "        1., 2., 1., 2., 0., 2., 1., 2., 3., 2., 2., 1., 3., 2.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 0., 3., 0., 1., 1., 2., 1., 2., 0., 2., 1., 2., 2., 3., 1., 0.,\n",
      "        2., 0., 0., 2., 2., 1., 0., 1., 1., 1., 2., 1., 0., 3., 2., 3., 2., 3.,\n",
      "        0., 2., 0., 1., 0., 2., 1., 0., 3., 1., 1., 0., 3., 1.])\n",
      "50 1689 1739\n",
      "After augmentation, size of train_set:  691  lake set:  1689\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 57.574298709630966 0.6526772793053546 3.2057632207870483 0.575 64.90503972768784 0.5383411580594679 4696.375428676605\n",
      "AL epoch:  10\n",
      "val, test error% for class  0  :  40.0 38.46\n",
      "val, test error% for class  1  :  50.0 46.54\n",
      "val, test error% for class  2  :  50.0 40.3\n",
      "val, test error% for class  3  :  30.0 60.18\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1689, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel compute time:  0.6493377685546875\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  17  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 2., 1., 1., 3., 2., 3., 3., 0., 2., 1., 0., 1., 1., 3., 1., 3., 1.,\n",
      "        3., 1., 1., 0., 0., 3., 1., 2., 3., 3., 1., 1., 3., 1., 3., 1., 3., 0.,\n",
      "        1., 3., 2., 3., 3., 2., 1., 1., 2., 1., 2., 0., 2., 2.])\n",
      "Hypothesized targets of subset:  tensor([1., 2., 3., 1., 3., 2., 0., 2., 0., 1., 0., 0., 0., 1., 2., 0., 3., 1.,\n",
      "        3., 1., 1., 0., 0., 3., 0., 2., 1., 3., 2., 0., 3., 1., 2., 1., 3., 0.,\n",
      "        2., 3., 3., 2., 2., 2., 1., 3., 1., 0., 1., 0., 2., 2.])\n",
      "50 1639 1689\n",
      "After augmentation, size of train_set:  741  lake set:  1639\n",
      "Epoch: 11 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 65.8680009841919 0.6167341430499326 2.8853878676891327 0.65 66.14696830511093 0.543035993740219 5063.139382839203\n",
      "AL epoch:  11\n",
      "val, test error% for class  0  :  40.0 35.38\n",
      "val, test error% for class  1  :  30.0 47.69\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  20.0 39.82\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1639, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.5868003368377686\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 1., 3., 1., 1., 2., 3., 3., 1., 3., 2., 3., 1., 1., 1., 2., 1., 2.,\n",
      "        3., 2., 0., 3., 2., 1., 3., 2., 2., 0., 1., 1., 1., 3., 3., 1., 3., 3.,\n",
      "        2., 0., 1., 3., 2., 1., 1., 0., 1., 2., 1., 1., 2., 3.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 0., 0., 2., 2., 0., 2., 2., 1., 3., 0., 0., 2., 1., 1., 2.,\n",
      "        3., 3., 0., 3., 1., 1., 2., 2., 3., 0., 0., 1., 1., 1., 3., 0., 2., 3.,\n",
      "        0., 2., 2., 3., 1., 0., 1., 0., 0., 3., 0., 1., 3., 2.])\n",
      "50 1589 1639\n",
      "After augmentation, size of train_set:  791  lake set:  1589\n",
      "Epoch: 12 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 71.64822047948837 0.606826801517067 3.187554359436035 0.575 69.83818084001541 0.5101721439749609 5426.219938755035\n",
      "AL epoch:  12\n",
      "val, test error% for class  0  :  40.0 30.77\n",
      "val, test error% for class  1  :  50.0 50.38\n",
      "val, test error% for class  2  :  40.0 52.74\n",
      "val, test error% for class  3  :  40.0 49.56\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1589, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n",
      "kernel compute time:  0.7422590255737305\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  17  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 3., 2., 1., 2., 2., 2., 2., 1., 0., 1., 1., 2., 1., 2., 1.,\n",
      "        2., 3., 3., 1., 0., 2., 2., 3., 3., 1., 3., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        2., 2., 3., 2., 1., 2., 2., 2., 0., 2., 1., 3., 0., 1.])\n",
      "Hypothesized targets of subset:  tensor([0., 1., 3., 3., 0., 1., 0., 2., 0., 2., 0., 0., 0., 1., 2., 1., 1., 0.,\n",
      "        1., 3., 3., 1., 0., 3., 2., 3., 2., 1., 3., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        2., 3., 2., 2., 1., 2., 2., 0., 0., 1., 3., 3., 3., 2.])\n",
      "50 1539 1589\n",
      "After augmentation, size of train_set:  841  lake set:  1539\n",
      "Epoch: 13 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 70.6075593829155 0.6444708680142688 3.172585964202881 0.65 61.7319073677063 0.5586854460093896 5838.497944116592\n",
      "AL epoch:  13\n",
      "val, test error% for class  0  :  30.0 36.92\n",
      "val, test error% for class  1  :  40.0 52.69\n",
      "val, test error% for class  2  :  20.0 37.81\n",
      "val, test error% for class  3  :  50.0 39.82\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1539, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.4929676055908203\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 2., 1., 1., 2., 1., 3., 3., 1., 2., 3., 2., 3., 2., 2., 2., 0., 3.,\n",
      "        0., 2., 1., 1., 1., 1., 3., 3., 2., 1., 0., 2., 0., 1., 1., 3., 2., 1.,\n",
      "        0., 3., 3., 3., 2., 0., 2., 1., 3., 2., 2., 1., 3., 1.])\n",
      "Hypothesized targets of subset:  tensor([3., 2., 3., 0., 2., 1., 3., 3., 0., 2., 3., 1., 3., 2., 2., 1., 0., 3.,\n",
      "        1., 2., 1., 1., 1., 1., 3., 3., 1., 0., 0., 2., 0., 0., 0., 3., 2., 1.,\n",
      "        0., 2., 3., 2., 3., 0., 3., 1., 1., 3., 3., 0., 2., 0.])\n",
      "50 1489 1539\n",
      "After augmentation, size of train_set:  891  lake set:  1489\n",
      "Epoch: 14 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 75.81520435214043 0.6352413019079686 2.9937873482704163 0.65 63.763407826423645 0.5555555555555556 6182.668705940247\n",
      "AL epoch:  14\n",
      "val, test error% for class  0  :  30.0 36.92\n",
      "val, test error% for class  1  :  40.0 51.15\n",
      "val, test error% for class  2  :  30.0 44.78\n",
      "val, test error% for class  3  :  40.0 32.74\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1489, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.6919732093811035\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 0., 1., 3., 2., 3., 1., 0., 1., 1., 2., 2., 2., 3., 2., 3., 2., 1.,\n",
      "        2., 1., 1., 2., 2., 1., 3., 2., 1., 3., 3., 2., 2., 0., 0., 0., 3., 1.,\n",
      "        2., 1., 1., 0., 1., 3., 1., 1., 1., 0., 1., 1., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([3., 2., 0., 3., 2., 3., 1., 1., 0., 3., 2., 3., 1., 3., 3., 3., 2., 0.,\n",
      "        0., 0., 1., 3., 2., 0., 3., 2., 0., 2., 2., 2., 3., 0., 1., 0., 3., 1.,\n",
      "        2., 2., 0., 1., 1., 3., 0., 1., 0., 0., 1., 1., 2., 1.])\n",
      "50 1439 1489\n",
      "After augmentation, size of train_set:  941  lake set:  1439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 82.72060441970825 0.6323060573857598 3.2334787845611572 0.625 61.430308520793915 0.568075117370892 6534.424946784973\n",
      "AL epoch:  15\n",
      "val, test error% for class  0  :  30.0 29.23\n",
      "val, test error% for class  1  :  40.0 47.31\n",
      "val, test error% for class  2  :  50.0 47.76\n",
      "val, test error% for class  3  :  30.0 33.63\n",
      "total misclassified ex from imb classes:  15\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1439, 5124])\n",
      "val minibatch gradients shape  torch.Size([15, 5124])\n",
      "kernel compute time:  0.5626442432403564\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  15  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([3., 2., 1., 1., 1., 2., 2., 1., 3., 1., 1., 2., 0., 2., 2., 2., 3., 1.,\n",
      "        3., 1., 1., 2., 3., 2., 2., 3., 2., 1., 3., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        2., 1., 3., 2., 2., 2., 3., 1., 1., 0., 1., 0., 2., 3.])\n",
      "Hypothesized targets of subset:  tensor([2., 3., 3., 0., 0., 2., 2., 1., 2., 1., 0., 3., 0., 2., 3., 1., 3., 2.,\n",
      "        0., 1., 0., 3., 2., 1., 0., 3., 0., 2., 2., 1., 0., 2., 1., 0., 1., 0.,\n",
      "        3., 0., 2., 2., 1., 2., 3., 1., 1., 0., 3., 2., 3., 3.])\n",
      "50 1389 1439\n",
      "After augmentation, size of train_set:  991  lake set:  1389\n",
      "Epoch: 16 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 77.78142288327217 0.6821392532795156 2.9425390362739563 0.65 61.032109409570694 0.5492957746478874 6867.935442447662\n",
      "AL epoch:  16\n",
      "val, test error% for class  0  :  10.0 29.23\n",
      "val, test error% for class  1  :  30.0 47.69\n",
      "val, test error% for class  2  :  60.0 54.23\n",
      "val, test error% for class  3  :  40.0 31.86\n",
      "total misclassified ex from imb classes:  14\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1389, 5124])\n",
      "val minibatch gradients shape  torch.Size([14, 5124])\n",
      "kernel compute time:  0.5280313491821289\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  14  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 3., 1., 1., 2., 1., 3., 2., 1., 2., 0., 1., 2., 1., 3., 1., 1., 1.,\n",
      "        3., 2., 3., 1., 1., 2., 2., 3., 3., 2., 1., 3., 2., 3., 1., 0., 2., 3.,\n",
      "        1., 0., 1., 2., 3., 2., 3., 1., 1., 1., 2., 2., 2., 1.])\n",
      "Hypothesized targets of subset:  tensor([1., 3., 1., 0., 3., 3., 3., 2., 1., 3., 0., 1., 2., 2., 3., 1., 1., 1.,\n",
      "        2., 2., 3., 0., 0., 1., 2., 3., 3., 2., 1., 0., 2., 3., 0., 1., 2., 0.,\n",
      "        1., 0., 2., 3., 3., 2., 3., 1., 1., 2., 2., 2., 2., 2.])\n",
      "50 1339 1389\n",
      "After augmentation, size of train_set:  1041  lake set:  1339\n",
      "Epoch: 17 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 88.16091111302376 0.6407300672430355 2.7356441020965576 0.7 62.42228502035141 0.564945226917058 7241.87736082077\n",
      "AL epoch:  17\n",
      "val, test error% for class  0  :  40.0 32.31\n",
      "val, test error% for class  1  :  20.0 41.54\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  10.0 43.36\n",
      "total misclassified ex from imb classes:  12\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1339, 5124])\n",
      "val minibatch gradients shape  torch.Size([12, 5124])\n",
      "kernel compute time:  0.592418909072876\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  12  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([2., 1., 3., 1., 0., 2., 2., 3., 0., 1., 1., 2., 2., 3., 2., 1., 1., 2.,\n",
      "        1., 2., 2., 0., 2., 3., 2., 2., 2., 1., 0., 2., 2., 2., 1., 2., 2., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 2., 1., 2., 0., 1., 3., 3., 0.])\n",
      "Hypothesized targets of subset:  tensor([2., 0., 3., 1., 0., 2., 2., 3., 0., 1., 0., 2., 1., 3., 3., 0., 0., 2.,\n",
      "        0., 2., 2., 0., 3., 2., 2., 1., 1., 2., 0., 2., 3., 1., 1., 2., 1., 2.,\n",
      "        2., 0., 0., 0., 2., 1., 2., 0., 1., 0., 0., 0., 3., 0.])\n",
      "50 1289 1339\n",
      "After augmentation, size of train_set:  1091  lake set:  1289\n",
      "Epoch: 18 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 99.78812801837921 0.5802016498625114 3.349065363407135 0.575 64.5497316122055 0.5117370892018779 7301.767403841019\n",
      "AL epoch:  18\n",
      "val, test error% for class  0  :  40.0 24.62\n",
      "val, test error% for class  1  :  50.0 50.38\n",
      "val, test error% for class  2  :  50.0 66.67\n",
      "val, test error% for class  3  :  30.0 27.43\n",
      "total misclassified ex from imb classes:  17\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1289, 5124])\n",
      "val minibatch gradients shape  torch.Size([17, 5124])\n",
      "kernel compute time:  0.37871479988098145\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  17  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n",
      "True targets of subset:  tensor([0., 2., 1., 1., 2., 0., 2., 2., 2., 1., 3., 2., 2., 1., 2., 0., 1., 1.,\n",
      "        2., 3., 3., 1., 0., 3., 2., 2., 1., 2., 2., 2., 1., 3., 2., 1., 3., 2.,\n",
      "        2., 1., 2., 2., 0., 1., 1., 1., 1., 2., 2., 1., 0., 1.])\n",
      "Hypothesized targets of subset:  tensor([0., 2., 1., 1., 2., 0., 1., 2., 3., 1., 3., 3., 2., 1., 3., 0., 1., 0.,\n",
      "        1., 3., 3., 0., 0., 3., 1., 1., 0., 2., 1., 2., 0., 2., 1., 0., 3., 3.,\n",
      "        0., 0., 3., 1., 0., 0., 0., 2., 2., 2., 2., 3., 0., 1.])\n",
      "50 1239 1289\n",
      "After augmentation, size of train_set:  1141  lake set:  1239\n",
      "Epoch: 19 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 98.99988523125648 0.6143733567046451 2.815349206328392 0.7 63.19356516003609 0.5524256651017214 7790.68147110939\n",
      "AL epoch:  19\n",
      "val, test error% for class  0  :  20.0 29.23\n",
      "val, test error% for class  1  :  50.0 49.23\n",
      "val, test error% for class  2  :  50.0 56.72\n",
      "val, test error% for class  3  :  0.0 22.12\n",
      "total misclassified ex from imb classes:  12\n",
      "Per Element Training Gradient Computation is Completed\n",
      "Per Element Validation Gradient Computation is Completed\n",
      "train minibatch gradients shape  torch.Size([1239, 5124])\n",
      "val minibatch gradients shape  torch.Size([12, 5124])\n",
      "kernel compute time:  0.376605749130249\n",
      "Executing SIM command:  /home/snk170001/bioml/dss/notebooks/datk/build/cifarSubsetSelector_ng -mode query -naiveOrRandom naive -logDetLambda 1 -magnificationLambda 1 -numSummaries 1 -budget 50 -queryPrivacyOptimizer logdetmi -numQueries  12  -dontComputeKernel true -imageKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_kernel_1.hdf5 -queryKernelFile /home/snk170001/bioml/dss/notebooks/smi_lake_target_kernel_1.hdf5 -queryqueryKernelFile /home/snk170001/bioml/dss/notebooks/smi_target_kernel_1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True targets of subset:  tensor([1., 0., 3., 1., 2., 1., 0., 3., 2., 3., 0., 1., 2., 2., 2., 1., 2., 3.,\n",
      "        3., 1., 2., 2., 1., 1., 0., 1., 1., 2., 2., 0., 1., 3., 1., 2., 1., 1.,\n",
      "        2., 1., 1., 1., 2., 2., 1., 2., 3., 1., 2., 1., 1., 2.])\n",
      "Hypothesized targets of subset:  tensor([1., 0., 3., 2., 1., 0., 1., 3., 1., 2., 0., 1., 2., 2., 2., 0., 2., 1.,\n",
      "        3., 0., 2., 0., 2., 2., 1., 1., 0., 2., 2., 0., 1., 3., 1., 3., 1., 0.,\n",
      "        1., 0., 1., 0., 2., 2., 1., 2., 3., 0., 1., 1., 0., 2.])\n",
      "50 1189 1239\n",
      "After augmentation, size of train_set:  1191  lake set:  1189\n",
      "Epoch: 20 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 93.03159859776497 0.6809403862300588 2.8329659700393677 0.7 57.633639603853226 0.6087636932707355 8080.754028558731\n",
      "val, test error% for class  0  :  30.0 33.85\n",
      "val, test error% for class  1  :  50.0 40.38\n",
      "val, test error% for class  2  :  30.0 43.78\n",
      "val, test error% for class  3  :  10.0 30.97\n",
      "[[89.23, 46.15, 72.64, 86.73, 73.6875], [50.77, 46.15, 44.78, 68.14, 52.459999999999994], [58.46, 30.38, 54.23, 75.22, 54.5725], [44.62, 39.62, 51.24, 50.44, 46.48], [29.23, 48.08, 56.72, 44.25, 44.57], [36.92, 51.92, 47.26, 41.59, 44.4225], [32.31, 41.92, 50.25, 61.95, 46.6075], [36.92, 51.92, 54.73, 35.4, 44.7425], [27.69, 46.54, 47.76, 40.71, 40.675000000000004], [38.46, 46.54, 40.3, 60.18, 46.37], [35.38, 47.69, 49.75, 39.82, 43.16], [30.77, 50.38, 52.74, 49.56, 45.862500000000004], [36.92, 52.69, 37.81, 39.82, 41.81], [36.92, 51.15, 44.78, 32.74, 41.3975], [29.23, 47.31, 47.76, 33.63, 39.4825], [29.23, 47.69, 54.23, 31.86, 40.7525], [32.31, 41.54, 49.75, 43.36, 41.739999999999995], [24.62, 50.38, 66.67, 27.43, 42.275000000000006], [29.23, 49.23, 56.72, 22.12, 39.325], [33.85, 40.38, 43.78, 30.97, 37.245000000000005]]\n"
     ]
    }
   ],
   "source": [
    "logdetmi_tst, logdetmi_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fl_tst, fl_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc_tst, gc_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'gc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logdet_tst, logdet_csvlog = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'logdet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random random\n",
      "Breast-density Custom dataset stats: Train size:  261 Val size:  40 Lake size:  2139\n",
      "selected classes are:  [0, 3]\n",
      "Saving results to:  SMI_active_learning_results_woVal/breast_density/classimb/random/50/2\n",
      "breast_density_classimb_random_2_random_budget:50_epochs:20_linear:True_runs2\n",
      "AL epoch:  0\n",
      "initial training epoch\n",
      "Init model loaded from disk, skipping init training:  weights/breast_density_MobileNetV2_0.01_{0: 35, 3: 40}_{1: 93, 2: 73}\n",
      "AL epoch:  1\n",
      "val, test error% for class  0  :  80.0 89.23\n",
      "val, test error% for class  1  :  50.0 46.15\n",
      "val, test error% for class  2  :  70.0 72.64\n",
      "val, test error% for class  3  :  100.0 86.73\n",
      "50 2089 2139\n",
      "After augmentation, size of train_set:  311  lake set:  2089\n",
      "Epoch: 2 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 31.729574978351593 0.572347266881029 3.69535756111145 0.575 70.8421282172203 0.47104851330203446 2019.6698310375214\n",
      "AL epoch:  2\n",
      "val, test error% for class  0  :  40.0 30.77\n",
      "val, test error% for class  1  :  60.0 56.92\n",
      "val, test error% for class  2  :  50.0 65.17\n",
      "val, test error% for class  3  :  20.0 34.51\n",
      "50 2039 2089\n",
      "After augmentation, size of train_set:  361  lake set:  2039\n",
      "Epoch: 3 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 36.04263839125633 0.5706371191135734 3.4721153378486633 0.625 64.00381964445114 0.5179968701095462 2360.5135147571564\n",
      "AL epoch:  3\n",
      "val, test error% for class  0  :  20.0 40.0\n",
      "val, test error% for class  1  :  20.0 40.0\n",
      "val, test error% for class  2  :  90.0 65.67\n",
      "val, test error% for class  3  :  20.0 40.71\n",
      "50 1989 2039\n",
      "After augmentation, size of train_set:  411  lake set:  1989\n",
      "Epoch: 4 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 123.27385005354881 0.3284671532846715 14.016320884227753 0.35 168.07931531965733 0.23943661971830985 2669.418761253357\n",
      "AL epoch:  4\n",
      "val, test error% for class  0  :  10.0 13.85\n",
      "val, test error% for class  1  :  50.0 66.92\n",
      "val, test error% for class  2  :  100.0 95.02\n",
      "val, test error% for class  3  :  100.0 99.12\n",
      "50 1939 1989\n",
      "After augmentation, size of train_set:  461  lake set:  1939\n",
      "Epoch: 5 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 60.78976488113403 0.49023861171366595 5.860463500022888 0.475 73.39931386709213 0.4491392801251956 3001.713359117508\n",
      "AL epoch:  5\n",
      "val, test error% for class  0  :  20.0 30.77\n",
      "val, test error% for class  1  :  50.0 42.69\n",
      "val, test error% for class  2  :  70.0 71.14\n",
      "val, test error% for class  3  :  70.0 69.03\n",
      "50 1889 1939\n",
      "After augmentation, size of train_set:  511  lake set:  1889\n",
      "Epoch: 6 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 48.76243484020233 0.5870841487279843 3.0712939500808716 0.65 69.7135682106018 0.5007824726134585 3315.3356070518494\n",
      "AL epoch:  6\n",
      "val, test error% for class  0  :  30.0 21.54\n",
      "val, test error% for class  1  :  30.0 50.77\n",
      "val, test error% for class  2  :  70.0 55.72\n",
      "val, test error% for class  3  :  10.0 53.98\n",
      "50 1839 1889\n",
      "After augmentation, size of train_set:  561  lake set:  1839\n",
      "Epoch: 7 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 56.81001389026642 0.5525846702317291 3.8604289293289185 0.475 69.948421895504 0.460093896713615 3665.68163895607\n",
      "AL epoch:  7\n",
      "val, test error% for class  0  :  50.0 38.46\n",
      "val, test error% for class  1  :  70.0 65.38\n",
      "val, test error% for class  2  :  60.0 56.72\n",
      "val, test error% for class  3  :  30.0 31.86\n",
      "50 1789 1839\n",
      "After augmentation, size of train_set:  611  lake set:  1789\n",
      "Epoch: 8 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 59.69584399461746 0.5941080196399345 2.90768626332283 0.75 64.52295517921448 0.5336463223787168 3972.7148768901825\n",
      "AL epoch:  8\n",
      "val, test error% for class  0  :  40.0 44.62\n",
      "val, test error% for class  1  :  30.0 46.54\n",
      "val, test error% for class  2  :  10.0 46.77\n",
      "val, test error% for class  3  :  20.0 47.79\n",
      "50 1739 1789\n",
      "After augmentation, size of train_set:  661  lake set:  1739\n",
      "Epoch: 9 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 59.6582750082016 0.5748865355521936 3.452463537454605 0.65 67.17151951789856 0.5101721439749609 4346.620023012161\n",
      "AL epoch:  9\n",
      "val, test error% for class  0  :  40.0 27.69\n",
      "val, test error% for class  1  :  40.0 56.54\n",
      "val, test error% for class  2  :  40.0 47.76\n",
      "val, test error% for class  3  :  20.0 46.02\n",
      "50 1689 1739\n",
      "After augmentation, size of train_set:  711  lake set:  1689\n",
      "Epoch: 10 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 74.57207798957825 0.5457102672292545 4.039910435676575 0.475 67.3516834974289 0.5117370892018779 4672.6437520980835\n",
      "AL epoch:  10\n",
      "val, test error% for class  0  :  40.0 26.15\n",
      "val, test error% for class  1  :  50.0 43.46\n",
      "val, test error% for class  2  :  70.0 62.19\n",
      "val, test error% for class  3  :  50.0 50.44\n",
      "50 1639 1689\n",
      "After augmentation, size of train_set:  761  lake set:  1639\n",
      "Epoch: 11 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 75.86768567562103 0.5834428383705651 3.3240476548671722 0.6 64.49960833787918 0.5665101721439749 4968.161025762558\n",
      "AL epoch:  11\n",
      "val, test error% for class  0  :  40.0 46.15\n",
      "val, test error% for class  1  :  40.0 33.46\n",
      "val, test error% for class  2  :  60.0 50.75\n",
      "val, test error% for class  3  :  20.0 51.33\n",
      "50 1589 1639\n",
      "After augmentation, size of train_set:  811  lake set:  1589\n",
      "Epoch: 12 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 73.83511891961098 0.6017262638717632 3.3716474175453186 0.625 61.47042894363403 0.5524256651017214 5268.381823539734\n",
      "AL epoch:  12\n",
      "val, test error% for class  0  :  30.0 41.54\n",
      "val, test error% for class  1  :  40.0 42.31\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  30.0 43.36\n",
      "50 1539 1589\n",
      "After augmentation, size of train_set:  861  lake set:  1539\n",
      "Epoch: 13 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 94.155921459198 0.49361207897793263 4.062448665499687 0.55 75.85044610500336 0.42566510172143973 5630.066622972488\n",
      "AL epoch:  13\n",
      "val, test error% for class  0  :  30.0 16.92\n",
      "val, test error% for class  1  :  60.0 70.0\n",
      "val, test error% for class  2  :  90.0 74.13\n",
      "val, test error% for class  3  :  0.0 22.12\n",
      "50 1489 1539\n",
      "After augmentation, size of train_set:  911  lake set:  1489\n",
      "Epoch: 14 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 83.09704878926277 0.6256860592755215 2.851384937763214 0.75 59.0727396607399 0.568075117370892 5921.996418237686\n",
      "AL epoch:  14\n",
      "val, test error% for class  0  :  30.0 52.31\n",
      "val, test error% for class  1  :  40.0 35.38\n",
      "val, test error% for class  2  :  20.0 48.76\n",
      "val, test error% for class  3  :  10.0 46.02\n",
      "50 1439 1489\n",
      "After augmentation, size of train_set:  961  lake set:  1439\n",
      "Epoch: 15 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 90.03280314803123 0.5848074921956296 2.966593623161316 0.7 62.81592047214508 0.5336463223787168 6263.190521240234\n",
      "AL epoch:  15\n",
      "val, test error% for class  0  :  10.0 24.62\n",
      "val, test error% for class  1  :  50.0 51.92\n",
      "val, test error% for class  2  :  60.0 62.69\n",
      "val, test error% for class  3  :  0.0 18.58\n",
      "50 1389 1439\n",
      "After augmentation, size of train_set:  1011  lake set:  1389\n",
      "Epoch: 16 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 97.71351191401482 0.5687438180019783 2.879222333431244 0.65 60.165692150592804 0.5805946791862285 6594.137750864029\n",
      "AL epoch:  16\n",
      "val, test error% for class  0  :  30.0 40.0\n",
      "val, test error% for class  1  :  20.0 38.85\n",
      "val, test error% for class  2  :  50.0 36.32\n",
      "val, test error% for class  3  :  40.0 60.18\n",
      "50 1339 1389\n",
      "After augmentation, size of train_set:  1061  lake set:  1339\n",
      "Epoch: 17 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 93.0324255824089 0.6060320452403393 3.2871562838554382 0.575 60.47174718976021 0.5712050078247262 6944.252722024918\n",
      "AL epoch:  17\n",
      "val, test error% for class  0  :  50.0 41.54\n",
      "val, test error% for class  1  :  60.0 42.31\n",
      "val, test error% for class  2  :  50.0 49.75\n",
      "val, test error% for class  3  :  10.0 32.74\n",
      "50 1289 1339\n",
      "After augmentation, size of train_set:  1111  lake set:  1289\n",
      "Epoch: 18 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 104.64263242483139 0.5670567056705671 3.160198152065277 0.6 60.3795505464077 0.5461658841940532 7312.527335643768\n",
      "AL epoch:  18\n",
      "val, test error% for class  0  :  80.0 84.62\n",
      "val, test error% for class  1  :  0.0 23.85\n",
      "val, test error% for class  2  :  60.0 74.13\n",
      "val, test error% for class  3  :  20.0 21.24\n",
      "50 1239 1289\n",
      "After augmentation, size of train_set:  1161  lake set:  1239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 97.54228103160858 0.632213608957795 2.8004336059093475 0.75 58.62907028198242 0.5821596244131455 7615.39546918869\n",
      "AL epoch:  19\n",
      "val, test error% for class  0  :  40.0 46.15\n",
      "val, test error% for class  1  :  0.0 36.54\n",
      "val, test error% for class  2  :  60.0 51.24\n",
      "val, test error% for class  3  :  0.0 34.51\n",
      "50 1189 1239\n",
      "After augmentation, size of train_set:  1211  lake set:  1189\n",
      "Epoch: 20 FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time: 104.68075922131538 0.6077621800165153 3.2187494337558746 0.65 57.81700223684311 0.5727699530516432 7946.918114185333\n",
      "val, test error% for class  0  :  50.0 47.69\n",
      "val, test error% for class  1  :  30.0 43.08\n",
      "val, test error% for class  2  :  40.0 42.29\n",
      "val, test error% for class  3  :  20.0 39.82\n",
      "[[89.23, 46.15, 72.64, 86.73, 73.6875], [30.77, 56.92, 65.17, 34.51, 46.8425], [40.0, 40.0, 65.67, 40.71, 46.595000000000006], [13.85, 66.92, 95.02, 99.12, 68.72749999999999], [30.77, 42.69, 71.14, 69.03, 53.4075], [21.54, 50.77, 55.72, 53.98, 45.5025], [38.46, 65.38, 56.72, 31.86, 48.105000000000004], [44.62, 46.54, 46.77, 47.79, 46.43], [27.69, 56.54, 47.76, 46.02, 44.502500000000005], [26.15, 43.46, 62.19, 50.44, 45.56], [46.15, 33.46, 50.75, 51.33, 45.4225], [41.54, 42.31, 49.75, 43.36, 44.239999999999995], [16.92, 70.0, 74.13, 22.12, 45.792500000000004], [52.31, 35.38, 48.76, 46.02, 45.6175], [24.62, 51.92, 62.69, 18.58, 39.4525], [40.0, 38.85, 36.32, 60.18, 43.8375], [41.54, 42.31, 49.75, 32.74, 41.585], [84.62, 23.85, 74.13, 21.24, 50.96], [46.15, 36.54, 51.24, 34.51, 42.11], [47.69, 43.08, 42.29, 39.82, 43.22]]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1,6):\n",
    "random_test_acc = train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"random\",'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
